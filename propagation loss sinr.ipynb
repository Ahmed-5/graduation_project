{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d559492b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d559492b",
        "outputId": "1ea70efa-f7a6-43ea-d31c-1e5f029f5db4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6e5917b2b0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "# import cupy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "random.seed(154012)\n",
        "np.random.seed(154012)\n",
        "torch.manual_seed(154012)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "21eb46de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21eb46de",
        "outputId": "35ea8e0b-2bcc-4b40-889d-43a7beffed6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 2)\n"
          ]
        }
      ],
      "source": [
        "fc = 2.6 #2.6 GHz\n",
        "bandwidth = 1e7 #10 MHz\n",
        "temp = 40 + 273.15 # 40 celsius in kelvin\n",
        "boltz = 1.381e-23\n",
        "epsilon = 1e-7\n",
        "row_bs = 4\n",
        "col_bs = 4\n",
        "mean_dist = 500\n",
        "grid_width = mean_dist*(row_bs+1)\n",
        "grid_height = mean_dist*(col_bs+1)\n",
        "n_actions = 2\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#   blackout_choices = cp.array([\n",
        "#       [3*3600, 4*3600],\n",
        "#       [6*3600, 4*3600],\n",
        "#       [14*3600, 4*3600],\n",
        "#       [19*3600, 4*3600],\n",
        "#   ])\n",
        "#   bs_loc = cp.mgrid[mean_dist: grid_width-mean_dist:mean_dist, mean_dist: grid_height-mean_dist:mean_dist]\n",
        "#   bs_loc = bs_loc.reshape((2,-1)).T\n",
        "#   bs_loc += mean_dist//10 - cp.random.randint(0, 2*mean_dist//10, bs_loc.shape)\n",
        "\n",
        "#   bs_power = cp.array([40. for i in bs_loc])\n",
        "\n",
        "#   print(bs_loc.shape)\n",
        "\n",
        "#   bs_blackout = blackout_choices[cp.random.randint(0,3, (row_bs*col_bs))]\n",
        "\n",
        "\n",
        "# else:\n",
        "blackout_choices = np.array([\n",
        "    [3*3600, 4*3600],\n",
        "    [6*3600, 4*3600],\n",
        "    [14*3600, 4*3600],\n",
        "    [19*3600, 4*3600],\n",
        "])\n",
        "\n",
        "baseline_times = [0]\n",
        "baseline_times.extend([i[0] for i in blackout_choices])\n",
        "baseline_times.extend([i[0]+i[1]+1 for i in blackout_choices])\n",
        "baseline_times.sort()\n",
        "\n",
        "bs_loc = np.mgrid[mean_dist: grid_width:mean_dist, mean_dist: grid_height:mean_dist]\n",
        "bs_loc = bs_loc.reshape((2,-1)).T\n",
        "bs_loc += mean_dist//10 - np.random.randint(0, 2*mean_dist//10, bs_loc.shape)\n",
        "\n",
        "bs_power = np.array([40. for i in bs_loc])\n",
        "\n",
        "print(bs_loc.shape)\n",
        "\n",
        "bs_blackout = blackout_choices[np.random.randint(0,3, (row_bs*col_bs))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "GiH0Yg4eyzBh",
      "metadata": {
        "id": "GiH0Yg4eyzBh"
      },
      "outputs": [],
      "source": [
        "# class Environment():\n",
        "#     def __init__(self, bs_loc, bs_power, bs_blackouts, max_power, min_power, n_actions, grid_width, grid_height, n_nearest):\n",
        "#         self.bs_loc = bs_loc\n",
        "#         self.bs_power = bs_power\n",
        "#         self.electricity = np.zeros_like(bs_power)\n",
        "#         self.bs_blackout_start = bs_blackouts[:,0]\n",
        "#         self.bs_blackout_end = np.sum(bs_blackouts, axis=-1)\n",
        "#         self.apply_blackouts(0)\n",
        "#         self.max_power = max_power # in watt\n",
        "#         self.min_power = min_power\n",
        "#         self.n_actions = n_actions\n",
        "#         self.grid_width = grid_width\n",
        "#         self.grid_height = grid_height\n",
        "#         self.points = np.mgrid[0:grid_width, 0:grid_height]\n",
        "#         self.points = np.stack(self.points, axis=-1)\n",
        "#         self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
        "#         self.bit_rate = self.bit_rate_from_grid()\n",
        "#         self.reward = self.get_reward()\n",
        "#         self.actions = self.watt_to_dbm(np.linspace(min_power, max_power, n_actions))\n",
        "#         self.n_nearest = min(n_nearest, bs_power.shape[0])\n",
        "#         self.n_bs = bs_power.shape[0]\n",
        "        \n",
        "        \n",
        "    \n",
        "#     def get_input_dim(self):\n",
        "#         return self.n_nearest*4 # after adding has electricity attrib 3 will be changed to 4\n",
        "    \n",
        "#     def get_output_dim(self):\n",
        "#         return self.n_actions\n",
        "    \n",
        "#     def apply_blackouts(self, time):\n",
        "#         self.electricity = (time>=self.bs_blackout_start) & (time<=self.bs_blackout_end)\n",
        "    \n",
        "#     def make_action(self, bs_index, action):\n",
        "#         state = self.get_state(bs_index) \n",
        "#         self.bs_power[bs_index] = self.actions[action]\n",
        "#         self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
        "#         self.bit_rate = self.bit_rate_from_grid()\n",
        "#         next_state = self.get_state(bs_index)\n",
        "#         reward = self.get_reward()\n",
        "#         return state, action, next_state, reward\n",
        "        \n",
        "#     def watt_to_dbm(self, watt):\n",
        "#         return 10*np.log10(1000*watt)\n",
        "\n",
        "#     def dbm_to_watt(self, dbm):\n",
        "#         return np.power(10, dbm/10)/1000\n",
        "\n",
        "#     def path_loss(self, distance, frequency=fc):\n",
        "#     #     return 36.7*np.log10(distance) + 47.7 + 26*np.log10(frequency)\n",
        "#         return 35*np.log10(distance) + 35.7\n",
        "\n",
        "#     def power_grid(self, bs_loc, bs_power, points):\n",
        "#         powers = []\n",
        "#         for loc, power in zip(bs_loc, bs_power):\n",
        "#             distance = np.linalg.norm(loc - points+epsilon, axis=-1)\n",
        "#             powers.append(power - self.path_loss(distance))\n",
        "#         return np.stack(powers, axis=-1)\n",
        "\n",
        "#     def get_bit_rate_sinr(self, signal, interference, bandwidth=bandwidth, temp=temp):\n",
        "#         return bandwidth*np.log10(1+ (signal/(interference + temp*boltz*bandwidth)))\n",
        "    \n",
        "#     def bit_rate_from_grid(self):\n",
        "#         max_power = self.dbm_to_watt(self.powers.max(axis=-1))\n",
        "#         interference_power = self.dbm_to_watt(self.powers).sum(axis=-1) - max_power\n",
        "#         return self.get_bit_rate_sinr(max_power, interference_power)\n",
        "\n",
        "#     def bit_rate_cost_function(self, b_rate):\n",
        "#         total_points = 1\n",
        "#         for i in b_rate.shape:\n",
        "#             total_points *= i\n",
        "\n",
        "#         under_1mb = b_rate<(1024**2)*8\n",
        "#         under_1mb = under_1mb.sum()\n",
        "#         min_speed = b_rate.min()/(1024*1024*8) #1MB\n",
        "#         return 10*under_1mb/total_points + 1/(min_speed+epsilon)\n",
        "\n",
        "#     def get_reward(self):\n",
        "#         constant = 10\n",
        "#         br_cost = self.bit_rate_cost_function(self.bit_rate)\n",
        "#         print(self.electricity.shape)\n",
        "#         print(self.dbm_to_watt(self.bs_power))\n",
        "#         elec_cost = np.inner(self.electricity, self.dbm_to_watt(self.bs_power))\n",
        "#         return 10 - br_cost - elec_cost\n",
        "    \n",
        "#     def get_state(self, bs_index):\n",
        "#         x,y = self.bs_loc[bs_index]\n",
        "#         powers = self.powers[np.round(x),np.round(y),:].reshape(-1)\n",
        "#         indecies = np.argsort(powers)[::-1][:self.n_nearest]\n",
        "#         bs_index_posistion = np.where(indecies == bs_index)[0]\n",
        "        \n",
        "#         if bs_index_posistion.size > 0:\n",
        "#             indecies = np.delete(indecies, bs_index_posistion)\n",
        "#             indecies = indecies[:self.n_nearest]\n",
        "#         else:\n",
        "#             indecies = indecies[:self.n_nearest-1]\n",
        "            \n",
        "#         bs_power = powers[bs_index]\n",
        "#         state = np.array([powers[bs_index], self.electricity[bs_index], 0, 0])\n",
        "#         nearest_powers = powers[np.array(indecies)]\n",
        "#         nearest_elec = self.electricity[np.array(indecies)]\n",
        "#         diff =  self.bs_loc[np.array(indecies)] - self.bs_loc[bs_index]\n",
        "#         nearest_distances = np.linalg.norm(diff, axis=-1)\n",
        "#         nearest_angles = np.arctan2(diff[:,1].reshape(-1), diff[:,0].reshape(-1))\n",
        "        \n",
        "#         return np.hstack((state, np.stack([nearest_powers, nearest_elec,\n",
        "#                                            nearest_distances, nearest_angles], axis=-1).reshape(-1)))\n",
        "    \n",
        "#     def plot_bit_rate(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "#     def plot_log_bit_rate(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "#     def plot_power_grid(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "#     def plot_log_power_grid(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "hg_4anvUcXxP",
      "metadata": {
        "id": "hg_4anvUcXxP"
      },
      "outputs": [],
      "source": [
        "class Environment():\n",
        "    def __init__(self, bs_loc, bs_power, bs_blackouts, max_power, min_power, n_actions, grid_width, grid_height, n_nearest):\n",
        "        self.bs_loc = bs_loc\n",
        "        self.bs_power = bs_power\n",
        "        self.electricity = np.zeros_like(bs_power) # 0 have electricity, 1 blackout\n",
        "        self.bs_blackout_start = bs_blackouts[:,0]\n",
        "        self.bs_blackout_end = np.sum(bs_blackouts, axis=-1)\n",
        "        self.apply_blackouts(0)\n",
        "        self.max_power = max_power # in watt\n",
        "        self.min_power = min_power\n",
        "        self.n_actions = n_actions\n",
        "        self.grid_width = grid_width\n",
        "        self.grid_height = grid_height\n",
        "        self.reward, _ = self.get_reward()\n",
        "        self.actions = self.watt_to_dbm(np.linspace(min_power, max_power, n_actions))\n",
        "        self.n_nearest = min(n_nearest, bs_power.shape[0])\n",
        "        self.n_bs = bs_power.shape[0]\n",
        "        self.episode = 0\n",
        "        \n",
        "    def baseline_action(self, bs_index):\n",
        "        if self.electricity[bs_index]:\n",
        "            self.set_action(bs_index, 0)\n",
        "        else:\n",
        "            self.set_action(bs_index, self.n_actions-1)\n",
        "\n",
        "    def apply_baseline(self):\n",
        "        for bs in range(self.bs_power.shape[0]):\n",
        "            self.baseline_action(bs)\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        return self.n_nearest*4 # after adding has electricity attrib 3 will be changed to 4\n",
        "    \n",
        "    def get_output_dim(self):\n",
        "        return self.n_actions\n",
        "    \n",
        "    def apply_blackouts(self, time):\n",
        "        self.episode = time\n",
        "        self.electricity = (time>=self.bs_blackout_start) & (time<=self.bs_blackout_end)\n",
        "\n",
        "    def set_action(self, bs_index, action):\n",
        "        self.bs_power[bs_index] = self.actions[action]\n",
        "    \n",
        "    def make_action(self, bs_index, action):\n",
        "        state = self.get_state(bs_index)\n",
        "        self.set_action(bs_index, action)\n",
        "        # print(action, \"-\", self.bs_power)\n",
        "        # self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
        "        # self.bit_rate = self.bit_rate_from_grid()\n",
        "        next_state = self.get_state(bs_index) \n",
        "        kpi, reward = self.get_reward(bs_index)\n",
        "        return state, action, next_state, reward, kpi\n",
        "        \n",
        "    def watt_to_dbm(self, watt):\n",
        "        return 10*np.log10(1000*watt)\n",
        "\n",
        "    def dbm_to_watt(self, dbm):\n",
        "        return np.power(10, dbm/10)/1000\n",
        "\n",
        "    def path_loss(self, distance, frequency=fc):\n",
        "    #     return 36.7*np.log10(distance) + 47.7 + 26*np.log10(frequency)\n",
        "        return 35*np.log10(distance+1) + 35.7\n",
        "\n",
        "    def power_grid(self, bs_loc, bs_power, points):\n",
        "        powers = []\n",
        "        for loc, power in zip(bs_loc, bs_power):\n",
        "            distance = np.linalg.norm(loc - points+epsilon, axis=-1)\n",
        "            powers.append(power - self.path_loss(distance))\n",
        "        return np.stack(powers, axis=-1)\n",
        "\n",
        "    def get_bit_rate_sinr(self, signal, interference, bandwidth=bandwidth, temp=temp):\n",
        "        return bandwidth*np.log10(1+ (signal/(interference + temp*boltz*bandwidth)))\n",
        "    \n",
        "    def bit_rate_from_grid(self, powers):\n",
        "        max_power = self.dbm_to_watt(powers.max(axis=-1))\n",
        "        interference_power = self.dbm_to_watt(powers).sum(axis=-1) - max_power\n",
        "        return self.get_bit_rate_sinr(max_power, interference_power)\n",
        "\n",
        "    def bit_rate_cost_function(self, b_rate):\n",
        "        # total_points = self.grid_height * self.grid_width\n",
        "        total_points = b_rate.shape[0]*b_rate.shape[1]\n",
        "\n",
        "        under_1mb = b_rate<(1024**2)*8\n",
        "        under_1mb = under_1mb.sum()\n",
        "        min_speed = b_rate.min()/(1024*1024*8) #1MB\n",
        "        max_speed = b_rate.max()/(1024*1024*8) #1MB\n",
        "        return 10*under_1mb/total_points , min_speed, max_speed\n",
        "\n",
        "    def get_reward(self, bs_index=None):\n",
        "        constant = 10\n",
        "        br_cost = 0\n",
        "        max_cost = 0\n",
        "        min_cost = 10000 # just arbitrary large number\n",
        "\n",
        "        # number of points propotional to the width height\n",
        "        # for i in range(0, self.grid_width, 1000):\n",
        "        #     for j in range(0, self.grid_height, 1000):\n",
        "        #         points = np.mgrid[i:min(i+1000, grid_width), j:min(j+1000, grid_height)]\n",
        "        #         points = np.stack(points, axis=-1)\n",
        "        #         power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        #         bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        #         percentage, min_speed_cost, max_speed = self.bit_rate_cost_function(bit_rate)\n",
        "        #         br_cost += percentage\n",
        "        #         min_cost = min(min_cost, min_speed_cost)\n",
        "        #         max_cost = max(max_cost, max_speed)\n",
        "\n",
        "\n",
        "        # fixed number of point despite the width height\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_width, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        percentage, min_speed_cost, max_speed = self.bit_rate_cost_function(bit_rate)\n",
        "        br_cost += percentage\n",
        "        min_cost = min(min_cost, min_speed_cost)\n",
        "        max_cost = max(max_cost, max_speed)\n",
        "\n",
        "        # print(self.electricity.shape)\n",
        "        # print(self.dbm_to_watt(self.bs_power))\n",
        "        disparity = (max_cost - min_cost + epsilon)/ (max_cost + epsilon*epsilon) * 10\n",
        "        elec_cost = np.inner(self.electricity, self.dbm_to_watt(self.bs_power))\n",
        "        r = constant - br_cost + min_cost - disparity\n",
        "        elec_loc_cost = elec_cost\n",
        "        if (bs_index != None):\n",
        "            elec_loc_cost = self.electricity[bs_index]*self.dbm_to_watt(self.bs_power[bs_index])\n",
        "        return r-elec_cost, r-elec_loc_cost\n",
        "    \n",
        "    def get_state(self, bs_index):\n",
        "        # get the base station location\n",
        "        loc = self.bs_loc[bs_index]\n",
        "\n",
        "        # get the recieved powers from each base station to the current base station\n",
        "        powers = self.power_grid(self.bs_loc, self.bs_power, loc)\n",
        "        # print(\"dbm\")\n",
        "        # print(powers)\n",
        "        powers = self.dbm_to_watt(powers)\n",
        "        # print(\"watt\")\n",
        "        # print(powers)\n",
        "\n",
        "        # choose the base stations that give the most power to this location\n",
        "        indecies = np.argsort(powers)[::-1][:self.n_nearest]\n",
        "\n",
        "        # check if this base station is one of the base stations that gives the max power\n",
        "        bs_index_posistion = np.where(indecies == bs_index)[0]\n",
        "        \n",
        "\n",
        "        # if the base station is one of them we remove it, that cause it is added later to the state\n",
        "        # we do not we this value to be duplicated in the state\n",
        "        if bs_index_posistion.size > 0:\n",
        "            indecies = np.delete(indecies, bs_index_posistion)\n",
        "            indecies = indecies[:self.n_nearest]\n",
        "        else:\n",
        "            indecies = indecies[:self.n_nearest-1]\n",
        "            \n",
        "        # get the power produce by the base station\n",
        "        bs_power = powers[bs_index]\n",
        "        self_power_norm = 1e3\n",
        "        other_power_norm = 2e12 # mean dist 500\n",
        "\n",
        "        # init state: rows represent base station \n",
        "        # in each row: recieved power, does it have electricity, distance from here, the angle\n",
        "        state = np.array([bs_power*self_power_norm, 1 - self.electricity[bs_index], 0, 0])\n",
        "        nearest_powers = other_power_norm*powers[np.array(indecies)]\n",
        "        nearest_elec = 1 - self.electricity[np.array(indecies)]\n",
        "        diff =  self.bs_loc[np.array(indecies)] - self.bs_loc[bs_index]\n",
        "        nearest_distances = np.linalg.norm(diff, axis=-1)/ mean_dist\n",
        "        nearest_angles = np.arctan2(diff[:,1].reshape(-1), diff[:,0].reshape(-1)) + np.pi\n",
        "        \n",
        "        state = np.hstack((state, np.stack([nearest_powers, nearest_elec,\n",
        "                                           nearest_distances, nearest_angles], axis=-1).reshape(-1)))\n",
        "\n",
        "        # print(state)\n",
        "        return state\n",
        "    \n",
        "    def plot_bit_rate(self):\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "    def plot_log_bit_rate(self):\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], np.log(bit_rate + epsilon), 100, cmap = plt.cm.jet)\n",
        "\n",
        "    def save_bit_rate_plot(self, index=None):\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        # plt.contourf(points[:,:,0], points[:,:,1], bit_rate, 100, cmap = plt.cm.jet)\n",
        "        # if not index:\n",
        "        #     plt.savefig('bitrate.png')\n",
        "        # else:\n",
        "        #     name = \"bitrate_{:06d}.png\".format(index)\n",
        "        #     plt.savefig(name)\n",
        "        if index == None:\n",
        "            plt.imsave('log_bitrate.png', np.log(bit_rate + epsilon), cmap = plt.cm.jet)\n",
        "            # plt.savefig('log_bitrate.png')\n",
        "        else:\n",
        "            name = \"log_bitrate_{:06d}.png\".format(index)\n",
        "            plt.imsave(name, np.log(bit_rate + epsilon), cmap = plt.cm.jet)\n",
        "            # plt.savefig(name)\n",
        "        \n",
        "    def plot_watt_power_grid(self, index):\n",
        "        if index < 0 or index >= self.n_bs:\n",
        "            print(\"Invalid index\")\n",
        "            return\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], self.dbm_to_watt(power_grid[:,:,index]), 100, cmap = plt.cm.jet)\n",
        "        \n",
        "    def plot_dbm_power_grid(self, index):\n",
        "        if index < 0 or index >= self.n_bs:\n",
        "            print(\"Invalid index\")\n",
        "            return\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], power_grid[:,:,index], 100, cmap = plt.cm.jet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "id": "LPsBJCYxkVWh",
      "metadata": {
        "id": "LPsBJCYxkVWh"
      },
      "outputs": [],
      "source": [
        "class EnvironmentCluster():\n",
        "    def __init__(self, bs_loc, bs_power, bs_blackouts, max_power, min_power, n_actions, grid_width, grid_height):\n",
        "        self.bs_loc = bs_loc # locations of the base stations\n",
        "        self.bs_power = bs_power # current power values of the base stations\n",
        "        self.electricity = np.zeros_like(bs_power) # current electricity state 0 have electricity, 1 blackout\n",
        "        self.bs_blackout_start = bs_blackouts[:,0] # times which blackout starts for each base station\n",
        "        self.bs_blackout_end = np.sum(bs_blackouts, axis=-1) # times which blackout ends for each base station\n",
        "        self.apply_blackouts(0)\n",
        "        self.max_power = max_power # in watt\n",
        "        self.min_power = min_power\n",
        "        self.n_actions = n_actions\n",
        "        self.grid_width = grid_width\n",
        "        self.grid_height = grid_height\n",
        "        self.reward, _ = self.get_reward()\n",
        "        self.actions = self.watt_to_dbm(np.linspace(min_power, max_power, n_actions))\n",
        "        self.n_bs = bs_power.shape[0]\n",
        "        self.episode = 0\n",
        "        \n",
        "    def baseline_action(self, bs_index):\n",
        "        if self.electricity[bs_index]:\n",
        "            self.set_action(bs_index, 0)\n",
        "        else:\n",
        "            self.set_action(bs_index, self.n_actions-1)\n",
        "\n",
        "    def bulk_actions(self, actions):\n",
        "        actions = np.round_(actions)\n",
        "        # actions = actions.astype(int)\n",
        "        self.bs_power = self.actions[actions]\n",
        "\n",
        "    def apply_baseline(self):\n",
        "        self.bulk_actions((1 - self.electricity)*(self.n_actions-1))\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        return self.n_bs\n",
        "    \n",
        "    def get_output_dim(self):\n",
        "        return self.n_bs\n",
        "    \n",
        "    def apply_blackouts(self, time):\n",
        "        self.episode = time\n",
        "        self.electricity = (time>=self.bs_blackout_start) & (time<=self.bs_blackout_end)\n",
        "\n",
        "    def set_action(self, bs_index, action):\n",
        "        self.bs_power[bs_index] = self.actions[action]\n",
        "    \n",
        "    def make_action(self, actions):\n",
        "        state = self.get_state()\n",
        "        self.bulk_actions(actions)\n",
        "        next_state = self.get_state() \n",
        "        kpi, reward = self.get_reward()\n",
        "        return state, action, next_state, reward, kpi\n",
        "        \n",
        "    def watt_to_dbm(self, watt):\n",
        "        return 10*np.log10(1000*watt)\n",
        "\n",
        "    def dbm_to_watt(self, dbm):\n",
        "        return np.power(10, dbm/10)/1000\n",
        "\n",
        "    def path_loss(self, distance):\n",
        "        return 35*np.log10(distance+1) + 35.7\n",
        "\n",
        "    def power_grid(self, bs_loc, bs_power, points):\n",
        "        powers = []\n",
        "        for loc, power in zip(bs_loc, bs_power):\n",
        "            distance = np.linalg.norm(loc - points+epsilon, axis=-1)\n",
        "            powers.append(power - self.path_loss(distance))\n",
        "        return np.stack(powers, axis=-1)\n",
        "\n",
        "    def get_bit_rate_sinr(self, signal, interference, bandwidth=bandwidth, temp=temp):\n",
        "        return bandwidth*np.log10(1+ (signal/(interference + temp*boltz*bandwidth)))\n",
        "    \n",
        "    def bit_rate_from_grid(self, powers):\n",
        "        max_power = self.dbm_to_watt(powers.max(axis=-1))\n",
        "        interference_power = self.dbm_to_watt(powers).sum(axis=-1) - max_power\n",
        "        return self.get_bit_rate_sinr(max_power, interference_power)\n",
        "\n",
        "    def bit_rate_cost_function(self, b_rate):\n",
        "        # total_points = self.grid_height * self.grid_width\n",
        "        total_points = b_rate.shape[0]*b_rate.shape[1]\n",
        "\n",
        "        under_1mb = b_rate<(1024**2)*8\n",
        "        under_1mb = under_1mb.sum()\n",
        "        min_speed = b_rate.min()/(1024*1024*8) #1MB\n",
        "        max_speed = b_rate.max()/(1024*1024*8) #1MB\n",
        "        return 10*under_1mb/total_points , min_speed, max_speed\n",
        "\n",
        "    def get_reward(self):\n",
        "        constant = 20\n",
        "        br_cost = 0\n",
        "        max_cost = 0\n",
        "        min_cost = 10000 # just arbitrary large number\n",
        "\n",
        "\n",
        "        # fixed number of point despite the width height\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_width, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        percentage, min_speed_cost, max_speed = self.bit_rate_cost_function(bit_rate)\n",
        "        br_cost += percentage\n",
        "        min_cost = min(min_cost, min_speed_cost)\n",
        "        max_cost = max(max_cost, max_speed)\n",
        "\n",
        "        # print(self.electricity.shape)\n",
        "        # print(self.dbm_to_watt(self.bs_power))\n",
        "        disparity = (max_cost - min_cost)/ (max_cost + 0.01) * 10\n",
        "        elec_cost = np.inner(self.electricity, self.dbm_to_watt(self.bs_power))\n",
        "        r = constant - br_cost + min_cost - disparity\n",
        "        elec_loc_cost = elec_cost\n",
        "        \n",
        "        return r-elec_cost, r-elec_loc_cost\n",
        "    \n",
        "    def get_state(self):\n",
        "        return self.electricity\n",
        "    \n",
        "    def plot_bit_rate(self):\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "    def plot_log_bit_rate(self):\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], np.log(bit_rate + epsilon), 100, cmap = plt.cm.jet)\n",
        "\n",
        "    def save_bit_rate_plot(self, index=None):\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "        # plt.contourf(points[:,:,0], points[:,:,1], bit_rate, 100, cmap = plt.cm.jet)\n",
        "        # if not index:\n",
        "        #     plt.savefig('bitrate.png')\n",
        "        # else:\n",
        "        #     name = \"bitrate_{:06d}.png\".format(index)\n",
        "        #     plt.savefig(name)\n",
        "        if index == None:\n",
        "            plt.imsave('log_bitrate.png', np.log(bit_rate + epsilon), cmap = plt.cm.jet)\n",
        "            # plt.savefig('log_bitrate.png')\n",
        "        else:\n",
        "            name = \"log_bitrate_{:06d}.png\".format(index)\n",
        "            plt.imsave(name, np.log(bit_rate + epsilon), cmap = plt.cm.jet)\n",
        "            # plt.savefig(name)\n",
        "        \n",
        "    def plot_watt_power_grid(self, index):\n",
        "        if index < 0 or index >= self.n_bs:\n",
        "            print(\"Invalid index\")\n",
        "            return\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], self.dbm_to_watt(power_grid[:,:,index]), 100, cmap = plt.cm.jet)\n",
        "        \n",
        "    def plot_dbm_power_grid(self, index):\n",
        "        if index < 0 or index >= self.n_bs:\n",
        "            print(\"Invalid index\")\n",
        "            return\n",
        "        points = np.meshgrid(np.linspace(0, self.grid_width, 1001), np.linspace(0, self.grid_height, 1001))\n",
        "        points = np.stack(points, axis=-1)\n",
        "        power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "        plt.contourf(points[:,:,0], points[:,:,1], power_grid[:,:,index], 100, cmap = plt.cm.jet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "id": "86a7311b",
      "metadata": {
        "id": "86a7311b"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module): # ready\n",
        "    def __init__(self, n_inputs, n_actions):\n",
        "        nn.Module.__init__(self)\n",
        "        self.fc1 = nn.Linear(in_features=n_inputs, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=64)\n",
        "        self.out = nn.Linear(in_features=64, out_features=n_actions)\n",
        "\n",
        "    def forward(self, t):\n",
        "        t = F.relu(self.fc1(t))\n",
        "        t = F.relu(self.fc2(t))\n",
        "        t = F.relu(self.fc3(t))\n",
        "        t = F.sigmoid(self.out(t))\n",
        "        return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "id": "8a2efd98",
      "metadata": {
        "id": "8a2efd98"
      },
      "outputs": [],
      "source": [
        "Experience = namedtuple(\n",
        "    \"Experience\", (\"state\", \"action\", \"next_state\", \"reward\")) # ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "id": "278da5c2",
      "metadata": {
        "id": "278da5c2"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(): # ready\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.push_count = 0\n",
        "\n",
        "    def push(self, experience):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory[self.push_count % self.capacity] = experience\n",
        "        self.push_count += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "id": "5c69e5d9",
      "metadata": {
        "id": "5c69e5d9"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedyStrategy(): # ready\n",
        "    def __init__(self, start, end, decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        "\n",
        "    def get_exploration_rate(self, current_step):\n",
        "        return self.end + (self.start - self.end) * np.exp(-1. * current_step * self.decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "id": "d11c365f",
      "metadata": {
        "id": "d11c365f"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, strategy, num_actions):\n",
        "        self.current_step = 0\n",
        "        self.strategy = strategy\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "    def select_action(self, state, policy_net):\n",
        "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
        "        self.current_step += 1\n",
        "        t = random.random()\n",
        "        \n",
        "        # print(rate, t)\n",
        "        if rate > t:\n",
        "        #     action = random.randrange(self.num_actions) # for normal environment\n",
        "            # action = random.randrange(self.num_actions+1) #when having 2 actions  for 1 bs or more\n",
        "            # return torch.tensor([[action]])\n",
        "            actions = np.random.randint(0, 2, (row_bs*col_bs)).astype(int)\n",
        "            return torch.tensor([actions])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                # return policy_net(state).argmax(dim=1) #when having multiple actions for 1 bs\n",
        "                # return torch.round_(policy_net(state)).type(torch.IntTensor) #when having 2 actions  for 1 bs or more\n",
        "                return torch.round(policy_net(state)).type(torch.IntTensor) #when having multiple bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "id": "f1943cf4",
      "metadata": {
        "id": "f1943cf4"
      },
      "outputs": [],
      "source": [
        "# class QValues():\n",
        "#     @staticmethod\n",
        "#     def get_current(policy_net, states, actions):\n",
        "#         return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "\n",
        "#     @staticmethod\n",
        "#     def get_next(target_net, next_states):\n",
        "#         values = target_net(next_states).max(dim=1)[0].detach()\n",
        "#         return values\n",
        "\n",
        "class QValues():\n",
        "    @staticmethod\n",
        "    def get_current(policy_net, states, actions):\n",
        "        dims = policy_net(states)\n",
        "        # print(dims.shape)\n",
        "        # print(dims.gather().shape)\n",
        "        # print(actions.shape)\n",
        "        # return dims.gather(dim=1, index=actions.unsqueeze(-1))\n",
        "        return dims\n",
        "\n",
        "    @staticmethod\n",
        "    def get_next(target_net, next_states):\n",
        "        # values = target_net(next_states).max(dim=1)[0].detach()\n",
        "        values = target_net(next_states)\n",
        "        return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "id": "8f80636d",
      "metadata": {
        "id": "8f80636d"
      },
      "outputs": [],
      "source": [
        "def get_moving_avg(values, period=25):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(\n",
        "            dim=1).flatten(start_dim=0)\n",
        "    else:\n",
        "        moving_avg = torch.zeros_like(values)\n",
        "    return moving_avg.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "id": "c6978e03",
      "metadata": {
        "id": "c6978e03"
      },
      "outputs": [],
      "source": [
        "def plot(values, moving_avg_period=25):\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    plt.title(\"Training...\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Duration\")\n",
        "    plt.plot(values)\n",
        "\n",
        "    moving_avg = get_moving_avg(values, moving_avg_period)\n",
        "    plt.plot(moving_avg)\n",
        "    plt.pause(0.001)\n",
        "    print(\"Episode\", len(values), \"\\n\", moving_avg_period,\n",
        "          \"episode moving avg:\", moving_avg[-1])\n",
        "#     if is_ipython:\n",
        "#         display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "id": "2efe6f43",
      "metadata": {
        "id": "2efe6f43"
      },
      "outputs": [],
      "source": [
        "def extract_tensors(exps):\n",
        "    batch = Experience(*zip(*exps))\n",
        "    \n",
        "    t1 = torch.cat(batch.state)\n",
        "    t2 = torch.cat(batch.action)\n",
        "    t3 = torch.cat(batch.next_state)\n",
        "    t4 = torch.cat(batch.reward)\n",
        "\n",
        "    return (t1, t2, t3, t4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "id": "4b4c6c25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b4c6c25",
        "outputId": "a0e91eb2-378e-4cba-c3e6-800c6458fbac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-240-28085a90cb48>:54: RuntimeWarning: divide by zero encountered in log10\n",
            "  return 10*np.log10(1000*watt)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "gamma = 0.9\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.01\n",
        "target_update = 1\n",
        "memory_size = 100_000\n",
        "lr = 0.001\n",
        "num_episodes = 24*3600\n",
        "# print(bs_power.shape)\n",
        "# env = Environment(bs_loc, bs_power, bs_blackout,10, 0, n_actions, grid_width, grid_height, 4)\n",
        "env = EnvironmentCluster(bs_loc, bs_power, bs_blackout,10, 0, n_actions, grid_width, grid_height)\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "# agent = Agent(strategy, env.get_output_dim()-1) # 2 actions 1 bs\n",
        "agent = Agent(strategy, env.get_output_dim()) #any condition not the above\n",
        "memory = ReplayMemory(memory_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "id": "3131c891",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3.392780042870866, 3.392780042870866)\n",
            "[False False False False False False False False False False False False\n",
            " False False False False]\n"
          ]
        }
      ],
      "source": [
        "env.apply_blackouts(0)\n",
        "for i in range(16):\n",
        "    env.set_action(i, 1)\n",
        "\n",
        "print(env.get_reward())\n",
        "print(env.get_state())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "id": "4c91d8a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n",
            "0\n",
            "10800\n",
            "21600\n",
            "25201\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-231-fb0f61da1657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_blackouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mrewards_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-213-b10a41a43023>\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mpower_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mbit_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_from_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_speed_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_cost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mbr_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-213-b10a41a43023>\u001b[0m in \u001b[0;36mbit_rate_from_grid\u001b[0;34m(self, powers)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbit_rate_from_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpowers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mmax_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbm_to_watt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpowers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0minterference_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbm_to_watt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpowers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_power\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bit_rate_sinr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterference_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     37\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     38\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(len(baseline_times))\n",
        "rewards_history = []\n",
        "for i in baseline_times:\n",
        "    print(i)\n",
        "    env.apply_blackouts(i)\n",
        "    env.apply_baseline()\n",
        "    r, _ = env.get_reward()\n",
        "    rewards_history.append([i, r])\n",
        "\n",
        "print(*rewards_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "id": "GSY7sOUfUFtJ",
      "metadata": {
        "id": "GSY7sOUfUFtJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "# policy_net = DQN(env.get_input_dim(), env.get_output_dim()-1) # 2 actions 1 bs\n",
        "# target_net = DQN(env.get_input_dim(), env.get_output_dim()-1) # 2 actions 1 bs\n",
        "policy_net = DQN(env.get_input_dim(), env.get_output_dim()) # anything else\n",
        "target_net = DQN(env.get_input_dim(), env.get_output_dim()) # anything else\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
        "print(env.get_input_dim())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "id": "64197bb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.392780042870866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-240-28085a90cb48>:54: RuntimeWarning: divide by zero encountered in log10\n",
            "  return 10*np.log10(1000*watt)\n"
          ]
        }
      ],
      "source": [
        "# env = Environment(bs_loc, bs_power, bs_blackout,10, 0, n_actions, grid_width, grid_height, 4)\n",
        "env = EnvironmentCluster(bs_loc, bs_power, bs_blackout,10, 0, n_actions, grid_width, grid_height)\n",
        "num_episodes = 24*3600\n",
        "step = 60\n",
        "print(env.reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "id": "1fe6c05a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# target_update = 1\n",
        "target_update = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "id": "d7637541",
      "metadata": {
        "id": "d7637541"
      },
      "outputs": [],
      "source": [
        "# kpis = [env.reward]\n",
        "# for episode in range(0, num_episodes+1, step):\n",
        "#     ep_timer = time.time()\n",
        "#     print(\"\\n\\nEPISODE_{:03d}:\".format(episode+1))\n",
        "    \n",
        "#     env.apply_blackouts(episode+1)\n",
        "    \n",
        "#     for i in range(env.n_bs):\n",
        "#         timer = time.time()\n",
        "#         state = torch.tensor(env.get_state(i), dtype=torch.float).unsqueeze(0)\n",
        "#         # print(time.time() - timer)\n",
        "\n",
        "#         action = agent.select_action(state, policy_net)\n",
        "#         # print(action)\n",
        "#         # print(time.time() - timer)\n",
        "#         _, action, next_state, reward, kpi = env.make_action(i, action)\n",
        "#         # print(time.time() - timer)\n",
        "        \n",
        "#         kpis.append(kpi)\n",
        "\n",
        "#         reward = torch.tensor([reward], dtype=torch.float)\n",
        "#         next_state = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
        "#         memory.push(Experience(state, action, next_state, reward))\n",
        "#         state = next_state\n",
        "\n",
        "#         if memory.can_provide_sample(batch_size):\n",
        "#             exps = memory.sample(batch_size)\n",
        "#             states, actions, next_states, rewards = extract_tensors(exps)\n",
        "#             current_q_values = QValues.get_current(policy_net, states, actions)\n",
        "#             next_q_values = QValues.get_next(target_net, next_states)\n",
        "#             target_q_values = (next_q_values * gamma) + rewards\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             loss = F.smooth_l1_loss(\n",
        "#                 current_q_values, target_q_values.unsqueeze(1))\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#         print(\"\\tBS_{:02d} DONE IN {} SECONDS, reward {}, action {}, kpi {}\".format(i+1, time.time() - timer, reward[0], action[0], kpi))\n",
        "\n",
        "#     if episode % target_update == 0:\n",
        "#         target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "#     timer2 = time.time()\n",
        "#     env.save_bit_rate_plot(episode+1)\n",
        "#     print(\"EPISODE_{:03d} DONE IN {} SECONDS, WITH KPI {}, PLOTTING TOOK {}\".format(episode+1, time.time() - ep_timer, kpi,time.time() - timer2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPISODE_001:\n",
            "tensor([[1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]])\n",
            "EPISODE_001 DONE IN 4.003678560256958 SECONDS, reward 3.1619160175323486\n",
            "EPISODE_061:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahmed/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPISODE_002 DONE IN 4.057104587554932 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_121:\n",
            "tensor([[1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1]])\n",
            "EPISODE_003 DONE IN 4.234703779220581 SECONDS, reward 3.730008363723755\n",
            "EPISODE_181:\n",
            "tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]])\n",
            "EPISODE_004 DONE IN 3.849219799041748 SECONDS, reward 3.6292293071746826\n",
            "EPISODE_241:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_005 DONE IN 4.081794023513794 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_301:\n",
            "tensor([[1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
            "EPISODE_006 DONE IN 4.153817653656006 SECONDS, reward 2.9879627227783203\n",
            "EPISODE_361:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]])\n",
            "EPISODE_007 DONE IN 4.023003816604614 SECONDS, reward 3.029538869857788\n",
            "EPISODE_421:\n",
            "tensor([[0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1]])\n",
            "EPISODE_008 DONE IN 4.125831127166748 SECONDS, reward 3.1514317989349365\n",
            "EPISODE_481:\n",
            "tensor([[1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0]])\n",
            "EPISODE_009 DONE IN 4.173980951309204 SECONDS, reward 3.137944221496582\n",
            "EPISODE_541:\n",
            "tensor([[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]])\n",
            "EPISODE_010 DONE IN 3.807054281234741 SECONDS, reward 3.5068373680114746\n",
            "EPISODE_601:\n",
            "tensor([[0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]])\n",
            "EPISODE_011 DONE IN 3.8250415325164795 SECONDS, reward 2.727203369140625\n",
            "EPISODE_661:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_012 DONE IN 4.091788291931152 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_721:\n",
            "tensor([[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0]])\n",
            "EPISODE_013 DONE IN 3.9709181785583496 SECONDS, reward 3.241389036178589\n",
            "EPISODE_781:\n",
            "tensor([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]])\n",
            "EPISODE_014 DONE IN 3.8418753147125244 SECONDS, reward 3.0886807441711426\n",
            "EPISODE_841:\n",
            "tensor([[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0]])\n",
            "EPISODE_015 DONE IN 3.9585914611816406 SECONDS, reward 2.627812147140503\n",
            "EPISODE_901:\n",
            "tensor([[0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0]])\n",
            "EPISODE_016 DONE IN 3.9657859802246094 SECONDS, reward 3.733287811279297\n",
            "EPISODE_961:\n",
            "tensor([[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0]])\n",
            "EPISODE_017 DONE IN 4.0157470703125 SECONDS, reward 4.016772270202637\n",
            "EPISODE_1021:\n",
            "tensor([[0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1]])\n",
            "EPISODE_018 DONE IN 3.962592601776123 SECONDS, reward 3.5310919284820557\n",
            "EPISODE_1081:\n",
            "tensor([[1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0]])\n",
            "EPISODE_019 DONE IN 4.095261573791504 SECONDS, reward 3.428647041320801\n",
            "EPISODE_1141:\n",
            "tensor([[0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1]])\n",
            "EPISODE_020 DONE IN 4.168413162231445 SECONDS, reward 2.451216697692871\n",
            "EPISODE_1201:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_021 DONE IN 4.22800612449646 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_1261:\n",
            "tensor([[1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1]])\n",
            "EPISODE_022 DONE IN 4.078648090362549 SECONDS, reward 3.522775411605835\n",
            "EPISODE_1321:\n",
            "tensor([[0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
            "EPISODE_023 DONE IN 4.249020099639893 SECONDS, reward 2.938089609146118\n",
            "EPISODE_1381:\n",
            "tensor([[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]])\n",
            "EPISODE_024 DONE IN 3.9881441593170166 SECONDS, reward 3.811432361602783\n",
            "EPISODE_1441:\n",
            "tensor([[0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]])\n",
            "EPISODE_025 DONE IN 4.133201599121094 SECONDS, reward 2.93023943901062\n",
            "EPISODE_1501:\n",
            "tensor([[1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1]])\n",
            "EPISODE_026 DONE IN 4.157612323760986 SECONDS, reward 3.606921911239624\n",
            "EPISODE_1561:\n",
            "tensor([[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]])\n",
            "EPISODE_027 DONE IN 3.978583812713623 SECONDS, reward 4.610354423522949\n",
            "EPISODE_1621:\n",
            "tensor([[1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1]])\n",
            "EPISODE_028 DONE IN 4.160686016082764 SECONDS, reward 3.0247175693511963\n",
            "EPISODE_1681:\n",
            "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]])\n",
            "EPISODE_029 DONE IN 3.860656261444092 SECONDS, reward 4.029338359832764\n",
            "EPISODE_1741:\n",
            "tensor([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]])\n",
            "EPISODE_030 DONE IN 4.146465063095093 SECONDS, reward 4.839016437530518\n",
            "EPISODE_1801:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_031 DONE IN 3.9714291095733643 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_1861:\n",
            "tensor([[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1]])\n",
            "EPISODE_032 DONE IN 3.8446497917175293 SECONDS, reward 2.7441930770874023\n",
            "EPISODE_1921:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_033 DONE IN 4.021158218383789 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_1981:\n",
            "tensor([[1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1]])\n",
            "EPISODE_034 DONE IN 4.1842122077941895 SECONDS, reward 3.5618224143981934\n",
            "EPISODE_2041:\n",
            "tensor([[1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]])\n",
            "EPISODE_035 DONE IN 3.849390983581543 SECONDS, reward 3.4433350563049316\n",
            "EPISODE_2101:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_036 DONE IN 4.054436445236206 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_2161:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_037 DONE IN 3.9705233573913574 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_2221:\n",
            "tensor([[0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]])\n",
            "EPISODE_038 DONE IN 3.9753808975219727 SECONDS, reward 3.0028235912323\n",
            "EPISODE_2281:\n",
            "tensor([[1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0]])\n",
            "EPISODE_039 DONE IN 4.331372976303101 SECONDS, reward 2.9111168384552\n",
            "EPISODE_2341:\n",
            "tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]])\n",
            "EPISODE_040 DONE IN 4.0109944343566895 SECONDS, reward 3.5416531562805176\n",
            "EPISODE_2401:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_041 DONE IN 4.136168003082275 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_2461:\n",
            "tensor([[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0]])\n",
            "EPISODE_042 DONE IN 4.110291004180908 SECONDS, reward 3.6650965213775635\n",
            "EPISODE_2521:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_043 DONE IN 4.0692994594573975 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_2581:\n",
            "tensor([[1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1]])\n",
            "EPISODE_044 DONE IN 4.1421120166778564 SECONDS, reward 3.577117443084717\n",
            "EPISODE_2641:\n",
            "tensor([[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]])\n",
            "EPISODE_045 DONE IN 4.044579029083252 SECONDS, reward 2.6981871128082275\n",
            "EPISODE_2701:\n",
            "tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]], dtype=torch.int32)\n",
            "EPISODE_046 DONE IN 4.16660213470459 SECONDS, reward 3.8795359134674072\n",
            "EPISODE_2761:\n",
            "tensor([[1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1]])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-263-c5f74e636c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtimer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_bit_rate_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPISODE_{:03d} DONE IN {} SECONDS, reward {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mep_timer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-28085a90cb48>\u001b[0m in \u001b[0;36msave_bit_rate_plot\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mpower_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mbit_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_from_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# plt.contourf(points[:,:,0], points[:,:,1], bit_rate, 100, cmap = plt.cm.jet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-28085a90cb48>\u001b[0m in \u001b[0;36mpower_grid\u001b[0;34m(self, bs_loc, bs_power, points)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mpowers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mpowers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpower\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpowers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2558\u001b[0m             \u001b[0;31m# special case for speedup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0;31m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m         \u001b[0;31m# are valid for vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "kpis = [env.reward]\n",
        "for episode in range(0, num_episodes+1, step):\n",
        "    ep_timer = time.time()\n",
        "    print(\"EPISODE_{:03d}:\".format(episode+1))\n",
        "    \n",
        "    env.apply_blackouts(episode+1)\n",
        "\n",
        "    state = torch.tensor(env.get_state(), dtype=torch.float).unsqueeze(0)\n",
        "    # print(state.shape)\n",
        "    # print(time.time() - timer)\n",
        "\n",
        "    action = agent.select_action(state, policy_net)\n",
        "    # print(time.time() - timer)\n",
        "    print(action)\n",
        "    _, action, next_state, reward, kpi = env.make_action(action[0])\n",
        "    # print(time.time() - timer)\n",
        "    \n",
        "    kpis.append(kpi)\n",
        "\n",
        "    reward = torch.tensor([reward], dtype=torch.float)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
        "    memory.push(Experience(state, action, next_state, reward))\n",
        "    state = next_state\n",
        "\n",
        "    if memory.can_provide_sample(batch_size):\n",
        "        exps = memory.sample(batch_size)\n",
        "        states, actions, next_states, rewards = extract_tensors(exps)\n",
        "        current_q_values = QValues.get_current(policy_net, states, actions)\n",
        "        # print(current_q_values.shape)\n",
        "        next_q_values = QValues.get_next(target_net, next_states)\n",
        "        # print(next_q_values.shape)\n",
        "        # print(rewards.repeat((16,1)).T[0])\n",
        "        target_q_values = (next_q_values * gamma) + rewards.repeat((16,1)).T\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = F.smooth_l1_loss(\n",
        "            current_q_values, target_q_values)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    if episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    timer2 = time.time()\n",
        "    env.save_bit_rate_plot(episode+1)\n",
        "    print(\"EPISODE_{:03d} DONE IN {} SECONDS, reward {}\".format(episode//step + 1, time.time() - ep_timer, reward[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "28aee0a3",
      "metadata": {
        "id": "28aee0a3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 2321 \n",
            " 50 episode moving avg: nan\n"
          ]
        }
      ],
      "source": [
        "plot(kpis, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6aca67",
      "metadata": {
        "id": "7c6aca67"
      },
      "outputs": [],
      "source": [
        "env.get_reward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "id": "rRujSA5PcXxX",
      "metadata": {
        "id": "rRujSA5PcXxX"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32, 32)\n",
        "        self.out = nn.Linear(32, output_size)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], 1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32, 32)\n",
        "        self.out = nn.Linear(32, output_size)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = torch.sigmoid(self.out(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "id": "CwcrX8E0cXxY",
      "metadata": {
        "id": "CwcrX8E0cXxY"
      },
      "outputs": [],
      "source": [
        "# Taken from https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
        "\n",
        "# class OUNoise(object):\n",
        "#     def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
        "#         self.mu           = mu\n",
        "#         self.theta        = theta\n",
        "#         self.sigma        = max_sigma\n",
        "#         self.max_sigma    = max_sigma\n",
        "#         self.min_sigma    = min_sigma\n",
        "#         self.decay_period = decay_period\n",
        "#         self.action_dim   = action_space.shape[0]\n",
        "#         self.low          = action_space.low\n",
        "#         self.high         = action_space.high\n",
        "#         self.reset()\n",
        "        \n",
        "#     def reset(self):\n",
        "#         self.state = np.ones(self.action_dim) * self.mu\n",
        "        \n",
        "#     def evolve_state(self):\n",
        "#         x  = self.state\n",
        "#         dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
        "#         self.state = x + dx\n",
        "#         return self.state\n",
        "    \n",
        "#     def get_action(self, action, t=0): \n",
        "#         ou_state = self.evolve_state()\n",
        "#         self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
        "#         return np.clip(action + ou_state, self.low, self.high)\n",
        "\n",
        "# THIS IS USED FOR CONTINUOUS ACTION SPACE AS NOISE FOR EXPLORATION VS EXPLOITATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "id": "ZR8X5-lLcXxY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR8X5-lLcXxY",
        "outputId": "be7da46f-9385-4599-b911-03edefbdd900"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-240-28085a90cb48>:54: RuntimeWarning: divide by zero encountered in log10\n",
            "  return 10*np.log10(1000*watt)\n"
          ]
        }
      ],
      "source": [
        "# env2 = Environment(bs_loc, bs_power, bs_blackout,10, 0, n_actions, grid_width, grid_height, 4)\n",
        "env2 = EnvironmentCluster(bs_loc, bs_power, bs_blackout,10, 0, n_actions, grid_width, grid_height)\n",
        "strategy2 = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "agent2 = Agent(strategy2, env.get_output_dim())\n",
        "memory2 = ReplayMemory(memory_size)\n",
        "\n",
        "actor = Actor(env2.get_input_dim(), env2.get_output_dim())\n",
        "actor_target = Actor(env2.get_input_dim(), env2.get_output_dim())\n",
        "critic = Critic(env2.get_input_dim() + env2.get_output_dim(), env2.get_output_dim())\n",
        "critic_target = Critic(env2.get_input_dim() + env2.get_output_dim(), env2.get_output_dim())\n",
        "\n",
        "for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "\n",
        "\n",
        "critic_criterion  = nn.MSELoss()\n",
        "actor_optimizer  = optim.Adam(actor.parameters(), lr=1e-4)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "tau=1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "id": "CipmunBtcXxY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CipmunBtcXxY",
        "outputId": "eb390e47-d661-4cd7-c9c2-39cf56035b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EPISODE_001:\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "get_state() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-270-a972c3041549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_bs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_state() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "# kpis2 = []\n",
        "# for episode in range(0, num_episodes+1, step):\n",
        "#     ep_timer = time.time()\n",
        "#     print(\"\\n\\nEPISODE_{:03d}:\".format(episode+1))\n",
        "    \n",
        "#     env2.apply_blackouts(episode+1)\n",
        "    \n",
        "#     for i in range(env2.n_bs):\n",
        "#         timer = time.time()\n",
        "#         state = torch.tensor(env2.get_state(i), dtype=torch.float).unsqueeze(0)\n",
        "\n",
        "#         action = agent2.select_action(state, actor) \n",
        "#         _, action, next_state, reward, kpi = env2.make_action(i, action)\n",
        "        \n",
        "#         kpis2.append(kpi) #\n",
        "\n",
        "#         reward = torch.tensor([reward], dtype=torch.float)\n",
        "#         next_state = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
        "#         memory2.push(Experience(state, action, next_state, reward))\n",
        "#         state = next_state\n",
        "\n",
        "#         if memory2.can_provide_sample(batch_size):\n",
        "#             exps = memory2.sample(batch_size)\n",
        "#             states, actions, next_states, rewards = extract_tensors(exps)\n",
        "\n",
        "#             actions_onehot = actions.numpy()\n",
        "#             actions_onehot = (np.arange(env2.get_output_dim()) == actions_onehot[:,None]).astype(np.float32)\n",
        "#             actions_onehot = torch.from_numpy(actions_onehot)\n",
        "            \n",
        "#             Qvals = critic.forward(states, actions_onehot)\n",
        "#             next_actions = actor_target.forward(next_states)\n",
        "#             next_Q = critic_target.forward(next_states, next_actions.detach())\n",
        "#             Qprime = rewards.unsqueeze(1) + gamma * next_Q\n",
        "#             critic_loss = critic_criterion(Qvals, Qprime)\n",
        "\n",
        "#             # Actor loss\n",
        "#             policy_loss = -critic.forward(states, actor.forward(states)).mean()\n",
        "            \n",
        "#             # update networks\n",
        "#             actor_optimizer.zero_grad()\n",
        "#             policy_loss.backward()\n",
        "#             actor_optimizer.step()\n",
        "\n",
        "#             critic_optimizer.zero_grad()\n",
        "#             critic_loss.backward() \n",
        "#             critic_optimizer.step()\n",
        "\n",
        "#         print(\"\\tBS_{:02d} DONE IN {} SECONDS, reward {}, action {}, kpi {}\".format(i+1, time.time() - timer, reward[0], action[0], kpi))\n",
        "\n",
        "#     for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
        "#         target_param.data.copy_(param.data * tau + target_param.data * (1.0 - tau))\n",
        "\n",
        "#     for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
        "#         target_param.data.copy_(param.data * tau + target_param.data * (1.0 - tau))\n",
        "\n",
        "#     print(\"\\tBS_{:02d} DONE IN {} SECONDS, reward {}\".format(i+1, time.time() - ep_timer, kpi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "id": "1514b521",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EPISODE_001:\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (16,) (1001,1001) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-272-289222661f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mkpis2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-28085a90cb48>\u001b[0m in \u001b[0;36mmake_action\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbulk_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mkpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-28085a90cb48>\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mpower_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mbit_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_from_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_speed_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_cost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-28085a90cb48>\u001b[0m in \u001b[0;36mpower_grid\u001b[0;34m(self, bs_loc, bs_power, points)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mpowers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpower\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpowers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16,) (1001,1001) "
          ]
        }
      ],
      "source": [
        "kpis2 = []\n",
        "for episode in range(0, num_episodes+1, step):\n",
        "    ep_timer = time.time()\n",
        "    print(\"\\n\\nEPISODE_{:03d}:\".format(episode+1))\n",
        "    \n",
        "    env2.apply_blackouts(episode+1)\n",
        "    state = torch.tensor(env2.get_state(), dtype=torch.float)\n",
        "\n",
        "    action = agent2.select_action(state, actor) \n",
        "    _, action, next_state, reward, kpi = env2.make_action(action)\n",
        "    \n",
        "    kpis2.append(kpi) \n",
        "\n",
        "    reward = torch.tensor([reward], dtype=torch.float)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "    memory2.push(Experience(state, action, next_state, reward))\n",
        "    state = next_state\n",
        "\n",
        "    if memory2.can_provide_sample(batch_size):\n",
        "        exps = memory2.sample(batch_size)\n",
        "        states, actions, next_states, rewards = extract_tensors(exps)\n",
        "\n",
        "        actions_onehot = actions.numpy()\n",
        "        # actions_onehot = (np.arange(env2.get_output_dim()) == actions_onehot[:,None]).astype(np.float32)\n",
        "        # actions_onehot = torch.from_numpy(actions_onehot)\n",
        "        \n",
        "        Qvals = critic.forward(states, actions_onehot)\n",
        "        next_actions = actor_target.forward(next_states)\n",
        "        next_Q = critic_target.forward(next_states, next_actions.detach())\n",
        "        Qprime = rewards.unsqueeze(1) + gamma * next_Q\n",
        "        critic_loss = critic_criterion(Qvals, Qprime)\n",
        "\n",
        "        # Actor loss\n",
        "        policy_loss = -critic.forward(states, actor.forward(states)).mean()\n",
        "        \n",
        "        # update networks\n",
        "        actor_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        actor_optimizer.step()\n",
        "\n",
        "        critic_optimizer.zero_grad()\n",
        "        critic_loss.backward() \n",
        "        critic_optimizer.step()\n",
        "\n",
        "    for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
        "        target_param.data.copy_(param.data * tau + target_param.data * (1.0 - tau))\n",
        "\n",
        "    for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
        "        target_param.data.copy_(param.data * tau + target_param.data * (1.0 - tau))\n",
        "\n",
        "    print(\"\\tBS_{:02d} DONE IN {} SECONDS, reward {}\".format(i+1, time.time() - ep_timer, kpi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c3abc179",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "a = np.array([0,0,0])\n",
        "b = a + 1\n",
        "print(np.column_stack((a,b)).ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff4aaeed",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Copy of Copy of propagation loss sinr.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e42243f72fa198d5d7b499bc77ec7a85778f1261c8f7ca8e772194d316f1b31e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
