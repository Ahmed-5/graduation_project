{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Copy of propagation loss sinr.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "interpreter": {
      "hash": "e42243f72fa198d5d7b499bc77ec7a85778f1261c8f7ca8e772194d316f1b31e"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "random.seed(154012)\n",
        "np.random.seed(154012)\n",
        "torch.manual_seed(154012)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb8289e2630>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "metadata": {
        "id": "d559492b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "blackout_choices = np.array([\n",
        "    [3*3600, 4*3600],\n",
        "    [6*3600, 4*3600],\n",
        "    [14*3600, 4*3600],\n",
        "    [19*3600, 4*3600],\n",
        "])\n",
        "\n",
        "fc = 2.6 #2.6 GHz\n",
        "bandwidth = 1e7 #10 MHz\n",
        "temp = 40 + 273.15 # 40 celsius in kelvin\n",
        "boltz = 1.381e-23\n",
        "epsilon = 1e-7\n",
        "row_bs = 2\n",
        "col_bs = 2\n",
        "mean_dist = 600\n",
        "grid_width = mean_dist*(row_bs+2)\n",
        "grid_height = mean_dist*(col_bs+2)\n",
        "bs_loc = np.mgrid[mean_dist: grid_width-mean_dist:mean_dist, mean_dist: grid_height-mean_dist:mean_dist]\n",
        "bs_loc = bs_loc.reshape((2,-1)).T\n",
        "bs_loc += mean_dist//10 - np.random.randint(0, 2*mean_dist//10, bs_loc.shape)\n",
        "\n",
        "bs_power = np.array([40. for i in bs_loc])\n",
        "\n",
        "print(bs_loc.shape)\n",
        "\n",
        "bs_blackout = blackout_choices[np.random.randint(0,3, (row_bs*col_bs))]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 2)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21eb46de",
        "outputId": "5afa89ef-6fca-4036-f1ae-598e0cae76d0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# class Environment():\n",
        "#     def __init__(self, bs_loc, bs_power, bs_blackouts, max_power, min_power, n_actions, grid_width, grid_height, n_nearest):\n",
        "#         self.bs_loc = bs_loc\n",
        "#         self.bs_power = bs_power\n",
        "#         self.electricity = np.zeros_like(bs_power)\n",
        "#         self.bs_blackout_start = bs_blackouts[:,0]\n",
        "#         self.bs_blackout_end = np.sum(bs_blackouts, axis=-1)\n",
        "#         self.apply_blackouts(0)\n",
        "#         self.max_power = max_power # in watt\n",
        "#         self.min_power = min_power\n",
        "#         self.n_actions = n_actions\n",
        "#         self.grid_width = grid_width\n",
        "#         self.grid_height = grid_height\n",
        "#         self.points = np.mgrid[0:grid_width, 0:grid_height]\n",
        "#         self.points = np.stack(self.points, axis=-1)\n",
        "#         self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
        "#         self.bit_rate = self.bit_rate_from_grid()\n",
        "#         self.reward = self.get_reward()\n",
        "#         self.actions = self.watt_to_dbm(np.linspace(min_power, max_power, n_actions))\n",
        "#         self.n_nearest = min(n_nearest, bs_power.shape[0])\n",
        "#         self.n_bs = bs_power.shape[0]\n",
        "        \n",
        "        \n",
        "    \n",
        "#     def get_input_dim(self):\n",
        "#         return self.n_nearest*4 # after adding has electricity attrib 3 will be changed to 4\n",
        "    \n",
        "#     def get_output_dim(self):\n",
        "#         return self.n_actions\n",
        "    \n",
        "#     def apply_blackouts(self, time):\n",
        "#         self.electricity = (time>=self.bs_blackout_start) & (time<=self.bs_blackout_end)\n",
        "    \n",
        "#     def make_action(self, bs_index, action):\n",
        "#         state = self.get_state(bs_index) \n",
        "#         self.bs_power[bs_index] = self.actions[action]\n",
        "#         self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
        "#         self.bit_rate = self.bit_rate_from_grid()\n",
        "#         next_state = self.get_state(bs_index)\n",
        "#         reward = self.get_reward()\n",
        "#         return state, action, next_state, reward\n",
        "        \n",
        "#     def watt_to_dbm(self, watt):\n",
        "#         return 10*np.log10(1000*watt)\n",
        "\n",
        "#     def dbm_to_watt(self, dbm):\n",
        "#         return np.power(10, dbm/10)/1000\n",
        "\n",
        "#     def path_loss(self, distance, frequency=fc):\n",
        "#     #     return 36.7*np.log10(distance) + 47.7 + 26*np.log10(frequency)\n",
        "#         return 35*np.log10(distance) + 35.7\n",
        "\n",
        "#     def power_grid(self, bs_loc, bs_power, points):\n",
        "#         powers = []\n",
        "#         for loc, power in zip(bs_loc, bs_power):\n",
        "#             distance = np.linalg.norm(loc - points+epsilon, axis=-1)\n",
        "#             powers.append(power - self.path_loss(distance))\n",
        "#         return np.stack(powers, axis=-1)\n",
        "\n",
        "#     def get_bit_rate_sinr(self, signal, interference, bandwidth=bandwidth, temp=temp):\n",
        "#         return bandwidth*np.log10(1+ (signal/(interference + temp*boltz*bandwidth)))\n",
        "    \n",
        "#     def bit_rate_from_grid(self):\n",
        "#         max_power = self.dbm_to_watt(self.powers.max(axis=-1))\n",
        "#         interference_power = self.dbm_to_watt(self.powers).sum(axis=-1) - max_power\n",
        "#         return self.get_bit_rate_sinr(max_power, interference_power)\n",
        "\n",
        "#     def bit_rate_cost_function(self, b_rate):\n",
        "#         total_points = 1\n",
        "#         for i in b_rate.shape:\n",
        "#             total_points *= i\n",
        "\n",
        "#         under_1mb = b_rate<(1024**2)*8\n",
        "#         under_1mb = under_1mb.sum()\n",
        "#         min_speed = b_rate.min()/(1024*1024*8) #1MB\n",
        "#         return 10*under_1mb/total_points + 1/(min_speed+epsilon)\n",
        "\n",
        "#     def get_reward(self):\n",
        "#         constant = 10\n",
        "#         br_cost = self.bit_rate_cost_function(self.bit_rate)\n",
        "#         print(self.electricity.shape)\n",
        "#         print(self.dbm_to_watt(self.bs_power))\n",
        "#         elec_cost = np.inner(self.electricity, self.dbm_to_watt(self.bs_power))\n",
        "#         return 10 - br_cost - elec_cost\n",
        "    \n",
        "#     def get_state(self, bs_index):\n",
        "#         x,y = self.bs_loc[bs_index]\n",
        "#         powers = self.powers[np.round(x),np.round(y),:].reshape(-1)\n",
        "#         indecies = np.argsort(powers)[::-1][:self.n_nearest]\n",
        "#         bs_index_posistion = np.where(indecies == bs_index)[0]\n",
        "        \n",
        "#         if bs_index_posistion.size > 0:\n",
        "#             indecies = np.delete(indecies, bs_index_posistion)\n",
        "#             indecies = indecies[:self.n_nearest]\n",
        "#         else:\n",
        "#             indecies = indecies[:self.n_nearest-1]\n",
        "            \n",
        "#         bs_power = powers[bs_index]\n",
        "#         state = np.array([powers[bs_index], self.electricity[bs_index], 0, 0])\n",
        "#         nearest_powers = powers[np.array(indecies)]\n",
        "#         nearest_elec = self.electricity[np.array(indecies)]\n",
        "#         diff =  self.bs_loc[np.array(indecies)] - self.bs_loc[bs_index]\n",
        "#         nearest_distances = np.linalg.norm(diff, axis=-1)\n",
        "#         nearest_angles = np.arctan2(diff[:,1].reshape(-1), diff[:,0].reshape(-1))\n",
        "        \n",
        "#         return np.hstack((state, np.stack([nearest_powers, nearest_elec,\n",
        "#                                            nearest_distances, nearest_angles], axis=-1).reshape(-1)))\n",
        "    \n",
        "#     def plot_bit_rate(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "#     def plot_log_bit_rate(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "#     def plot_power_grid(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "#     def plot_log_power_grid(self):\n",
        "#         plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)"
      ],
      "outputs": [],
      "metadata": {
        "id": "GiH0Yg4eyzBh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class Environment():\n",
        "    def __init__(self, bs_loc, bs_power, bs_blackouts, max_power, min_power, n_actions, grid_width, grid_height, n_nearest):\n",
        "        self.bs_loc = bs_loc\n",
        "        self.bs_power = bs_power\n",
        "        self.electricity = np.zeros_like(bs_power) # 0 have electricity, 1 blackout\n",
        "        self.bs_blackout_start = bs_blackouts[:,0]\n",
        "        self.bs_blackout_end = np.sum(bs_blackouts, axis=-1)\n",
        "        self.apply_blackouts(0)\n",
        "        self.max_power = max_power # in watt\n",
        "        self.min_power = min_power\n",
        "        self.n_actions = n_actions\n",
        "        self.grid_width = grid_width\n",
        "        self.grid_height = grid_height\n",
        "        self.reward = self.get_reward()\n",
        "        self.actions = self.watt_to_dbm(np.linspace(min_power, max_power, n_actions))\n",
        "        self.n_nearest = min(n_nearest, bs_power.shape[0])\n",
        "        self.n_bs = bs_power.shape[0]\n",
        "        \n",
        "    def baseline_action(self, bs_index):\n",
        "        if self.electricity[bs_index]:\n",
        "            self.make_action(bs_index, self.actions[0])\n",
        "        else:\n",
        "            self.make_action(bs_index, self.actions[-1])\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        return self.n_nearest*4 # after adding has electricity attrib 3 will be changed to 4\n",
        "    \n",
        "    def get_output_dim(self):\n",
        "        return self.n_actions\n",
        "    \n",
        "    def apply_blackouts(self, time):\n",
        "        self.electricity = (time>=self.bs_blackout_start) & (time<=self.bs_blackout_end)\n",
        "    \n",
        "    def make_action(self, bs_index, action):\n",
        "        state = self.get_state(bs_index) \n",
        "        self.bs_power[bs_index] = self.actions[action]\n",
        "        # print(action, \"-\", self.bs_power)\n",
        "        # self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
        "        # self.bit_rate = self.bit_rate_from_grid()\n",
        "        next_state = self.get_state(bs_index)\n",
        "        reward = self.get_reward()\n",
        "        return state, action, next_state, reward\n",
        "        \n",
        "    def watt_to_dbm(self, watt):\n",
        "        return 10*np.log10(1000*watt)\n",
        "\n",
        "    def dbm_to_watt(self, dbm):\n",
        "        return np.power(10, dbm/10)/1000\n",
        "\n",
        "    def path_loss(self, distance, frequency=fc):\n",
        "    #     return 36.7*np.log10(distance) + 47.7 + 26*np.log10(frequency)\n",
        "        return 35*np.log10(distance+1) + 35.7\n",
        "\n",
        "    def power_grid(self, bs_loc, bs_power, points):\n",
        "        powers = []\n",
        "        for loc, power in zip(bs_loc, bs_power):\n",
        "            distance = np.linalg.norm(loc - points+epsilon, axis=-1)\n",
        "            powers.append(power - self.path_loss(distance))\n",
        "        return np.stack(powers, axis=-1)\n",
        "\n",
        "    def get_bit_rate_sinr(self, signal, interference, bandwidth=bandwidth, temp=temp):\n",
        "        return bandwidth*np.log10(1+ (signal/(interference + temp*boltz*bandwidth)))\n",
        "    \n",
        "    def bit_rate_from_grid(self, powers):\n",
        "        max_power = self.dbm_to_watt(powers.max(axis=-1))\n",
        "        interference_power = self.dbm_to_watt(powers).sum(axis=-1) - max_power\n",
        "        return self.get_bit_rate_sinr(max_power, interference_power)\n",
        "\n",
        "    def bit_rate_cost_function(self, b_rate):\n",
        "        total_points = self.grid_height * self.grid_width\n",
        "\n",
        "        under_1mb = b_rate<(1024**2)*8\n",
        "        under_1mb = under_1mb.sum()\n",
        "        min_speed = b_rate.min()/(1024*1024*8) #1MB\n",
        "        return 10*under_1mb/total_points , 1/(min_speed+epsilon)\n",
        "\n",
        "    def get_reward(self):\n",
        "        constant = 10\n",
        "        br_cost = 0\n",
        "        min_cost = 0\n",
        "        for i in range(0, self.grid_width, 1000):\n",
        "            for j in range(0, self.grid_height, 1000):\n",
        "                points = np.mgrid[i:min(i+1000, grid_width), j:min(j+1000, grid_height)]\n",
        "                points = np.stack(points, axis=-1)\n",
        "                power_grid = self.power_grid(self.bs_loc, self.bs_power, points)\n",
        "                bit_rate = self.bit_rate_from_grid(power_grid)\n",
        "                percentage, min_speed_cost = self.bit_rate_cost_function(bit_rate)\n",
        "                br_cost += percentage\n",
        "                min_cost = max(min_cost, min_speed_cost)\n",
        "        # print(self.electricity.shape)\n",
        "        # print(self.dbm_to_watt(self.bs_power))\n",
        "        elec_cost = np.inner(self.electricity, self.dbm_to_watt(self.bs_power))\n",
        "        return constant - br_cost - elec_cost - min_cost\n",
        "    \n",
        "    def get_state(self, bs_index):\n",
        "        # get the base station location\n",
        "        loc = self.bs_loc[bs_index]\n",
        "\n",
        "        # get the recieved powers from each base station to the current base station\n",
        "        powers = self.power_grid(self.bs_loc, self.bs_power, loc)\n",
        "        # print(\"dbm\")\n",
        "        # print(powers)\n",
        "        powers = self.dbm_to_watt(powers)\n",
        "        # print(\"watt\")\n",
        "        # print(powers)\n",
        "\n",
        "        # choose the base stations that give the most power to this location\n",
        "        indecies = np.argsort(powers)[::-1][:self.n_nearest]\n",
        "\n",
        "        # check if this base station is one of the base stations that gives the max power\n",
        "        bs_index_posistion = np.where(indecies == bs_index)[0]\n",
        "        \n",
        "\n",
        "        # if the base station is one of them we remove it, that cause it is added later to the state\n",
        "        # we do not we this value to be duplicated in the state\n",
        "        if bs_index_posistion.size > 0:\n",
        "            indecies = np.delete(indecies, bs_index_posistion)\n",
        "            indecies = indecies[:self.n_nearest]\n",
        "        else:\n",
        "            indecies = indecies[:self.n_nearest-1]\n",
        "            \n",
        "        # get the power produce by the base station\n",
        "        bs_power = powers[bs_index]\n",
        "        self_power_norm = 1e3\n",
        "        other_power_norm = 1e13\n",
        "\n",
        "        # init state: rows represent base station \n",
        "        # in each row: recieved power, does it have electricity, distance from here, the angle\n",
        "        state = np.array([bs_power*self_power_norm, self.electricity[bs_index], 0, 0])\n",
        "        nearest_powers = other_power_norm*powers[np.array(indecies)]\n",
        "        nearest_elec = self.electricity[np.array(indecies)]\n",
        "        diff =  self.bs_loc[np.array(indecies)] - self.bs_loc[bs_index]\n",
        "        nearest_distances = np.linalg.norm(diff, axis=-1)\n",
        "        nearest_angles = np.arctan2(diff[:,1].reshape(-1), diff[:,0].reshape(-1))\n",
        "        \n",
        "        state = np.hstack((state, np.stack([nearest_powers, nearest_elec,\n",
        "                                           nearest_distances, nearest_angles], axis=-1).reshape(-1)))\n",
        "\n",
        "        # print(state)\n",
        "        return state\n",
        "    \n",
        "    # def plot_bit_rate(self):\n",
        "    #     plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "    # def plot_log_bit_rate(self):\n",
        "    #     plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "    # def plot_power_grid(self):\n",
        "    #     plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
        "        \n",
        "    # def plot_log_power_grid(self):\n",
        "    #     plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "class DQN(nn.Module): # ready\n",
        "    def __init__(self, n_inputs, n_actions):\n",
        "        nn.Module.__init__(self)\n",
        "        self.fc1 = nn.Linear(in_features=n_inputs, out_features=32)\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=32)\n",
        "        self.out = nn.Linear(in_features=32, out_features=n_actions)\n",
        "\n",
        "    def forward(self, t):\n",
        "        t = F.relu(self.fc1(t))\n",
        "        t = F.relu(self.fc2(t))\n",
        "        t = self.out(t)\n",
        "        return t"
      ],
      "outputs": [],
      "metadata": {
        "id": "86a7311b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "Experience = namedtuple(\n",
        "    \"Experience\", (\"state\", \"action\", \"next_state\", \"reward\")) # ready"
      ],
      "outputs": [],
      "metadata": {
        "id": "8a2efd98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "class ReplayMemory(): # ready\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.push_count = 0\n",
        "\n",
        "    def push(self, experience):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory[self.push_count % self.capacity] = experience\n",
        "        self.push_count += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size"
      ],
      "outputs": [],
      "metadata": {
        "id": "278da5c2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "class EpsilonGreedyStrategy(): # ready\n",
        "    def __init__(self, start, end, decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        "\n",
        "    def get_exploration_rate(self, current_step):\n",
        "        return self.end + (self.start - self.end) * np.exp(-1. * current_step * self.decay)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5c69e5d9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "class Agent():\n",
        "    def __init__(self, strategy, num_actions):\n",
        "        self.current_step = 0\n",
        "        self.strategy = strategy\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "    def select_action(self, state, policy_net):\n",
        "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
        "        self.current_step += 1\n",
        "\n",
        "        if rate > random.random():\n",
        "            action = random.randrange(self.num_actions)\n",
        "            return torch.tensor([action])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return policy_net(state).argmax(dim=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "d11c365f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "class QValues():\n",
        "    @staticmethod\n",
        "    def get_current(policy_net, states, actions):\n",
        "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_next(target_net, next_states):\n",
        "        values = target_net(next_states).max(dim=1)[0].detach()\n",
        "        return values"
      ],
      "outputs": [],
      "metadata": {
        "id": "f1943cf4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "def get_moving_avg(values, period=25):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(\n",
        "            dim=1).flatten(start_dim=0)\n",
        "    else:\n",
        "        moving_avg = torch.zeros_like(values)\n",
        "    return moving_avg.numpy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "8f80636d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def plot(values, moving_avg_period=25):\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    plt.title(\"Training...\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Duration\")\n",
        "    plt.plot(values)\n",
        "\n",
        "    moving_avg = get_moving_avg(values, moving_avg_period)\n",
        "    plt.plot(moving_avg)\n",
        "    plt.pause(0.001)\n",
        "    print(\"Episode\", len(values), \"\\n\", moving_avg_period,\n",
        "          \"episode moving avg:\", moving_avg[-1])\n",
        "#     if is_ipython:\n",
        "#         display.clear_output(wait=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "c6978e03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "def extract_tensors(exps):\n",
        "    batch = Experience(*zip(*exps))\n",
        "    \n",
        "    t1 = torch.cat(batch.state)\n",
        "    t2 = torch.cat(batch.action)\n",
        "    t3 = torch.cat(batch.next_state)\n",
        "    t4 = torch.cat(batch.reward)\n",
        "\n",
        "    return (t1, t2, t3, t4)"
      ],
      "outputs": [],
      "metadata": {
        "id": "2efe6f43"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "batch_size = 256\n",
        "gamma = 0.9\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update = 10\n",
        "memory_size = 1000\n",
        "lr = 0.001\n",
        "num_episodes = 5000\n",
        "# print(bs_power.shape)\n",
        "env = Environment(bs_loc, bs_power, bs_blackout,10, 0, 11, grid_width, grid_height, 4)\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "agent = Agent(strategy, env.get_output_dim())\n",
        "memory = ReplayMemory(memory_size)\n",
        "\n",
        "policy_net = DQN(env.get_input_dim(), env.get_output_dim())\n",
        "target_net = DQN(env.get_input_dim(), env.get_output_dim())\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9d324aca4b82>:45: RuntimeWarning: divide by zero encountered in log10\n",
            "  return 10*np.log10(1000*watt)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b4c6c25",
        "outputId": "8ce17b82-0221-44d7-ee53-b9ffed303c0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "rewards_history = []\n",
        "for episode in range(num_episodes):\n",
        "    timer = time.time()\n",
        "    print(\"\\n\\nEPISODE:\", episode+1)\n",
        "    \n",
        "    env.apply_blackouts(episode+1)\n",
        "    \n",
        "    for i in range(env.n_bs):\n",
        "        state = torch.tensor(env.get_state(i), dtype=torch.float).unsqueeze(0)\n",
        "\n",
        "        action = agent.select_action(state, policy_net)\n",
        "        _, action, next_state, reward = env.make_action(i, action)\n",
        "        \n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        reward = torch.tensor([reward], dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
        "        memory.push(Experience(state, action, next_state, reward))\n",
        "        state = next_state\n",
        "\n",
        "        if memory.can_provide_sample(batch_size):\n",
        "            exps = memory.sample(batch_size)\n",
        "            states, actions, next_states, rewards = extract_tensors(exps)\n",
        "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
        "            next_q_values = QValues.get_next(target_net, next_states)\n",
        "            target_q_values = (next_q_values * gamma) + rewards\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.smooth_l1_loss(\n",
        "                current_q_values, target_q_values.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        print(\"EPISODE DONE IN {} SECONDS, reward {}, action {}\".format(time.time() - timer, reward[0], action[0]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EPISODE: 1\n",
            "EPISODE DONE IN 4.158169984817505 SECONDS, reward -7.886629104614258, action 9\n",
            "EPISODE DONE IN 7.599247694015503 SECONDS, reward -7.631948947906494, action 7\n",
            "EPISODE DONE IN 11.000621557235718 SECONDS, reward -7.555418968200684, action 9\n",
            "EPISODE DONE IN 14.397579908370972 SECONDS, reward -20.551748275756836, action 4\n",
            "\n",
            "\n",
            "EPISODE: 2\n",
            "EPISODE DONE IN 3.404390573501587 SECONDS, reward -20.551748275756836, action 9\n",
            "EPISODE DONE IN 6.82259726524353 SECONDS, reward -20.751060485839844, action 8\n",
            "EPISODE DONE IN 10.237659454345703 SECONDS, reward -20.560176849365234, action 8\n",
            "EPISODE DONE IN 13.624441862106323 SECONDS, reward -11.297540664672852, action 7\n",
            "\n",
            "\n",
            "EPISODE: 3\n",
            "EPISODE DONE IN 3.4113612174987793 SECONDS, reward -11.084796905517578, action 6\n",
            "EPISODE DONE IN 6.79003381729126 SECONDS, reward -10.740035057067871, action 5\n",
            "EPISODE DONE IN 10.192148923873901 SECONDS, reward -11.16480827331543, action 2\n",
            "EPISODE DONE IN 13.605501174926758 SECONDS, reward -9.610021591186523, action 8\n",
            "\n",
            "\n",
            "EPISODE: 4\n",
            "EPISODE DONE IN 3.414018392562866 SECONDS, reward -9.741250038146973, action 7\n",
            "EPISODE DONE IN 6.684360980987549 SECONDS, reward -8.813508987426758, action 0\n",
            "EPISODE DONE IN 9.923176050186157 SECONDS, reward -8.357154846191406, action 6\n",
            "EPISODE DONE IN 13.151755332946777 SECONDS, reward -11.648582458496094, action 6\n",
            "\n",
            "\n",
            "EPISODE: 5\n",
            "EPISODE DONE IN 3.400853157043457 SECONDS, reward -11.734521865844727, action 9\n",
            "EPISODE DONE IN 7.224246501922607 SECONDS, reward -13.23920726776123, action 9\n",
            "EPISODE DONE IN 10.486307144165039 SECONDS, reward -12.760015487670898, action 0\n",
            "EPISODE DONE IN 13.763652563095093 SECONDS, reward -8.070145606994629, action 9\n",
            "\n",
            "\n",
            "EPISODE: 6\n",
            "EPISODE DONE IN 3.2416436672210693 SECONDS, reward -7.6997857093811035, action 5\n",
            "EPISODE DONE IN 6.552782297134399 SECONDS, reward -7.6997857093811035, action 9\n",
            "EPISODE DONE IN 9.923574447631836 SECONDS, reward -8.404507637023926, action 8\n",
            "EPISODE DONE IN 13.509164571762085 SECONDS, reward -7.451941967010498, action 10\n",
            "\n",
            "\n",
            "EPISODE: 7\n",
            "EPISODE DONE IN 3.394915819168091 SECONDS, reward -7.254572868347168, action 2\n",
            "EPISODE DONE IN 6.776262998580933 SECONDS, reward -7.322333335876465, action 10\n",
            "EPISODE DONE IN 10.075983762741089 SECONDS, reward -7.19630765914917, action 6\n",
            "EPISODE DONE IN 13.554174423217773 SECONDS, reward -9.267786026000977, action 8\n",
            "\n",
            "\n",
            "EPISODE: 8\n",
            "EPISODE DONE IN 3.4472270011901855 SECONDS, reward -9.267786026000977, action 2\n",
            "EPISODE DONE IN 6.877528667449951 SECONDS, reward -8.92172622680664, action 6\n",
            "EPISODE DONE IN 10.301133155822754 SECONDS, reward -9.001982688903809, action 7\n",
            "EPISODE DONE IN 13.70346713066101 SECONDS, reward -15.031779289245605, action 5\n",
            "\n",
            "\n",
            "EPISODE: 9\n",
            "EPISODE DONE IN 3.3615942001342773 SECONDS, reward -15.576777458190918, action 7\n",
            "EPISODE DONE IN 6.702275514602661 SECONDS, reward -16.04792594909668, action 9\n",
            "EPISODE DONE IN 9.836748361587524 SECONDS, reward -15.105701446533203, action 0\n",
            "EPISODE DONE IN 12.961313009262085 SECONDS, reward -19.502910614013672, action 3\n",
            "\n",
            "\n",
            "EPISODE: 10\n",
            "EPISODE DONE IN 3.003918409347534 SECONDS, reward -18.504859924316406, action 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f15a923ee0e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrewards_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9d324aca4b82>\u001b[0m in \u001b[0;36mmake_action\u001b[0;34m(self, bs_index, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# self.bit_rate = self.bit_rate_from_grid()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9d324aca4b82>\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mpower_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mbit_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_from_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_speed_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_cost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mbr_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9d324aca4b82>\u001b[0m in \u001b[0;36mbit_rate_from_grid\u001b[0;34m(self, powers)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbit_rate_from_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpowers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmax_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbm_to_watt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpowers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0minterference_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbm_to_watt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpowers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_power\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bit_rate_sinr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterference_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9d324aca4b82>\u001b[0m in \u001b[0;36mdbm_to_watt\u001b[0;34m(self, dbm)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdbm_to_watt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpath_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7637541",
        "outputId": "df945bc1-16f3-43a3-881c-adfbf1c9d0fb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plot(rewards_history)"
      ],
      "outputs": [],
      "metadata": {
        "id": "28aee0a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "env.get_reward()"
      ],
      "outputs": [],
      "metadata": {
        "id": "7c6aca67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32, output_size)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], 1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32, output_size)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = torch.tanh(self.linear3(x))\n",
        "\n",
        "        return x"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Taken from https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
        "\n",
        "# class OUNoise(object):\n",
        "#     def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
        "#         self.mu           = mu\n",
        "#         self.theta        = theta\n",
        "#         self.sigma        = max_sigma\n",
        "#         self.max_sigma    = max_sigma\n",
        "#         self.min_sigma    = min_sigma\n",
        "#         self.decay_period = decay_period\n",
        "#         self.action_dim   = action_space.shape[0]\n",
        "#         self.low          = action_space.low\n",
        "#         self.high         = action_space.high\n",
        "#         self.reset()\n",
        "        \n",
        "#     def reset(self):\n",
        "#         self.state = np.ones(self.action_dim) * self.mu\n",
        "        \n",
        "#     def evolve_state(self):\n",
        "#         x  = self.state\n",
        "#         dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
        "#         self.state = x + dx\n",
        "#         return self.state\n",
        "    \n",
        "#     def get_action(self, action, t=0): \n",
        "#         ou_state = self.evolve_state()\n",
        "#         self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
        "#         return np.clip(action + ou_state, self.low, self.high)\n",
        "\n",
        "# THIS IS USED FOR CONTINUOUS ACTION SPACE AS NOISE FOR EXPLORATION VS EXPLOITATION"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "env2 = Environment(bs_loc, bs_power, bs_blackout,10, 0, 11, grid_width, grid_height, 4)\n",
        "strategy2 = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "agent2 = Agent(strategy2, env.get_output_dim())\n",
        "memory2 = ReplayMemory(memory_size)\n",
        "\n",
        "actor = Actor(env2.get_input_dim(), env2.get_output_dim())\n",
        "actor_target = Actor(env2.get_input_dim(), env2.get_output_dim())\n",
        "critic = Critic(env2.get_input_dim() + env2.get_output_dim(), env2.get_output_dim())\n",
        "critic_target = Critic(env2.get_input_dim() + env2.get_output_dim(), env2.get_output_dim())\n",
        "\n",
        "for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "\n",
        "\n",
        "critic_criterion  = nn.MSELoss()\n",
        "actor_optimizer  = optim.Adam(actor.parameters(), lr=1e-4)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "tau=1e-2"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rewards_history2 = []\n",
        "for episode in range(num_episodes):\n",
        "    timer = time.time()\n",
        "    print(\"\\n\\nEPISODE:\", episode+1)\n",
        "    \n",
        "    env.apply_blackouts(episode+1)\n",
        "    \n",
        "    for i in range(env.n_bs):\n",
        "        state = torch.tensor(env.get_state(i), dtype=torch.float).unsqueeze(0)\n",
        "\n",
        "        action = agent.select_action(state, actor) \n",
        "        _, action, next_state, reward = env.make_action(i, action)\n",
        "        \n",
        "        rewards_history2.append(reward) #\n",
        "\n",
        "        reward = torch.tensor([reward], dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
        "        memory.push(Experience(state, action, next_state, reward))\n",
        "        state = next_state\n",
        "\n",
        "        if memory.can_provide_sample(batch_size):\n",
        "            exps = memory.sample(batch_size)\n",
        "            states, actions, next_states, rewards = extract_tensors(exps)\n",
        "            \n",
        "            Qvals = critic.forward(states, actions)\n",
        "            next_actions = actor_target.forward(next_states)\n",
        "            next_Q = critic_target.forward(next_states, next_actions.detach())\n",
        "            Qprime = rewards + gamma * next_Q\n",
        "            critic_loss = critic_criterion(Qvals, Qprime)\n",
        "\n",
        "            # Actor loss\n",
        "            policy_loss = -critic.forward(states, actor.forward(states)).mean()\n",
        "            \n",
        "            # update networks\n",
        "            actor_optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward() \n",
        "            critic_optimizer.step()\n",
        "\n",
        "            for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
        "                target_param.data.copy_(param.data * tau + target_param.data * (1.0 - tau))\n",
        "        \n",
        "            for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
        "                target_param.data.copy_(param.data * tau + target_param.data * (1.0 - tau))\n",
        "\n",
        "        print(\"EPISODE DONE IN {} SECONDS, reward {}, action {}\".format(time.time() - timer, reward[0], action[0]))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# plt.contourf(env.points[:,:,0], env.points[:,:,1], env.bit_rate, 100, cmap = plt.cm.jet)"
      ],
      "outputs": [],
      "metadata": {
        "id": "09ceeef2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# print(env.bs_power)"
      ],
      "outputs": [],
      "metadata": {
        "id": "623f5b39",
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.linspace(1000, 5000, 5)"
      ],
      "outputs": [],
      "metadata": {
        "id": "50535460"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "09797a75"
      }
    }
  ]
}