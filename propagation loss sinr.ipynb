{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d559492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21eb46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_loc = np.array([[100,100], [900,900], [100, 900], [900, 100]])\n",
    "bs_power = np.array([40., 40., 40., 40.]) #40 watts each\n",
    "fc = 2.6 #2.6 GHz\n",
    "bandwidth = 1e7 #10 MHz\n",
    "temp = 40 + 273.15 # 40 celsius in kelvin\n",
    "boltz = 1.381e-23\n",
    "epsilon = 1e-7\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, bs_loc, bs_power, max_power, min_power, n_actions, grid_width, grid_height, n_nearest):\n",
    "        self.bs_loc = bs_loc\n",
    "        self.bs_power = bs_power\n",
    "        self.electricity = np.zeros_like(bs_power)\n",
    "        self.max_power = max_power # in watt\n",
    "        self.min_power = min_power\n",
    "        self.n_actions = n_actions\n",
    "        self.grid_width = grid_width\n",
    "        self.grid_height = grid_height\n",
    "        self.points = np.mgrid[0:grid_width, 0:grid_height]\n",
    "        self.points = np.stack(self.points, axis=-1)\n",
    "        self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
    "        self.bit_rate = self.bit_rate_from_grid()\n",
    "        self.reward = self.get_reward()\n",
    "        self.actions = self.watt_to_dbm(np.linspace(min_power, max_power, n_actions))\n",
    "        self.n_nearest = min(n_nearest, bs_power.shape[0])\n",
    "        self.n_bs = bs_power.shape[0]\n",
    "        \n",
    "    def get_input_dim(self):\n",
    "        return self.n_nearest*3 # after adding has electricity attrib 3 will be changed to 4\n",
    "    \n",
    "    def get_output_dim(self):\n",
    "        return self.n_actions\n",
    "    \n",
    "    def make_action(self, bs_index, action):\n",
    "        state = self.get_state(bs_index) \n",
    "        self.bs_power[bs_index] = self.actions[action]\n",
    "        self.powers = self.power_grid(self.bs_loc, self.bs_power, self.points)\n",
    "        self.bit_rate = self.bit_rate_from_grid()\n",
    "        next_state = self.get_state(bs_index)\n",
    "        reward = self.get_reward()\n",
    "        return state, action, next_state, reward\n",
    "        \n",
    "    def watt_to_dbm(self, watt):\n",
    "        return 10*np.log10(1000*watt)\n",
    "\n",
    "    def dbm_to_watt(self, dbm):\n",
    "        return np.power(10, dbm/10)/1000\n",
    "\n",
    "    def path_loss(self, distance, frequency=fc):\n",
    "    #     return 36.7*np.log10(distance) + 47.7 + 26*np.log10(frequency)\n",
    "        return 35*np.log10(distance) + 35.7\n",
    "\n",
    "    def power_grid(self, bs_loc, bs_power, points):\n",
    "        powers = []\n",
    "        for loc, power in zip(bs_loc, bs_power):\n",
    "            distance = np.linalg.norm(loc - points+epsilon, axis=-1)\n",
    "            powers.append(power - self.path_loss(distance))\n",
    "        return np.stack(powers, axis=-1)\n",
    "\n",
    "    def get_bit_rate_sinr(self, signal, interference, bandwidth=bandwidth, temp=temp):\n",
    "        return bandwidth*np.log10(1+ (signal/(interference + temp*boltz*bandwidth)))\n",
    "    \n",
    "    def bit_rate_from_grid(self):\n",
    "        max_power = self.dbm_to_watt(self.powers.max(axis=-1))\n",
    "        interference_power = self.dbm_to_watt(self.powers).sum(axis=-1) - max_power\n",
    "        return self.get_bit_rate_sinr(max_power, interference_power)\n",
    "\n",
    "    def bit_rate_cost_function(self, b_rate):\n",
    "        total_points = 1\n",
    "        for i in b_rate.shape:\n",
    "            total_points *= i\n",
    "\n",
    "        under_1mb = b_rate<(1024**2)*8\n",
    "        under_1mb = under_1mb.sum()\n",
    "        min_speed = b_rate.min()/(1024*1024*8) #1MB\n",
    "        return 10*under_1mb/total_points + 1/(min_speed+epsilon)\n",
    "\n",
    "    def get_reward(self):\n",
    "        return 10 - self.bit_rate_cost_function(self.bit_rate) - np.inner(self.electricity, self.dbm_to_watt(self.bs_power))\n",
    "    \n",
    "    def get_state(self, bs_index):\n",
    "        x,y = self.bs_loc[bs_index]\n",
    "        powers = self.powers[np.round(x),np.round(y),:].reshape(-1)\n",
    "        indecies = np.argsort(powers)[::-1][:self.n_nearest]\n",
    "        bs_index_posistion = np.where(indecies == bs_index)[0]\n",
    "        if bs_index_posistion.size > 0:\n",
    "            indecies = np.delete(indecies, bs_index_posistion)\n",
    "            indecies = indecies[:self.n_nearest]\n",
    "        else:\n",
    "            indecies = indecies[:self.n_nearest-1]\n",
    "        bs_power = powers[bs_index]\n",
    "        state = np.array([powers[bs_index], 0, 0])\n",
    "        nearest_powers = powers[np.array(indecies)]\n",
    "        diff =  self.bs_loc[np.array(indecies)] - self.bs_loc[bs_index]\n",
    "        nearest_distances = np.linalg.norm(diff, axis=-1)\n",
    "        nearest_angles = np.arctan2(diff[:,1].reshape(-1), diff[:,0].reshape(-1))\n",
    "        \n",
    "        return np.hstack((state, np.stack([nearest_powers, \n",
    "                                           nearest_distances, nearest_angles], axis=-1).reshape(-1)))\n",
    "    \n",
    "    def plot_bit_rate(self):\n",
    "        plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
    "        \n",
    "    def plot_log_bit_rate(self):\n",
    "        plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
    "        \n",
    "    def plot_power_grid(self):\n",
    "        plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)\n",
    "        \n",
    "    def plot_log_power_grid(self):\n",
    "        plt.contourf(self.points[:,:,0], self.points[:,:,1], self.bit_rate, 100, cmap = plt.cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86a7311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module): # ready\n",
    "    def __init__(self, n_inputs, n_actions):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(in_features=n_inputs, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=n_actions)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a2efd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    \"Experience\", (\"state\", \"action\", \"next_state\", \"reward\")) # ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "278da5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(): # ready\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c69e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy(): # ready\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * np.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d11c365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1943cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        values = target_net(next_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6978e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title(\"Training...\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_avg(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", moving_avg_period,\n",
    "          \"episode moving avg:\", moving_avg[-1])\n",
    "#     if is_ipython:\n",
    "#         display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f80636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_avg(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(\n",
    "            dim=1).flatten(start_dim=0)\n",
    "    else:\n",
    "        moving_avg = torch.zeros_like(values)\n",
    "    return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2efe6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(exps):\n",
    "    batch = Experience(*zip(*exps))\n",
    "    \n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.next_state)\n",
    "    t4 = torch.cat(batch.reward)\n",
    "\n",
    "    return (t1, t2, t3, t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b4c6c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-6b7e6bff6962>:44: RuntimeWarning: divide by zero encountered in log10\n",
      "  return 10*np.log10(1000*watt)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.7\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 1000\n",
    "lr = 0.001\n",
    "num_episodes = 5000\n",
    "\n",
    "env = Environment(bs_loc, bs_power, 10, 0, 9, 1000, 1000, 4)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, env.get_output_dim())\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(env.get_input_dim(), env.get_output_dim())\n",
    "target_net = DQN(env.get_input_dim(), env.get_output_dim())\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7637541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EPISODE: 1\n",
      "EPISODE DONE IN 0.5580413341522217 SECONDS, reward -0.4617145359516144, action 1\n",
      "EPISODE DONE IN 1.1043682098388672 SECONDS, reward -0.22439336776733398, action 6\n",
      "EPISODE DONE IN 1.6632885932922363 SECONDS, reward -1.5816013813018799, action 1\n",
      "EPISODE DONE IN 2.2177176475524902 SECONDS, reward -2.122255563735962, action 6\n",
      "\n",
      "\n",
      "EPISODE: 2\n",
      "EPISODE DONE IN 0.5586962699890137 SECONDS, reward -0.5947555303573608, action 6\n",
      "EPISODE DONE IN 1.1116735935211182 SECONDS, reward -0.7096627354621887, action 5\n",
      "EPISODE DONE IN 1.669276475906372 SECONDS, reward -1.6588608026504517, action 5\n",
      "EPISODE DONE IN 2.2215049266815186 SECONDS, reward -1.0727852582931519, action 4\n",
      "\n",
      "\n",
      "EPISODE: 3\n",
      "EPISODE DONE IN 0.5566411018371582 SECONDS, reward -1.0577448606491089, action 2\n",
      "EPISODE DONE IN 1.109679937362671 SECONDS, reward -1.0577448606491089, action 5\n",
      "EPISODE DONE IN 1.66756010055542 SECONDS, reward -0.8664935827255249, action 6\n",
      "EPISODE DONE IN 2.2203614711761475 SECONDS, reward -0.8664935827255249, action 4\n",
      "\n",
      "\n",
      "EPISODE: 4\n",
      "EPISODE DONE IN 0.5580694675445557 SECONDS, reward -0.8664935827255249, action 2\n",
      "EPISODE DONE IN 1.11224365234375 SECONDS, reward -0.40881168842315674, action 3\n",
      "EPISODE DONE IN 1.677945852279663 SECONDS, reward -0.996392548084259, action 3\n",
      "EPISODE DONE IN 2.2927167415618896 SECONDS, reward -1.3667473793029785, action 3\n",
      "\n",
      "\n",
      "EPISODE: 5\n",
      "EPISODE DONE IN 0.5688912868499756 SECONDS, reward -0.4939455986022949, action 8\n",
      "EPISODE DONE IN 1.1528959274291992 SECONDS, reward -0.32195326685905457, action 4\n",
      "EPISODE DONE IN 1.7368605136871338 SECONDS, reward -0.7496929168701172, action 5\n",
      "EPISODE DONE IN 2.306013345718384 SECONDS, reward -0.5926405787467957, action 2\n",
      "\n",
      "\n",
      "EPISODE: 6\n",
      "EPISODE DONE IN 0.5627727508544922 SECONDS, reward -1.165954828262329, action 1\n",
      "EPISODE DONE IN 1.1209216117858887 SECONDS, reward -0.24066109955310822, action 1\n",
      "EPISODE DONE IN 1.7166800498962402 SECONDS, reward -0.13743415474891663, action 6\n",
      "EPISODE DONE IN 2.28253436088562 SECONDS, reward 0.029975978657603264, action 3\n",
      "\n",
      "\n",
      "EPISODE: 7\n",
      "EPISODE DONE IN 0.5576426982879639 SECONDS, reward -1.330960988998413, action 7\n",
      "EPISODE DONE IN 1.0875585079193115 SECONDS, reward -0.3908901810646057, action 0\n",
      "EPISODE DONE IN 1.6215877532958984 SECONDS, reward -0.3164917528629303, action 7\n",
      "EPISODE DONE IN 2.2031021118164062 SECONDS, reward -0.41878822445869446, action 2\n",
      "\n",
      "\n",
      "EPISODE: 8\n",
      "EPISODE DONE IN 0.5837306976318359 SECONDS, reward -0.3728195130825043, action 6\n",
      "EPISODE DONE IN 1.1200520992279053 SECONDS, reward -0.3728195130825043, action 0\n",
      "EPISODE DONE IN 1.670400857925415 SECONDS, reward -0.9076295495033264, action 4\n",
      "EPISODE DONE IN 2.2516133785247803 SECONDS, reward -0.9076295495033264, action 2\n",
      "\n",
      "\n",
      "EPISODE: 9\n",
      "EPISODE DONE IN 0.541480302810669 SECONDS, reward 0.5345990061759949, action 0\n",
      "EPISODE DONE IN 1.0745408535003662 SECONDS, reward -1.3669699430465698, action 8\n",
      "EPISODE DONE IN 1.6242709159851074 SECONDS, reward -0.450459361076355, action 0\n",
      "EPISODE DONE IN 2.1575777530670166 SECONDS, reward 0.3869994282722473, action 8\n",
      "\n",
      "\n",
      "EPISODE: 10\n",
      "EPISODE DONE IN 0.5386543273925781 SECONDS, reward -0.22750288248062134, action 4\n",
      "EPISODE DONE IN 1.0698182582855225 SECONDS, reward -0.2850998342037201, action 7\n",
      "EPISODE DONE IN 1.6302993297576904 SECONDS, reward -1.4219375848770142, action 4\n",
      "EPISODE DONE IN 2.184677839279175 SECONDS, reward -1.4219375848770142, action 8\n",
      "\n",
      "\n",
      "EPISODE: 11\n",
      "EPISODE DONE IN 0.5612246990203857 SECONDS, reward -1.4219375848770142, action 4\n",
      "EPISODE DONE IN 1.123922348022461 SECONDS, reward -0.6892313361167908, action 4\n",
      "EPISODE DONE IN 1.7434463500976562 SECONDS, reward -1.77614164352417, action 1\n",
      "EPISODE DONE IN 2.2969605922698975 SECONDS, reward -0.8129220008850098, action 4\n",
      "\n",
      "\n",
      "EPISODE: 12\n",
      "EPISODE DONE IN 0.5621793270111084 SECONDS, reward -1.047853946685791, action 3\n",
      "EPISODE DONE IN 1.1169755458831787 SECONDS, reward -0.8334125876426697, action 5\n",
      "EPISODE DONE IN 1.6759371757507324 SECONDS, reward -1.0713145732879639, action 2\n",
      "EPISODE DONE IN 2.230348825454712 SECONDS, reward -1.626902461051941, action 7\n",
      "\n",
      "\n",
      "EPISODE: 13\n",
      "EPISODE DONE IN 0.5596108436584473 SECONDS, reward -1.5686218738555908, action 4\n",
      "EPISODE DONE IN 1.1161093711853027 SECONDS, reward -1.5686218738555908, action 5\n",
      "EPISODE DONE IN 1.7100491523742676 SECONDS, reward -0.44355836510658264, action 8\n",
      "EPISODE DONE IN 2.3221473693847656 SECONDS, reward -0.716701328754425, action 5\n",
      "\n",
      "\n",
      "EPISODE: 14\n",
      "EPISODE DONE IN 0.5954771041870117 SECONDS, reward -0.716701328754425, action 4\n",
      "EPISODE DONE IN 1.1523683071136475 SECONDS, reward -0.286704421043396, action 2\n",
      "EPISODE DONE IN 1.708984375 SECONDS, reward -0.6724832653999329, action 0\n",
      "EPISODE DONE IN 2.2932300567626953 SECONDS, reward -0.330563485622406, action 1\n",
      "\n",
      "\n",
      "EPISODE: 15\n",
      "EPISODE DONE IN 0.5610458850860596 SECONDS, reward -1.5789419412612915, action 1\n",
      "EPISODE DONE IN 1.0969610214233398 SECONDS, reward -0.8147212862968445, action 3\n",
      "EPISODE DONE IN 1.698009729385376 SECONDS, reward -1.138173222541809, action 6\n",
      "EPISODE DONE IN 2.2763473987579346 SECONDS, reward -0.2928343117237091, action 4\n",
      "\n",
      "\n",
      "EPISODE: 16\n",
      "EPISODE DONE IN 0.5633347034454346 SECONDS, reward -1.3650095462799072, action 6\n",
      "EPISODE DONE IN 1.1175882816314697 SECONDS, reward -1.6882023811340332, action 4\n",
      "EPISODE DONE IN 1.6973838806152344 SECONDS, reward -0.14901304244995117, action 0\n",
      "EPISODE DONE IN 2.285432815551758 SECONDS, reward 0.08885727822780609, action 1\n",
      "\n",
      "\n",
      "EPISODE: 17\n",
      "EPISODE DONE IN 0.540485143661499 SECONDS, reward 0.004198141396045685, action 5\n",
      "EPISODE DONE IN 1.0764265060424805 SECONDS, reward 0.27017906308174133, action 8\n",
      "EPISODE DONE IN 1.64585542678833 SECONDS, reward 0.14372172951698303, action 2\n",
      "EPISODE DONE IN 2.2017102241516113 SECONDS, reward -0.7842071056365967, action 7\n",
      "\n",
      "\n",
      "EPISODE: 18\n",
      "EPISODE DONE IN 0.5944440364837646 SECONDS, reward -1.2103625535964966, action 3\n",
      "EPISODE DONE IN 1.1898736953735352 SECONDS, reward -1.626902461051941, action 5\n",
      "EPISODE DONE IN 1.7589507102966309 SECONDS, reward -0.550613284111023, action 6\n",
      "EPISODE DONE IN 2.320361375808716 SECONDS, reward -1.095446228981018, action 4\n",
      "\n",
      "\n",
      "EPISODE: 19\n",
      "EPISODE DONE IN 0.5650434494018555 SECONDS, reward -0.9289527535438538, action 8\n",
      "EPISODE DONE IN 1.1362473964691162 SECONDS, reward -1.1776169538497925, action 4\n",
      "EPISODE DONE IN 1.7046871185302734 SECONDS, reward -0.5365515351295471, action 3\n",
      "EPISODE DONE IN 2.238166093826294 SECONDS, reward 0.042686089873313904, action 0\n",
      "\n",
      "\n",
      "EPISODE: 20\n",
      "EPISODE DONE IN 0.5413501262664795 SECONDS, reward -1.1906092166900635, action 0\n",
      "EPISODE DONE IN 1.0691022872924805 SECONDS, reward -0.5608869194984436, action 7\n",
      "EPISODE DONE IN 1.6000993251800537 SECONDS, reward -1.1390821933746338, action 2\n",
      "EPISODE DONE IN 2.132915496826172 SECONDS, reward -1.8910012245178223, action 3\n",
      "\n",
      "\n",
      "EPISODE: 21\n",
      "EPISODE DONE IN 0.5737650394439697 SECONDS, reward -0.029025094583630562, action 7\n",
      "EPISODE DONE IN 1.1410391330718994 SECONDS, reward -0.029025094583630562, action 7\n",
      "EPISODE DONE IN 1.7011387348175049 SECONDS, reward -0.9342076778411865, action 8\n",
      "EPISODE DONE IN 2.260749101638794 SECONDS, reward -1.335608720779419, action 7\n",
      "\n",
      "\n",
      "EPISODE: 22\n",
      "EPISODE DONE IN 0.5470411777496338 SECONDS, reward -0.045279935002326965, action 0\n",
      "EPISODE DONE IN 1.0901854038238525 SECONDS, reward 0.0066936565563082695, action 6\n",
      "EPISODE DONE IN 1.6107285022735596 SECONDS, reward 0.09737089276313782, action 0\n",
      "EPISODE DONE IN 2.127488374710083 SECONDS, reward -0.38944563269615173, action 4\n",
      "\n",
      "\n",
      "EPISODE: 23\n",
      "EPISODE DONE IN 0.5285894870758057 SECONDS, reward -0.38944563269615173, action 0\n",
      "EPISODE DONE IN 1.0413718223571777 SECONDS, reward -0.0993165671825409, action 8\n",
      "EPISODE DONE IN 1.5882065296173096 SECONDS, reward -0.3472086787223816, action 6\n",
      "EPISODE DONE IN 2.1311755180358887 SECONDS, reward -0.13253837823867798, action 8\n",
      "\n",
      "\n",
      "EPISODE: 24\n",
      "EPISODE DONE IN 0.5772721767425537 SECONDS, reward -1.603292465209961, action 6\n",
      "EPISODE DONE IN 1.1424317359924316 SECONDS, reward -0.9243426322937012, action 5\n",
      "EPISODE DONE IN 1.70888090133667 SECONDS, reward -0.6254870295524597, action 8\n",
      "EPISODE DONE IN 2.2682418823242188 SECONDS, reward -0.7514554858207703, action 7\n",
      "\n",
      "\n",
      "EPISODE: 25\n",
      "EPISODE DONE IN 0.573603630065918 SECONDS, reward -0.7514554858207703, action 6\n",
      "EPISODE DONE IN 1.1338598728179932 SECONDS, reward -0.7514554858207703, action 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 1.698732614517212 SECONDS, reward -0.7514554858207703, action 8\n",
      "EPISODE DONE IN 2.2588257789611816 SECONDS, reward -1.4375109672546387, action 4\n",
      "\n",
      "\n",
      "EPISODE: 26\n",
      "EPISODE DONE IN 0.5766568183898926 SECONDS, reward -0.4721514582633972, action 1\n",
      "EPISODE DONE IN 1.138720989227295 SECONDS, reward 0.24485132098197937, action 1\n",
      "EPISODE DONE IN 1.7055912017822266 SECONDS, reward 0.1381617933511734, action 6\n",
      "EPISODE DONE IN 2.2674624919891357 SECONDS, reward -0.45567840337753296, action 1\n",
      "\n",
      "\n",
      "EPISODE: 27\n",
      "EPISODE DONE IN 0.5676157474517822 SECONDS, reward -1.7956104278564453, action 5\n",
      "EPISODE DONE IN 1.1155035495758057 SECONDS, reward -0.6026293039321899, action 0\n",
      "EPISODE DONE IN 1.6604301929473877 SECONDS, reward -1.2021427154541016, action 4\n",
      "EPISODE DONE IN 2.2025091648101807 SECONDS, reward -0.2771685719490051, action 5\n",
      "\n",
      "\n",
      "EPISODE: 28\n",
      "EPISODE DONE IN 0.5529069900512695 SECONDS, reward -0.3240374028682709, action 6\n",
      "EPISODE DONE IN 1.11332368850708 SECONDS, reward -1.3130933046340942, action 2\n",
      "EPISODE DONE IN 1.6855225563049316 SECONDS, reward -1.073475956916809, action 5\n",
      "EPISODE DONE IN 2.2500481605529785 SECONDS, reward -0.7311170697212219, action 7\n",
      "\n",
      "\n",
      "EPISODE: 29\n",
      "EPISODE DONE IN 0.5607645511627197 SECONDS, reward -0.1808525174856186, action 3\n",
      "EPISODE DONE IN 1.1397886276245117 SECONDS, reward -0.2802673280239105, action 3\n",
      "EPISODE DONE IN 1.6990365982055664 SECONDS, reward -0.1761096864938736, action 6\n",
      "EPISODE DONE IN 2.259521245956421 SECONDS, reward -0.09719402343034744, action 8\n",
      "\n",
      "\n",
      "EPISODE: 30\n",
      "EPISODE DONE IN 0.5593481063842773 SECONDS, reward -0.02327238768339157, action 2\n",
      "EPISODE DONE IN 1.1278371810913086 SECONDS, reward -0.18127970397472382, action 4\n",
      "EPISODE DONE IN 1.6862270832061768 SECONDS, reward -1.3133177757263184, action 1\n",
      "EPISODE DONE IN 2.2433018684387207 SECONDS, reward -1.7940475940704346, action 5\n",
      "\n",
      "\n",
      "EPISODE: 31\n",
      "EPISODE DONE IN 0.5654313564300537 SECONDS, reward -0.657943844795227, action 0\n",
      "EPISODE DONE IN 1.1025819778442383 SECONDS, reward -0.41652700304985046, action 1\n",
      "EPISODE DONE IN 1.6372246742248535 SECONDS, reward -1.7963917255401611, action 0\n",
      "EPISODE DONE IN 2.1489968299865723 SECONDS, reward -5.15889835357666, action 2\n",
      "\n",
      "\n",
      "EPISODE: 32\n",
      "EPISODE DONE IN 0.5272865295410156 SECONDS, reward -5.15889835357666, action 0\n",
      "EPISODE DONE IN 1.0413107872009277 SECONDS, reward -1.1390821933746338, action 7\n",
      "EPISODE DONE IN 1.5846350193023682 SECONDS, reward -1.9755300283432007, action 1\n",
      "EPISODE DONE IN 2.1279208660125732 SECONDS, reward -1.1013522148132324, action 1\n",
      "\n",
      "\n",
      "EPISODE: 33\n",
      "EPISODE DONE IN 0.5750167369842529 SECONDS, reward 0.0995553657412529, action 3\n",
      "EPISODE DONE IN 1.1346583366394043 SECONDS, reward 0.029605988413095474, action 6\n",
      "EPISODE DONE IN 1.7039241790771484 SECONDS, reward -0.45174115896224976, action 3\n",
      "EPISODE DONE IN 2.2643046379089355 SECONDS, reward -1.1098076105117798, action 4\n",
      "\n",
      "\n",
      "EPISODE: 34\n",
      "EPISODE DONE IN 0.5750267505645752 SECONDS, reward -0.36481502652168274, action 7\n",
      "EPISODE DONE IN 1.1444027423858643 SECONDS, reward -1.8955879211425781, action 1\n",
      "EPISODE DONE IN 1.6946384906768799 SECONDS, reward -0.4054747521877289, action 0\n",
      "EPISODE DONE IN 2.2345194816589355 SECONDS, reward -0.23675407469272614, action 2\n",
      "\n",
      "\n",
      "EPISODE: 35\n",
      "EPISODE DONE IN 0.5237772464752197 SECONDS, reward -5.15889835357666, action 0\n",
      "EPISODE DONE IN 1.0452921390533447 SECONDS, reward -1.5024242401123047, action 6\n",
      "EPISODE DONE IN 1.593308448791504 SECONDS, reward -0.3780195116996765, action 7\n",
      "EPISODE DONE IN 2.132636785507202 SECONDS, reward -0.5504130721092224, action 1\n",
      "\n",
      "\n",
      "EPISODE: 36\n",
      "EPISODE DONE IN 0.5484087467193604 SECONDS, reward -0.5504130721092224, action 0\n",
      "EPISODE DONE IN 1.0870497226715088 SECONDS, reward -0.7429381608963013, action 8\n",
      "EPISODE DONE IN 1.633195400238037 SECONDS, reward -1.2941467761993408, action 2\n",
      "EPISODE DONE IN 2.174194812774658 SECONDS, reward -0.5706282258033752, action 6\n",
      "\n",
      "\n",
      "EPISODE: 37\n",
      "EPISODE DONE IN 0.5692870616912842 SECONDS, reward -0.3171021044254303, action 8\n",
      "EPISODE DONE IN 1.1278753280639648 SECONDS, reward -0.7768025398254395, action 4\n",
      "EPISODE DONE IN 1.7027437686920166 SECONDS, reward -0.6338165402412415, action 1\n",
      "EPISODE DONE IN 2.2670674324035645 SECONDS, reward -0.6338165402412415, action 6\n",
      "\n",
      "\n",
      "EPISODE: 38\n",
      "EPISODE DONE IN 0.5651109218597412 SECONDS, reward -1.6152033805847168, action 3\n",
      "EPISODE DONE IN 1.1208484172821045 SECONDS, reward -1.116654396057129, action 6\n",
      "EPISODE DONE IN 1.6825361251831055 SECONDS, reward -0.8968632817268372, action 6\n",
      "EPISODE DONE IN 2.23732852935791 SECONDS, reward -1.7571154832839966, action 3\n",
      "\n",
      "\n",
      "EPISODE: 39\n",
      "EPISODE DONE IN 0.5599243640899658 SECONDS, reward -0.7341440320014954, action 7\n",
      "EPISODE DONE IN 1.1251463890075684 SECONDS, reward -1.4633293151855469, action 3\n",
      "EPISODE DONE IN 1.6934504508972168 SECONDS, reward -0.6227611303329468, action 3\n",
      "EPISODE DONE IN 2.250425338745117 SECONDS, reward -0.6227611303329468, action 3\n",
      "\n",
      "\n",
      "EPISODE: 40\n",
      "EPISODE DONE IN 0.5863227844238281 SECONDS, reward -1.3667473793029785, action 2\n",
      "EPISODE DONE IN 1.1707344055175781 SECONDS, reward -1.833877444267273, action 4\n",
      "EPISODE DONE IN 1.738978624343872 SECONDS, reward -0.8546610474586487, action 6\n",
      "EPISODE DONE IN 2.307386875152588 SECONDS, reward -0.8546610474586487, action 3\n",
      "\n",
      "\n",
      "EPISODE: 41\n",
      "EPISODE DONE IN 0.569206714630127 SECONDS, reward -1.3650695085525513, action 6\n",
      "EPISODE DONE IN 1.1265344619750977 SECONDS, reward -1.383575201034546, action 2\n",
      "EPISODE DONE IN 1.700071096420288 SECONDS, reward -1.6869953870773315, action 5\n",
      "EPISODE DONE IN 2.25952410697937 SECONDS, reward -0.8791260719299316, action 6\n",
      "\n",
      "\n",
      "EPISODE: 42\n",
      "EPISODE DONE IN 0.5719435214996338 SECONDS, reward -0.47786861658096313, action 4\n",
      "EPISODE DONE IN 1.132939338684082 SECONDS, reward -1.3461047410964966, action 6\n",
      "EPISODE DONE IN 1.6990134716033936 SECONDS, reward -1.3461047410964966, action 5\n",
      "EPISODE DONE IN 2.261096954345703 SECONDS, reward -0.8664236068725586, action 2\n",
      "\n",
      "\n",
      "EPISODE: 43\n",
      "EPISODE DONE IN 0.5638830661773682 SECONDS, reward -0.5477039217948914, action 6\n",
      "EPISODE DONE IN 1.121586561203003 SECONDS, reward -1.1192705631256104, action 3\n",
      "EPISODE DONE IN 1.6943438053131104 SECONDS, reward -1.3836451768875122, action 6\n",
      "EPISODE DONE IN 2.2565457820892334 SECONDS, reward -1.086879014968872, action 5\n",
      "\n",
      "\n",
      "EPISODE: 44\n",
      "EPISODE DONE IN 0.5679552555084229 SECONDS, reward -1.086879014968872, action 6\n",
      "EPISODE DONE IN 1.1292579174041748 SECONDS, reward -0.7093726992607117, action 1\n",
      "EPISODE DONE IN 1.6971514225006104 SECONDS, reward -1.0641931295394897, action 4\n",
      "EPISODE DONE IN 2.2617969512939453 SECONDS, reward -1.467860460281372, action 1\n",
      "\n",
      "\n",
      "EPISODE: 45\n",
      "EPISODE DONE IN 0.5746438503265381 SECONDS, reward -1.467860460281372, action 6\n",
      "EPISODE DONE IN 1.1363720893859863 SECONDS, reward -1.467860460281372, action 1\n",
      "EPISODE DONE IN 1.701406478881836 SECONDS, reward -2.1209757328033447, action 6\n",
      "EPISODE DONE IN 2.261970043182373 SECONDS, reward -0.8786677718162537, action 4\n",
      "\n",
      "\n",
      "EPISODE: 46\n",
      "EPISODE DONE IN 0.5662240982055664 SECONDS, reward -0.8786677718162537, action 6\n",
      "EPISODE DONE IN 1.1321930885314941 SECONDS, reward -1.0951651334762573, action 6\n",
      "EPISODE DONE IN 1.704627275466919 SECONDS, reward -1.0951651334762573, action 6\n",
      "EPISODE DONE IN 2.2699384689331055 SECONDS, reward -1.6387993097305298, action 6\n",
      "\n",
      "\n",
      "EPISODE: 47\n",
      "EPISODE DONE IN 0.5725655555725098 SECONDS, reward -1.1186891794204712, action 8\n",
      "EPISODE DONE IN 1.1331255435943604 SECONDS, reward -1.3703107833862305, action 5\n",
      "EPISODE DONE IN 1.7009508609771729 SECONDS, reward -1.3703107833862305, action 6\n",
      "EPISODE DONE IN 2.261742115020752 SECONDS, reward -1.1432890892028809, action 5\n",
      "\n",
      "\n",
      "EPISODE: 48\n",
      "EPISODE DONE IN 0.5489006042480469 SECONDS, reward -0.15058432519435883, action 0\n",
      "EPISODE DONE IN 1.0561308860778809 SECONDS, reward 1.3113434314727783, action 0\n",
      "EPISODE DONE IN 1.5692975521087646 SECONDS, reward 1.3113434314727783, action 6\n",
      "EPISODE DONE IN 2.049099922180176 SECONDS, reward 1.074416160583496, action 0\n",
      "\n",
      "\n",
      "EPISODE: 49\n",
      "EPISODE DONE IN 0.514531135559082 SECONDS, reward -0.17086197435855865, action 5\n",
      "EPISODE DONE IN 1.0581820011138916 SECONDS, reward -0.1956571638584137, action 6\n",
      "EPISODE DONE IN 1.5980148315429688 SECONDS, reward -0.1956571638584137, action 6\n",
      "EPISODE DONE IN 2.131101608276367 SECONDS, reward -0.1956571638584137, action 0\n",
      "\n",
      "\n",
      "EPISODE: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 0.5630519390106201 SECONDS, reward -0.08716513961553574, action 7\n",
      "EPISODE DONE IN 1.1109349727630615 SECONDS, reward -0.08716513961553574, action 6\n",
      "EPISODE DONE IN 1.6647639274597168 SECONDS, reward -0.16384343802928925, action 8\n",
      "EPISODE DONE IN 2.2206485271453857 SECONDS, reward -0.16384343802928925, action 0\n",
      "\n",
      "\n",
      "EPISODE: 51\n",
      "EPISODE DONE IN 0.5499114990234375 SECONDS, reward 0.20716089010238647, action 0\n",
      "EPISODE DONE IN 1.0611662864685059 SECONDS, reward -0.44022637605667114, action 2\n",
      "EPISODE DONE IN 1.5791497230529785 SECONDS, reward -1.1282588243484497, action 7\n",
      "EPISODE DONE IN 2.1235320568084717 SECONDS, reward -1.1282588243484497, action 0\n",
      "\n",
      "\n",
      "EPISODE: 52\n",
      "EPISODE DONE IN 0.5396504402160645 SECONDS, reward -2.816877841949463, action 2\n",
      "EPISODE DONE IN 1.084263563156128 SECONDS, reward -2.816877841949463, action 2\n",
      "EPISODE DONE IN 1.6234283447265625 SECONDS, reward -2.050957202911377, action 3\n",
      "EPISODE DONE IN 2.193619728088379 SECONDS, reward -0.3176479935646057, action 6\n",
      "\n",
      "\n",
      "EPISODE: 53\n",
      "EPISODE DONE IN 0.5689220428466797 SECONDS, reward -0.21768099069595337, action 1\n",
      "EPISODE DONE IN 1.1322031021118164 SECONDS, reward -1.5447440147399902, action 8\n",
      "EPISODE DONE IN 1.7041456699371338 SECONDS, reward -0.9041847586631775, action 6\n",
      "EPISODE DONE IN 2.2409868240356445 SECONDS, reward -0.4633648097515106, action 0\n",
      "\n",
      "\n",
      "EPISODE: 54\n",
      "EPISODE DONE IN 0.5452671051025391 SECONDS, reward 0.03915572911500931, action 8\n",
      "EPISODE DONE IN 1.0594265460968018 SECONDS, reward 0.21723133325576782, action 0\n",
      "EPISODE DONE IN 1.5763046741485596 SECONDS, reward -0.08570638298988342, action 4\n",
      "EPISODE DONE IN 2.1158456802368164 SECONDS, reward -0.4132479727268219, action 5\n",
      "\n",
      "\n",
      "EPISODE: 55\n",
      "EPISODE DONE IN 0.5436804294586182 SECONDS, reward -0.4132479727268219, action 8\n",
      "EPISODE DONE IN 1.0880303382873535 SECONDS, reward -0.4132479727268219, action 0\n",
      "EPISODE DONE IN 1.6085686683654785 SECONDS, reward 0.09309913218021393, action 0\n",
      "EPISODE DONE IN 2.09037709236145 SECONDS, reward 2.6174464225769043, action 0\n",
      "\n",
      "\n",
      "EPISODE: 56\n",
      "EPISODE DONE IN 0.48860669136047363 SECONDS, reward -3.3864054679870605, action 3\n",
      "EPISODE DONE IN 1.002000093460083 SECONDS, reward 1.2672688961029053, action 6\n",
      "EPISODE DONE IN 1.5415527820587158 SECONDS, reward 0.01507886778563261, action 1\n",
      "EPISODE DONE IN 2.097790479660034 SECONDS, reward -1.5444540977478027, action 8\n",
      "\n",
      "\n",
      "EPISODE: 57\n",
      "EPISODE DONE IN 0.5602231025695801 SECONDS, reward -0.9037447571754456, action 6\n",
      "EPISODE DONE IN 1.1356079578399658 SECONDS, reward -1.5439140796661377, action 3\n",
      "EPISODE DONE IN 1.7603209018707275 SECONDS, reward -0.6108624935150146, action 6\n",
      "EPISODE DONE IN 2.3353261947631836 SECONDS, reward -0.8965432643890381, action 6\n",
      "\n",
      "\n",
      "EPISODE: 58\n",
      "EPISODE DONE IN 0.5640604496002197 SECONDS, reward -0.8965432643890381, action 6\n",
      "EPISODE DONE IN 1.1232194900512695 SECONDS, reward -0.5944055318832397, action 1\n",
      "EPISODE DONE IN 1.6960136890411377 SECONDS, reward -1.4630228281021118, action 2\n",
      "EPISODE DONE IN 2.2523748874664307 SECONDS, reward -1.7571815252304077, action 5\n",
      "\n",
      "\n",
      "EPISODE: 59\n",
      "EPISODE DONE IN 0.561924934387207 SECONDS, reward -1.9247570037841797, action 7\n",
      "EPISODE DONE IN 1.132457971572876 SECONDS, reward -0.3417859673500061, action 7\n",
      "EPISODE DONE IN 1.6955342292785645 SECONDS, reward -0.9161183834075928, action 6\n",
      "EPISODE DONE IN 2.2586090564727783 SECONDS, reward -0.7447559833526611, action 4\n",
      "\n",
      "\n",
      "EPISODE: 60\n",
      "EPISODE DONE IN 0.5711362361907959 SECONDS, reward -0.9108000993728638, action 6\n",
      "EPISODE DONE IN 1.1087076663970947 SECONDS, reward -0.25700655579566956, action 0\n",
      "EPISODE DONE IN 1.6281511783599854 SECONDS, reward -0.37345090508461, action 0\n",
      "EPISODE DONE IN 2.117164134979248 SECONDS, reward 1.0881047248840332, action 0\n",
      "\n",
      "\n",
      "EPISODE: 61\n",
      "EPISODE DONE IN 0.45567870140075684 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.9691596031188965 SECONDS, reward 2.5918285846710205, action 8\n",
      "EPISODE DONE IN 1.4633753299713135 SECONDS, reward 2.5918285846710205, action 0\n",
      "EPISODE DONE IN 1.979727029800415 SECONDS, reward 0.20630088448524475, action 6\n",
      "\n",
      "\n",
      "EPISODE: 62\n",
      "EPISODE DONE IN 0.5479753017425537 SECONDS, reward -0.13732174038887024, action 4\n",
      "EPISODE DONE IN 1.0805761814117432 SECONDS, reward -0.3267974257469177, action 5\n",
      "EPISODE DONE IN 1.6314237117767334 SECONDS, reward -0.3267974257469177, action 0\n",
      "EPISODE DONE IN 2.189297914505005 SECONDS, reward -0.1499374657869339, action 3\n",
      "\n",
      "\n",
      "EPISODE: 63\n",
      "EPISODE DONE IN 0.575798749923706 SECONDS, reward 0.09293178468942642, action 8\n",
      "EPISODE DONE IN 1.1320569515228271 SECONDS, reward -0.2336248755455017, action 1\n",
      "EPISODE DONE IN 1.7020018100738525 SECONDS, reward -1.806292176246643, action 5\n",
      "EPISODE DONE IN 2.2725439071655273 SECONDS, reward -1.0477163791656494, action 6\n",
      "\n",
      "\n",
      "EPISODE: 64\n",
      "EPISODE DONE IN 0.5637350082397461 SECONDS, reward -0.7093726992607117, action 6\n",
      "EPISODE DONE IN 1.131805419921875 SECONDS, reward -1.3415303230285645, action 6\n",
      "EPISODE DONE IN 1.71238374710083 SECONDS, reward -0.7268506288528442, action 2\n",
      "EPISODE DONE IN 2.3115079402923584 SECONDS, reward 0.010828934609889984, action 2\n",
      "\n",
      "\n",
      "EPISODE: 65\n",
      "EPISODE DONE IN 0.5850567817687988 SECONDS, reward 0.08368279039859772, action 7\n",
      "EPISODE DONE IN 1.1297521591186523 SECONDS, reward -2.809016704559326, action 0\n",
      "EPISODE DONE IN 1.6484904289245605 SECONDS, reward -1.1188030242919922, action 0\n",
      "EPISODE DONE IN 2.1646151542663574 SECONDS, reward 0.21635834872722626, action 7\n",
      "\n",
      "\n",
      "EPISODE: 66\n",
      "EPISODE DONE IN 0.5569612979888916 SECONDS, reward 0.21635834872722626, action 7\n",
      "EPISODE DONE IN 1.1188740730285645 SECONDS, reward -0.4204382300376892, action 2\n",
      "EPISODE DONE IN 1.7015399932861328 SECONDS, reward -0.640789806842804, action 7\n",
      "EPISODE DONE IN 2.257267713546753 SECONDS, reward -0.4204382300376892, action 0\n",
      "\n",
      "\n",
      "EPISODE: 67\n",
      "EPISODE DONE IN 0.5691909790039062 SECONDS, reward -1.1282588243484497, action 0\n",
      "EPISODE DONE IN 1.068821668624878 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 1.5064821243286133 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.9426777362823486 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 68\n",
      "EPISODE DONE IN 0.5197696685791016 SECONDS, reward 1.9200869798660278, action 7\n",
      "EPISODE DONE IN 1.0518019199371338 SECONDS, reward 1.530889630317688, action 7\n",
      "EPISODE DONE IN 1.5917611122131348 SECONDS, reward 1.530889630317688, action 0\n",
      "EPISODE DONE IN 2.1775619983673096 SECONDS, reward -0.1312236487865448, action 8\n",
      "\n",
      "\n",
      "EPISODE: 69\n",
      "EPISODE DONE IN 0.5618419647216797 SECONDS, reward 0.3046761453151703, action 0\n",
      "EPISODE DONE IN 1.0815720558166504 SECONDS, reward -0.44022637605667114, action 2\n",
      "EPISODE DONE IN 1.6551868915557861 SECONDS, reward 0.2712126672267914, action 8\n",
      "EPISODE DONE IN 2.2063848972320557 SECONDS, reward 0.2712126672267914, action 8\n",
      "\n",
      "\n",
      "EPISODE: 70\n",
      "EPISODE DONE IN 0.5718955993652344 SECONDS, reward 0.2712126672267914, action 0\n",
      "EPISODE DONE IN 1.1199314594268799 SECONDS, reward 1.6778125762939453, action 0\n",
      "EPISODE DONE IN 1.665384292602539 SECONDS, reward 1.6778125762939453, action 8\n",
      "EPISODE DONE IN 2.1538031101226807 SECONDS, reward 2.6046342849731445, action 0\n",
      "\n",
      "\n",
      "EPISODE: 71\n",
      "EPISODE DONE IN 0.5054771900177002 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.0480360984802246 SECONDS, reward -0.43211010098457336, action 3\n",
      "EPISODE DONE IN 1.5733885765075684 SECONDS, reward -1.1899491548538208, action 4\n",
      "EPISODE DONE IN 2.116563558578491 SECONDS, reward -0.24575482308864594, action 4\n",
      "\n",
      "\n",
      "EPISODE: 72\n",
      "EPISODE DONE IN 0.5596368312835693 SECONDS, reward -0.24575482308864594, action 0\n",
      "EPISODE DONE IN 1.1283063888549805 SECONDS, reward -0.17240384221076965, action 2\n",
      "EPISODE DONE IN 1.707247018814087 SECONDS, reward -0.4265030324459076, action 2\n",
      "EPISODE DONE IN 2.2606587409973145 SECONDS, reward -0.09589824825525284, action 7\n",
      "\n",
      "\n",
      "EPISODE: 73\n",
      "EPISODE DONE IN 0.6187286376953125 SECONDS, reward -1.282893180847168, action 5\n",
      "EPISODE DONE IN 1.194033145904541 SECONDS, reward -0.7842071056365967, action 8\n",
      "EPISODE DONE IN 1.7697503566741943 SECONDS, reward -1.386790156364441, action 5\n",
      "EPISODE DONE IN 2.318342924118042 SECONDS, reward -0.04218624532222748, action 0\n",
      "\n",
      "\n",
      "EPISODE: 74\n",
      "EPISODE DONE IN 0.5535287857055664 SECONDS, reward 0.08200801908969879, action 0\n",
      "EPISODE DONE IN 1.1060187816619873 SECONDS, reward -0.04301220923662186, action 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 1.627319574356079 SECONDS, reward 1.8937617540359497, action 0\n",
      "EPISODE DONE IN 2.128897190093994 SECONDS, reward 1.8937617540359497, action 0\n",
      "\n",
      "\n",
      "EPISODE: 75\n",
      "EPISODE DONE IN 0.5028238296508789 SECONDS, reward 1.8937617540359497, action 0\n",
      "EPISODE DONE IN 1.0159635543823242 SECONDS, reward -1.3662217855453491, action 4\n",
      "EPISODE DONE IN 1.5691227912902832 SECONDS, reward -1.1906092166900635, action 3\n",
      "EPISODE DONE IN 2.0925381183624268 SECONDS, reward -1.1906092166900635, action 0\n",
      "\n",
      "\n",
      "EPISODE: 76\n",
      "EPISODE DONE IN 0.548820972442627 SECONDS, reward -1.1906092166900635, action 0\n",
      "EPISODE DONE IN 1.0993123054504395 SECONDS, reward -1.1906092166900635, action 4\n",
      "EPISODE DONE IN 1.6380901336669922 SECONDS, reward -0.3883856236934662, action 6\n",
      "EPISODE DONE IN 2.1847119331359863 SECONDS, reward -0.5347522497177124, action 1\n",
      "\n",
      "\n",
      "EPISODE: 77\n",
      "EPISODE DONE IN 0.6006107330322266 SECONDS, reward -1.0645132064819336, action 5\n",
      "EPISODE DONE IN 1.206606388092041 SECONDS, reward -0.8552821278572083, action 5\n",
      "EPISODE DONE IN 1.7779321670532227 SECONDS, reward -1.0434186458587646, action 7\n",
      "EPISODE DONE IN 2.3301875591278076 SECONDS, reward -0.3076218068599701, action 0\n",
      "\n",
      "\n",
      "EPISODE: 78\n",
      "EPISODE DONE IN 0.5906128883361816 SECONDS, reward -0.3076218068599701, action 5\n",
      "EPISODE DONE IN 1.1433205604553223 SECONDS, reward -0.3076218068599701, action 5\n",
      "EPISODE DONE IN 1.6626935005187988 SECONDS, reward 1.0592378377914429, action 0\n",
      "EPISODE DONE IN 2.1987764835357666 SECONDS, reward -0.2639177143573761, action 6\n",
      "\n",
      "\n",
      "EPISODE: 79\n",
      "EPISODE DONE IN 0.5490515232086182 SECONDS, reward -0.18177874386310577, action 0\n",
      "EPISODE DONE IN 1.0758781433105469 SECONDS, reward -0.18177874386310577, action 5\n",
      "EPISODE DONE IN 1.6018061637878418 SECONDS, reward -0.18177874386310577, action 0\n",
      "EPISODE DONE IN 2.1504621505737305 SECONDS, reward -1.6391124725341797, action 2\n",
      "\n",
      "\n",
      "EPISODE: 80\n",
      "EPISODE DONE IN 0.5644965171813965 SECONDS, reward -1.6391124725341797, action 0\n",
      "EPISODE DONE IN 1.079451322555542 SECONDS, reward -1.5024242401123047, action 6\n",
      "EPISODE DONE IN 1.63289213180542 SECONDS, reward -0.4625924527645111, action 6\n",
      "EPISODE DONE IN 2.2227306365966797 SECONDS, reward -0.1462831050157547, action 6\n",
      "\n",
      "\n",
      "EPISODE: 81\n",
      "EPISODE DONE IN 0.5807168483734131 SECONDS, reward -1.331235647201538, action 7\n",
      "EPISODE DONE IN 1.1332335472106934 SECONDS, reward -0.17803838849067688, action 0\n",
      "EPISODE DONE IN 1.7123770713806152 SECONDS, reward -0.5115522146224976, action 2\n",
      "EPISODE DONE IN 2.2593302726745605 SECONDS, reward -1.1188030242919922, action 0\n",
      "\n",
      "\n",
      "EPISODE: 82\n",
      "EPISODE DONE IN 0.5493834018707275 SECONDS, reward -2.2807090282440186, action 3\n",
      "EPISODE DONE IN 1.075683832168579 SECONDS, reward -2.2807090282440186, action 0\n",
      "EPISODE DONE IN 1.5696332454681396 SECONDS, reward -3.3864054679870605, action 0\n",
      "EPISODE DONE IN 2.094503402709961 SECONDS, reward -4.6587958335876465, action 1\n",
      "\n",
      "\n",
      "EPISODE: 83\n",
      "EPISODE DONE IN 0.5525081157684326 SECONDS, reward -0.7702431082725525, action 6\n",
      "EPISODE DONE IN 1.118767261505127 SECONDS, reward 0.01678885705769062, action 3\n",
      "EPISODE DONE IN 1.6897993087768555 SECONDS, reward -1.5439140796661377, action 8\n",
      "EPISODE DONE IN 2.2266037464141846 SECONDS, reward -0.4319802522659302, action 0\n",
      "\n",
      "\n",
      "EPISODE: 84\n",
      "EPISODE DONE IN 0.5266244411468506 SECONDS, reward -0.43211010098457336, action 0\n",
      "EPISODE DONE IN 1.0407893657684326 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.5702524185180664 SECONDS, reward 0.03539615496993065, action 5\n",
      "EPISODE DONE IN 2.0898046493530273 SECONDS, reward 1.6535899639129639, action 8\n",
      "\n",
      "\n",
      "EPISODE: 85\n",
      "EPISODE DONE IN 0.5674870014190674 SECONDS, reward -0.12973187863826752, action 7\n",
      "EPISODE DONE IN 1.1621968746185303 SECONDS, reward -0.9428574442863464, action 3\n",
      "EPISODE DONE IN 1.7679898738861084 SECONDS, reward -1.6284658908843994, action 2\n",
      "EPISODE DONE IN 2.3326475620269775 SECONDS, reward -1.6284658908843994, action 8\n",
      "\n",
      "\n",
      "EPISODE: 86\n",
      "EPISODE DONE IN 0.5726585388183594 SECONDS, reward -0.09657951444387436, action 0\n",
      "EPISODE DONE IN 1.1470019817352295 SECONDS, reward -0.3499305546283722, action 7\n",
      "EPISODE DONE IN 1.6795926094055176 SECONDS, reward 0.3046761453151703, action 0\n",
      "EPISODE DONE IN 2.206191062927246 SECONDS, reward 0.2050275355577469, action 7\n",
      "\n",
      "\n",
      "EPISODE: 87\n",
      "EPISODE DONE IN 0.5810904502868652 SECONDS, reward -0.04291993007063866, action 8\n",
      "EPISODE DONE IN 1.1540203094482422 SECONDS, reward -0.5185622572898865, action 1\n",
      "EPISODE DONE IN 1.7259066104888916 SECONDS, reward -0.7602102756500244, action 6\n",
      "EPISODE DONE IN 2.2712838649749756 SECONDS, reward -0.459964781999588, action 0\n",
      "\n",
      "\n",
      "EPISODE: 88\n",
      "EPISODE DONE IN 0.5312931537628174 SECONDS, reward -0.7819997668266296, action 0\n",
      "EPISODE DONE IN 1.0755326747894287 SECONDS, reward -0.18177874386310577, action 5\n",
      "EPISODE DONE IN 1.5989696979522705 SECONDS, reward 0.020883090794086456, action 0\n",
      "EPISODE DONE IN 2.132211923599243 SECONDS, reward -0.578562319278717, action 4\n",
      "\n",
      "\n",
      "EPISODE: 89\n",
      "EPISODE DONE IN 0.5530636310577393 SECONDS, reward -0.03079509176313877, action 7\n",
      "EPISODE DONE IN 1.0913410186767578 SECONDS, reward -0.218637153506279, action 0\n",
      "EPISODE DONE IN 1.6718792915344238 SECONDS, reward -1.1369651556015015, action 2\n",
      "EPISODE DONE IN 2.2128689289093018 SECONDS, reward -2.809016704559326, action 2\n",
      "\n",
      "\n",
      "EPISODE: 90\n",
      "EPISODE DONE IN 0.5534048080444336 SECONDS, reward -2.705448627471924, action 5\n",
      "EPISODE DONE IN 1.1559433937072754 SECONDS, reward 0.010669044218957424, action 7\n",
      "EPISODE DONE IN 1.7481541633605957 SECONDS, reward -0.5508016347885132, action 5\n",
      "EPISODE DONE IN 2.312938928604126 SECONDS, reward -0.5508016347885132, action 2\n",
      "\n",
      "\n",
      "EPISODE: 91\n",
      "EPISODE DONE IN 0.5700459480285645 SECONDS, reward -0.6640985608100891, action 0\n",
      "EPISODE DONE IN 1.1178655624389648 SECONDS, reward 0.9413972496986389, action 0\n",
      "EPISODE DONE IN 1.6447360515594482 SECONDS, reward 0.9413972496986389, action 5\n",
      "EPISODE DONE IN 2.128974437713623 SECONDS, reward 0.03539615496993065, action 0\n",
      "\n",
      "\n",
      "EPISODE: 92\n",
      "EPISODE DONE IN 0.560678243637085 SECONDS, reward -1.6223472356796265, action 2\n",
      "EPISODE DONE IN 1.1279911994934082 SECONDS, reward -0.23889648914337158, action 8\n",
      "EPISODE DONE IN 1.6461467742919922 SECONDS, reward 1.7009329795837402, action 0\n",
      "EPISODE DONE IN 2.159982204437256 SECONDS, reward 1.7009329795837402, action 0\n",
      "\n",
      "\n",
      "EPISODE: 93\n",
      "EPISODE DONE IN 0.5506381988525391 SECONDS, reward 1.6525698900222778, action 5\n",
      "EPISODE DONE IN 1.051055908203125 SECONDS, reward 0.04991356283426285, action 0\n",
      "EPISODE DONE IN 1.572737216949463 SECONDS, reward -1.6203471422195435, action 2\n",
      "EPISODE DONE IN 2.106468915939331 SECONDS, reward -1.6203471422195435, action 0\n",
      "\n",
      "\n",
      "EPISODE: 94\n",
      "EPISODE DONE IN 0.5604064464569092 SECONDS, reward -1.1188030242919922, action 7\n",
      "EPISODE DONE IN 1.109501600265503 SECONDS, reward 0.19667181372642517, action 7\n",
      "EPISODE DONE IN 1.620915174484253 SECONDS, reward 1.530889630317688, action 0\n",
      "EPISODE DONE IN 2.1581432819366455 SECONDS, reward -0.1312236487865448, action 8\n",
      "\n",
      "\n",
      "EPISODE: 95\n",
      "EPISODE DONE IN 0.5449366569519043 SECONDS, reward -0.2850998342037201, action 4\n",
      "EPISODE DONE IN 1.0837070941925049 SECONDS, reward -0.8430870771408081, action 4\n",
      "EPISODE DONE IN 1.6263408660888672 SECONDS, reward -0.8430870771408081, action 0\n",
      "EPISODE DONE IN 2.2021405696868896 SECONDS, reward -0.6642552018165588, action 7\n",
      "\n",
      "\n",
      "EPISODE: 96\n",
      "EPISODE DONE IN 0.5746409893035889 SECONDS, reward -0.3054130971431732, action 6\n",
      "EPISODE DONE IN 1.1108295917510986 SECONDS, reward -0.8335766792297363, action 1\n",
      "EPISODE DONE IN 1.6791789531707764 SECONDS, reward -0.8335766792297363, action 0\n",
      "EPISODE DONE IN 2.2582411766052246 SECONDS, reward -1.0705549716949463, action 8\n",
      "\n",
      "\n",
      "EPISODE: 97\n",
      "EPISODE DONE IN 0.5619747638702393 SECONDS, reward -1.377510666847229, action 4\n",
      "EPISODE DONE IN 1.1038153171539307 SECONDS, reward -1.3627870082855225, action 2\n",
      "EPISODE DONE IN 1.6487936973571777 SECONDS, reward -1.3627870082855225, action 0\n",
      "EPISODE DONE IN 2.161412477493286 SECONDS, reward 0.537438154220581, action 0\n",
      "\n",
      "\n",
      "EPISODE: 98\n",
      "EPISODE DONE IN 0.5469975471496582 SECONDS, reward -1.4561176300048828, action 1\n",
      "EPISODE DONE IN 1.0610384941101074 SECONDS, reward -16.960826873779297, action 0\n",
      "EPISODE DONE IN 1.584219217300415 SECONDS, reward -7.169710159301758, action 1\n",
      "EPISODE DONE IN 2.1120975017547607 SECONDS, reward -7.169710159301758, action 0\n",
      "\n",
      "\n",
      "EPISODE: 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 0.5247294902801514 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.0301969051361084 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.458120346069336 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.8885610103607178 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 100\n",
      "EPISODE DONE IN 0.5233523845672607 SECONDS, reward -7.049383640289307, action 2\n",
      "EPISODE DONE IN 1.0587458610534668 SECONDS, reward 0.939697265625, action 5\n",
      "EPISODE DONE IN 1.6075172424316406 SECONDS, reward -0.4351504445075989, action 4\n",
      "EPISODE DONE IN 2.187483549118042 SECONDS, reward -0.5930805802345276, action 8\n",
      "\n",
      "\n",
      "EPISODE: 101\n",
      "EPISODE DONE IN 0.6096906661987305 SECONDS, reward -0.5930805802345276, action 2\n",
      "EPISODE DONE IN 1.190150260925293 SECONDS, reward -0.020508287474513054, action 2\n",
      "EPISODE DONE IN 1.734253168106079 SECONDS, reward -2.0379323959350586, action 0\n",
      "EPISODE DONE IN 2.306128978729248 SECONDS, reward -1.6952961683273315, action 2\n",
      "\n",
      "\n",
      "EPISODE: 102\n",
      "EPISODE DONE IN 0.583099365234375 SECONDS, reward -2.046823024749756, action 1\n",
      "EPISODE DONE IN 1.122276782989502 SECONDS, reward -0.5027923583984375, action 5\n",
      "EPISODE DONE IN 1.7029438018798828 SECONDS, reward -1.2768919467926025, action 2\n",
      "EPISODE DONE IN 2.2751760482788086 SECONDS, reward -0.5027923583984375, action 0\n",
      "\n",
      "\n",
      "EPISODE: 103\n",
      "EPISODE DONE IN 0.5645360946655273 SECONDS, reward -0.165845587849617, action 3\n",
      "EPISODE DONE IN 1.0840041637420654 SECONDS, reward -2.2807090282440186, action 0\n",
      "EPISODE DONE IN 1.5918865203857422 SECONDS, reward -3.3864054679870605, action 0\n",
      "EPISODE DONE IN 2.1097023487091064 SECONDS, reward -3.3864054679870605, action 0\n",
      "\n",
      "\n",
      "EPISODE: 104\n",
      "EPISODE DONE IN 0.5074892044067383 SECONDS, reward -7.049383640289307, action 2\n",
      "EPISODE DONE IN 1.0164265632629395 SECONDS, reward 1.4916999340057373, action 7\n",
      "EPISODE DONE IN 1.5599777698516846 SECONDS, reward 1.4916999340057373, action 0\n",
      "EPISODE DONE IN 2.1022539138793945 SECONDS, reward 1.4916999340057373, action 0\n",
      "\n",
      "\n",
      "EPISODE: 105\n",
      "EPISODE DONE IN 0.5103998184204102 SECONDS, reward 1.8937617540359497, action 0\n",
      "EPISODE DONE IN 0.9353194236755371 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.4463000297546387 SECONDS, reward 0.03539615496993065, action 5\n",
      "EPISODE DONE IN 1.9934494495391846 SECONDS, reward 0.9413972496986389, action 2\n",
      "\n",
      "\n",
      "EPISODE: 106\n",
      "EPISODE DONE IN 0.5148563385009766 SECONDS, reward 0.9413972496986389, action 0\n",
      "EPISODE DONE IN 1.023972988128662 SECONDS, reward 0.9413972496986389, action 0\n",
      "EPISODE DONE IN 1.5305147171020508 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 2.0167741775512695 SECONDS, reward -17.0012149810791, action 1\n",
      "\n",
      "\n",
      "EPISODE: 107\n",
      "EPISODE DONE IN 0.5427207946777344 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.087249517440796 SECONDS, reward -0.793126106262207, action 6\n",
      "EPISODE DONE IN 1.6041269302368164 SECONDS, reward -0.793126106262207, action 0\n",
      "EPISODE DONE IN 2.1411519050598145 SECONDS, reward -0.7186035513877869, action 3\n",
      "\n",
      "\n",
      "EPISODE: 108\n",
      "EPISODE DONE IN 0.5864949226379395 SECONDS, reward -0.2668240964412689, action 2\n",
      "EPISODE DONE IN 1.1385395526885986 SECONDS, reward -0.2668240964412689, action 6\n",
      "EPISODE DONE IN 1.6773765087127686 SECONDS, reward -0.2668240964412689, action 0\n",
      "EPISODE DONE IN 2.260277032852173 SECONDS, reward -0.19307900965213776, action 2\n",
      "\n",
      "\n",
      "EPISODE: 109\n",
      "EPISODE DONE IN 0.5604140758514404 SECONDS, reward -1.5024242401123047, action 0\n",
      "EPISODE DONE IN 1.085723876953125 SECONDS, reward -1.1390821933746338, action 7\n",
      "EPISODE DONE IN 1.6504523754119873 SECONDS, reward -0.42420822381973267, action 7\n",
      "EPISODE DONE IN 2.233966588973999 SECONDS, reward -0.045279935002326965, action 8\n",
      "\n",
      "\n",
      "EPISODE: 110\n",
      "EPISODE DONE IN 0.5795474052429199 SECONDS, reward -0.045279935002326965, action 0\n",
      "EPISODE DONE IN 1.1232500076293945 SECONDS, reward 0.050325386226177216, action 5\n",
      "EPISODE DONE IN 1.7113103866577148 SECONDS, reward 0.050325386226177216, action 7\n",
      "EPISODE DONE IN 2.296583890914917 SECONDS, reward -0.31464388966560364, action 2\n",
      "\n",
      "\n",
      "EPISODE: 111\n",
      "EPISODE DONE IN 0.6154122352600098 SECONDS, reward -1.2961074113845825, action 5\n",
      "EPISODE DONE IN 1.2290983200073242 SECONDS, reward -1.2961074113845825, action 5\n",
      "EPISODE DONE IN 1.8312225341796875 SECONDS, reward -0.061665624380111694, action 1\n",
      "EPISODE DONE IN 2.4126625061035156 SECONDS, reward 0.09269489347934723, action 0\n",
      "\n",
      "\n",
      "EPISODE: 112\n",
      "EPISODE DONE IN 0.5607805252075195 SECONDS, reward -1.8085614442825317, action 0\n",
      "EPISODE DONE IN 1.0810997486114502 SECONDS, reward -3.2461915016174316, action 4\n",
      "EPISODE DONE IN 1.597900629043579 SECONDS, reward -0.5780023336410522, action 5\n",
      "EPISODE DONE IN 2.131594181060791 SECONDS, reward -0.4368904232978821, action 2\n",
      "\n",
      "\n",
      "EPISODE: 113\n",
      "EPISODE DONE IN 0.5613992214202881 SECONDS, reward -1.733033537864685, action 3\n",
      "EPISODE DONE IN 1.176971435546875 SECONDS, reward -1.733033537864685, action 4\n",
      "EPISODE DONE IN 1.7699642181396484 SECONDS, reward -0.4636456072330475, action 1\n",
      "EPISODE DONE IN 2.3053138256073 SECONDS, reward -0.18366311490535736, action 0\n",
      "\n",
      "\n",
      "EPISODE: 114\n",
      "EPISODE DONE IN 0.5541009902954102 SECONDS, reward 0.14983536303043365, action 7\n",
      "EPISODE DONE IN 1.0950007438659668 SECONDS, reward 0.015346411615610123, action 2\n",
      "EPISODE DONE IN 1.632866382598877 SECONDS, reward 1.499874234199524, action 0\n",
      "EPISODE DONE IN 2.2150628566741943 SECONDS, reward -0.4204382300376892, action 7\n",
      "\n",
      "\n",
      "EPISODE: 115\n",
      "EPISODE DONE IN 0.5373020172119141 SECONDS, reward -1.1282588243484497, action 0\n",
      "EPISODE DONE IN 1.0516307353973389 SECONDS, reward 0.3042661249637604, action 8\n",
      "EPISODE DONE IN 1.6029164791107178 SECONDS, reward -0.7429381608963013, action 1\n",
      "EPISODE DONE IN 2.1505441665649414 SECONDS, reward -1.355412483215332, action 3\n",
      "\n",
      "\n",
      "EPISODE: 116\n",
      "EPISODE DONE IN 0.6066420078277588 SECONDS, reward 0.15960806608200073, action 8\n",
      "EPISODE DONE IN 1.1815917491912842 SECONDS, reward -0.8223620057106018, action 1\n",
      "EPISODE DONE IN 1.73708176612854 SECONDS, reward -0.2336248755455017, action 0\n",
      "EPISODE DONE IN 2.2700693607330322 SECONDS, reward 1.807543396949768, action 0\n",
      "\n",
      "\n",
      "EPISODE: 117\n",
      "EPISODE DONE IN 0.5394012928009033 SECONDS, reward -17.041624069213867, action 0\n",
      "EPISODE DONE IN 0.9673027992248535 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.4818296432495117 SECONDS, reward -3.4047372341156006, action 3\n",
      "EPISODE DONE IN 2.019535541534424 SECONDS, reward 0.9911530613899231, action 5\n",
      "\n",
      "\n",
      "EPISODE: 118\n",
      "EPISODE DONE IN 0.6016330718994141 SECONDS, reward -0.4675091505050659, action 7\n",
      "EPISODE DONE IN 1.1941590309143066 SECONDS, reward -1.5658804178237915, action 1\n",
      "EPISODE DONE IN 1.7844977378845215 SECONDS, reward -1.9247570037841797, action 2\n",
      "EPISODE DONE IN 2.387094497680664 SECONDS, reward -1.181107997894287, action 3\n",
      "\n",
      "\n",
      "EPISODE: 119\n",
      "EPISODE DONE IN 0.5765500068664551 SECONDS, reward -1.8756916522979736, action 4\n",
      "EPISODE DONE IN 1.1508221626281738 SECONDS, reward -0.40888169407844543, action 6\n",
      "EPISODE DONE IN 1.7746777534484863 SECONDS, reward -0.8251876831054688, action 4\n",
      "EPISODE DONE IN 2.344143867492676 SECONDS, reward -0.14986303448677063, action 0\n",
      "\n",
      "\n",
      "EPISODE: 120\n",
      "EPISODE DONE IN 0.5433249473571777 SECONDS, reward 0.0708627849817276, action 8\n",
      "EPISODE DONE IN 1.0774483680725098 SECONDS, reward -0.08570638298988342, action 0\n",
      "EPISODE DONE IN 1.6337871551513672 SECONDS, reward -0.08570638298988342, action 4\n",
      "EPISODE DONE IN 2.1865668296813965 SECONDS, reward -0.22436289489269257, action 8\n",
      "\n",
      "\n",
      "EPISODE: 121\n",
      "EPISODE DONE IN 0.5453624725341797 SECONDS, reward -0.030397024005651474, action 4\n",
      "EPISODE DONE IN 1.1101036071777344 SECONDS, reward -0.030397024005651474, action 0\n",
      "EPISODE DONE IN 1.6677448749542236 SECONDS, reward -0.08761638402938843, action 0\n",
      "EPISODE DONE IN 2.15368914604187 SECONDS, reward -1.3344745635986328, action 0\n",
      "\n",
      "\n",
      "EPISODE: 122\n",
      "EPISODE DONE IN 0.43698954582214355 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.9447355270385742 SECONDS, reward 2.5918285846710205, action 8\n",
      "EPISODE DONE IN 1.5009899139404297 SECONDS, reward -0.450459361076355, action 2\n",
      "EPISODE DONE IN 2.015627145767212 SECONDS, reward -0.450459361076355, action 0\n",
      "\n",
      "\n",
      "EPISODE: 123\n",
      "EPISODE DONE IN 0.5472724437713623 SECONDS, reward -0.12882277369499207, action 1\n",
      "EPISODE DONE IN 1.0935497283935547 SECONDS, reward -5.133672714233398, action 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 1.6384754180908203 SECONDS, reward -0.010230560787022114, action 7\n",
      "EPISODE DONE IN 2.1684532165527344 SECONDS, reward 0.01561641599982977, action 2\n",
      "\n",
      "\n",
      "EPISODE: 124\n",
      "EPISODE DONE IN 0.5569968223571777 SECONDS, reward -0.4655759930610657, action 8\n",
      "EPISODE DONE IN 1.1690244674682617 SECONDS, reward -1.209702491760254, action 3\n",
      "EPISODE DONE IN 1.750006914138794 SECONDS, reward -0.14442338049411774, action 2\n",
      "EPISODE DONE IN 2.305293560028076 SECONDS, reward -0.5795155763626099, action 4\n",
      "\n",
      "\n",
      "EPISODE: 125\n",
      "EPISODE DONE IN 0.5832254886627197 SECONDS, reward -0.4959698021411896, action 0\n",
      "EPISODE DONE IN 1.1269052028656006 SECONDS, reward 0.5345990061759949, action 0\n",
      "EPISODE DONE IN 1.619159460067749 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 2.104076385498047 SECONDS, reward 1.074416160583496, action 6\n",
      "\n",
      "\n",
      "EPISODE: 126\n",
      "EPISODE DONE IN 0.5586953163146973 SECONDS, reward -0.705272912979126, action 3\n",
      "EPISODE DONE IN 1.0956060886383057 SECONDS, reward -0.705272912979126, action 0\n",
      "EPISODE DONE IN 1.6142125129699707 SECONDS, reward -0.705272912979126, action 0\n",
      "EPISODE DONE IN 2.124530792236328 SECONDS, reward -0.42051881551742554, action 8\n",
      "\n",
      "\n",
      "EPISODE: 127\n",
      "EPISODE DONE IN 0.49558377265930176 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.0105516910552979 SECONDS, reward -0.44022637605667114, action 2\n",
      "EPISODE DONE IN 1.5574896335601807 SECONDS, reward 0.2486025094985962, action 7\n",
      "EPISODE DONE IN 2.1156842708587646 SECONDS, reward 0.16133348643779755, action 6\n",
      "\n",
      "\n",
      "EPISODE: 128\n",
      "EPISODE DONE IN 0.5871627330780029 SECONDS, reward 0.16133348643779755, action 0\n",
      "EPISODE DONE IN 1.136167049407959 SECONDS, reward -0.04079242795705795, action 5\n",
      "EPISODE DONE IN 1.6799664497375488 SECONDS, reward 0.009616049006581306, action 8\n",
      "EPISODE DONE IN 2.2245030403137207 SECONDS, reward 0.08336801826953888, action 0\n",
      "\n",
      "\n",
      "EPISODE: 129\n",
      "EPISODE DONE IN 0.5548009872436523 SECONDS, reward 0.08336801826953888, action 0\n",
      "EPISODE DONE IN 1.0657079219818115 SECONDS, reward -0.09740656614303589, action 4\n",
      "EPISODE DONE IN 1.5861358642578125 SECONDS, reward -0.09740656614303589, action 8\n",
      "EPISODE DONE IN 2.1344215869903564 SECONDS, reward -0.09740656614303589, action 0\n",
      "\n",
      "\n",
      "EPISODE: 130\n",
      "EPISODE DONE IN 0.5789430141448975 SECONDS, reward -0.8430870771408081, action 4\n",
      "EPISODE DONE IN 1.134565830230713 SECONDS, reward -0.4161979556083679, action 5\n",
      "EPISODE DONE IN 1.6815831661224365 SECONDS, reward -0.22355197370052338, action 4\n",
      "EPISODE DONE IN 2.2140979766845703 SECONDS, reward -0.22355197370052338, action 0\n",
      "\n",
      "\n",
      "EPISODE: 131\n",
      "EPISODE DONE IN 0.564244270324707 SECONDS, reward -0.22355197370052338, action 4\n",
      "EPISODE DONE IN 1.1108932495117188 SECONDS, reward -0.8151940703392029, action 0\n",
      "EPISODE DONE IN 1.630528450012207 SECONDS, reward -0.22013716399669647, action 7\n",
      "EPISODE DONE IN 2.139186143875122 SECONDS, reward -0.22013716399669647, action 0\n",
      "\n",
      "\n",
      "EPISODE: 132\n",
      "EPISODE DONE IN 0.5554153919219971 SECONDS, reward 0.10833720862865448, action 6\n",
      "EPISODE DONE IN 1.1268599033355713 SECONDS, reward -0.18043838441371918, action 6\n",
      "EPISODE DONE IN 1.665125846862793 SECONDS, reward -0.02959141880273819, action 4\n",
      "EPISODE DONE IN 2.210986852645874 SECONDS, reward -0.02959141880273819, action 0\n",
      "\n",
      "\n",
      "EPISODE: 133\n",
      "EPISODE DONE IN 0.5601999759674072 SECONDS, reward -0.38944563269615173, action 0\n",
      "EPISODE DONE IN 1.0947375297546387 SECONDS, reward -0.38944563269615173, action 6\n",
      "EPISODE DONE IN 1.5859334468841553 SECONDS, reward 1.0607329607009888, action 0\n",
      "EPISODE DONE IN 2.085317850112915 SECONDS, reward 1.0607329607009888, action 0\n",
      "\n",
      "\n",
      "EPISODE: 134\n",
      "EPISODE DONE IN 0.5531985759735107 SECONDS, reward 1.665329098701477, action 8\n",
      "EPISODE DONE IN 1.0676853656768799 SECONDS, reward 1.7100001573562622, action 2\n",
      "EPISODE DONE IN 1.5812454223632812 SECONDS, reward 1.7100001573562622, action 0\n",
      "EPISODE DONE IN 2.1479716300964355 SECONDS, reward -0.1722436249256134, action 4\n",
      "\n",
      "\n",
      "EPISODE: 135\n",
      "EPISODE DONE IN 0.5626564025878906 SECONDS, reward -1.8887147903442383, action 0\n",
      "EPISODE DONE IN 1.0433390140533447 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.5535461902618408 SECONDS, reward 0.38370925188064575, action 1\n",
      "EPISODE DONE IN 2.0907886028289795 SECONDS, reward 1.5414730310440063, action 7\n",
      "\n",
      "\n",
      "EPISODE: 136\n",
      "EPISODE DONE IN 0.5746057033538818 SECONDS, reward -0.5448330640792847, action 6\n",
      "EPISODE DONE IN 1.1068048477172852 SECONDS, reward -0.5448330640792847, action 0\n",
      "EPISODE DONE IN 1.6579575538635254 SECONDS, reward -0.04293403401970863, action 7\n",
      "EPISODE DONE IN 2.228952646255493 SECONDS, reward -0.265358030796051, action 3\n",
      "\n",
      "\n",
      "EPISODE: 137\n",
      "EPISODE DONE IN 0.5243527889251709 SECONDS, reward 1.485865592956543, action 0\n",
      "EPISODE DONE IN 1.0403048992156982 SECONDS, reward 1.485865592956543, action 0\n",
      "EPISODE DONE IN 1.6178390979766846 SECONDS, reward 1.485865592956543, action 7\n",
      "EPISODE DONE IN 2.1523449420928955 SECONDS, reward 1.90692138671875, action 0\n",
      "\n",
      "\n",
      "EPISODE: 138\n",
      "EPISODE DONE IN 0.5484158992767334 SECONDS, reward -0.22013716399669647, action 4\n",
      "EPISODE DONE IN 1.0582385063171387 SECONDS, reward -0.22013716399669647, action 0\n",
      "EPISODE DONE IN 1.5945723056793213 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 2.1179592609405518 SECONDS, reward -1.3344745635986328, action 0\n",
      "\n",
      "\n",
      "EPISODE: 139\n",
      "EPISODE DONE IN 0.5089461803436279 SECONDS, reward 2.6174464225769043, action 8\n",
      "EPISODE DONE IN 0.9921858310699463 SECONDS, reward 2.6174464225769043, action 0\n",
      "EPISODE DONE IN 1.5272886753082275 SECONDS, reward 2.6174464225769043, action 0\n",
      "EPISODE DONE IN 2.0830328464508057 SECONDS, reward -0.43096116185188293, action 2\n",
      "\n",
      "\n",
      "EPISODE: 140\n",
      "EPISODE DONE IN 0.5472297668457031 SECONDS, reward -1.8721354007720947, action 4\n",
      "EPISODE DONE IN 1.1053078174591064 SECONDS, reward -0.007443170994520187, action 6\n",
      "EPISODE DONE IN 1.7152087688446045 SECONDS, reward -1.3158650398254395, action 7\n",
      "EPISODE DONE IN 2.2880361080169678 SECONDS, reward -1.634227991104126, action 3\n",
      "\n",
      "\n",
      "EPISODE: 141\n",
      "EPISODE DONE IN 0.5687735080718994 SECONDS, reward -0.270298033952713, action 0\n",
      "EPISODE DONE IN 1.1281445026397705 SECONDS, reward 1.485865592956543, action 0\n",
      "EPISODE DONE IN 1.686000108718872 SECONDS, reward 1.485865592956543, action 7\n",
      "EPISODE DONE IN 2.179550886154175 SECONDS, reward 1.90692138671875, action 0\n",
      "\n",
      "\n",
      "EPISODE: 142\n",
      "EPISODE DONE IN 0.5309762954711914 SECONDS, reward -0.02937251888215542, action 5\n",
      "EPISODE DONE IN 1.094090461730957 SECONDS, reward -0.02937251888215542, action 0\n",
      "EPISODE DONE IN 1.6355388164520264 SECONDS, reward -0.02937251888215542, action 7\n",
      "EPISODE DONE IN 2.1814093589782715 SECONDS, reward -0.03650243207812309, action 6\n",
      "\n",
      "\n",
      "EPISODE: 143\n",
      "EPISODE DONE IN 0.5869367122650146 SECONDS, reward 0.2548256516456604, action 1\n",
      "EPISODE DONE IN 1.1859893798828125 SECONDS, reward -0.3266022503376007, action 5\n",
      "EPISODE DONE IN 1.7416913509368896 SECONDS, reward -0.9839819073677063, action 0\n",
      "EPISODE DONE IN 2.251213788986206 SECONDS, reward 0.857713520526886, action 0\n",
      "\n",
      "\n",
      "EPISODE: 144\n",
      "EPISODE DONE IN 0.5207855701446533 SECONDS, reward 0.020883090794086456, action 0\n",
      "EPISODE DONE IN 1.034459114074707 SECONDS, reward 0.020883090794086456, action 5\n",
      "EPISODE DONE IN 1.5210561752319336 SECONDS, reward 0.020883090794086456, action 0\n",
      "EPISODE DONE IN 1.999239206314087 SECONDS, reward 0.020883090794086456, action 0\n",
      "\n",
      "\n",
      "EPISODE: 145\n",
      "EPISODE DONE IN 0.5426735877990723 SECONDS, reward 0.939697265625, action 2\n",
      "EPISODE DONE IN 1.080474615097046 SECONDS, reward 1.4916999340057373, action 7\n",
      "EPISODE DONE IN 1.6431212425231934 SECONDS, reward -0.1769305020570755, action 3\n",
      "EPISODE DONE IN 2.192289352416992 SECONDS, reward -0.1769305020570755, action 0\n",
      "\n",
      "\n",
      "EPISODE: 146\n",
      "EPISODE DONE IN 0.5792465209960938 SECONDS, reward -0.1769305020570755, action 2\n",
      "EPISODE DONE IN 1.1396703720092773 SECONDS, reward -0.0959395244717598, action 8\n",
      "EPISODE DONE IN 1.6818737983703613 SECONDS, reward -0.34852054715156555, action 7\n",
      "EPISODE DONE IN 2.2517738342285156 SECONDS, reward -0.34852054715156555, action 0\n",
      "\n",
      "\n",
      "EPISODE: 147\n",
      "EPISODE DONE IN 0.5836246013641357 SECONDS, reward -0.13253186643123627, action 5\n",
      "EPISODE DONE IN 1.1137800216674805 SECONDS, reward -0.02937251888215542, action 0\n",
      "EPISODE DONE IN 1.648012638092041 SECONDS, reward -0.5635496973991394, action 4\n",
      "EPISODE DONE IN 2.225649356842041 SECONDS, reward -0.6702232360839844, action 2\n",
      "\n",
      "\n",
      "EPISODE: 148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 0.5610344409942627 SECONDS, reward 0.5345990061759949, action 0\n",
      "EPISODE DONE IN 1.1228222846984863 SECONDS, reward 0.5345990061759949, action 0\n",
      "EPISODE DONE IN 1.631404161453247 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 2.061154842376709 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 149\n",
      "EPISODE DONE IN 0.4466991424560547 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.9713442325592041 SECONDS, reward -1.3662217855453491, action 4\n",
      "EPISODE DONE IN 1.4794731140136719 SECONDS, reward -1.3662217855453491, action 0\n",
      "EPISODE DONE IN 2.002321243286133 SECONDS, reward -0.23154957592487335, action 7\n",
      "\n",
      "\n",
      "EPISODE: 150\n",
      "EPISODE DONE IN 0.5377120971679688 SECONDS, reward -0.23154957592487335, action 0\n",
      "EPISODE DONE IN 1.08659029006958 SECONDS, reward -0.04206221178174019, action 5\n",
      "EPISODE DONE IN 1.6427812576293945 SECONDS, reward 0.0038442194927483797, action 7\n",
      "EPISODE DONE IN 2.17647647857666 SECONDS, reward -0.4915789067745209, action 1\n",
      "\n",
      "\n",
      "EPISODE: 151\n",
      "EPISODE DONE IN 0.5687942504882812 SECONDS, reward -0.4915789067745209, action 0\n",
      "EPISODE DONE IN 1.1441619396209717 SECONDS, reward -0.4096747636795044, action 4\n",
      "EPISODE DONE IN 1.6852214336395264 SECONDS, reward -1.6261696815490723, action 3\n",
      "EPISODE DONE IN 2.1967787742614746 SECONDS, reward -1.1906092166900635, action 0\n",
      "\n",
      "\n",
      "EPISODE: 152\n",
      "EPISODE DONE IN 0.5798816680908203 SECONDS, reward -0.08378848433494568, action 6\n",
      "EPISODE DONE IN 1.1375329494476318 SECONDS, reward 0.14075817167758942, action 8\n",
      "EPISODE DONE IN 1.6747992038726807 SECONDS, reward 0.011396042071282864, action 5\n",
      "EPISODE DONE IN 2.2541236877441406 SECONDS, reward -0.33582717180252075, action 2\n",
      "\n",
      "\n",
      "EPISODE: 153\n",
      "EPISODE DONE IN 0.5811452865600586 SECONDS, reward -0.8685962557792664, action 0\n",
      "EPISODE DONE IN 1.1241083145141602 SECONDS, reward -0.3044644594192505, action 2\n",
      "EPISODE DONE IN 1.6397857666015625 SECONDS, reward -3.053356885910034, action 0\n",
      "EPISODE DONE IN 2.148210287094116 SECONDS, reward -7.09658670425415, action 0\n",
      "\n",
      "\n",
      "EPISODE: 154\n",
      "EPISODE DONE IN 0.548729658126831 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 1.0838205814361572 SECONDS, reward -7.09658670425415, action 2\n",
      "EPISODE DONE IN 1.6420211791992188 SECONDS, reward -1.4999343156814575, action 6\n",
      "EPISODE DONE IN 2.1564152240753174 SECONDS, reward -1.4999343156814575, action 0\n",
      "\n",
      "\n",
      "EPISODE: 155\n",
      "EPISODE DONE IN 0.5264592170715332 SECONDS, reward -1.4999343156814575, action 0\n",
      "EPISODE DONE IN 1.0398733615875244 SECONDS, reward 1.074416160583496, action 0\n",
      "EPISODE DONE IN 1.5511808395385742 SECONDS, reward -7.0729827880859375, action 2\n",
      "EPISODE DONE IN 2.0308854579925537 SECONDS, reward -7.0729827880859375, action 0\n",
      "\n",
      "\n",
      "EPISODE: 156\n",
      "EPISODE DONE IN 0.5256986618041992 SECONDS, reward -1.6203471422195435, action 5\n",
      "EPISODE DONE IN 1.071934461593628 SECONDS, reward -1.6203471422195435, action 0\n",
      "EPISODE DONE IN 1.6195144653320312 SECONDS, reward -0.02937251888215542, action 7\n",
      "EPISODE DONE IN 2.2101516723632812 SECONDS, reward -0.3096939027309418, action 2\n",
      "\n",
      "\n",
      "EPISODE: 157\n",
      "EPISODE DONE IN 0.5871899127960205 SECONDS, reward -0.09188824146986008, action 2\n",
      "EPISODE DONE IN 1.1620523929595947 SECONDS, reward -0.09188824146986008, action 0\n",
      "EPISODE DONE IN 1.682082176208496 SECONDS, reward -3.03354811668396, action 0\n",
      "EPISODE DONE IN 2.1745245456695557 SECONDS, reward -7.049383640289307, action 0\n",
      "\n",
      "\n",
      "EPISODE: 158\n",
      "EPISODE DONE IN 0.5190787315368652 SECONDS, reward 1.9200869798660278, action 7\n",
      "EPISODE DONE IN 1.030472755432129 SECONDS, reward 1.9200869798660278, action 0\n",
      "EPISODE DONE IN 1.528892993927002 SECONDS, reward 1.9200869798660278, action 0\n",
      "EPISODE DONE IN 2.046875476837158 SECONDS, reward 0.00024068844504654408, action 1\n",
      "\n",
      "\n",
      "EPISODE: 159\n",
      "EPISODE DONE IN 0.5534512996673584 SECONDS, reward -1.7839012145996094, action 5\n",
      "EPISODE DONE IN 1.1121623516082764 SECONDS, reward -0.4127669930458069, action 1\n",
      "EPISODE DONE IN 1.6884331703186035 SECONDS, reward -0.5812061429023743, action 1\n",
      "EPISODE DONE IN 2.2463676929473877 SECONDS, reward -0.4127669930458069, action 0\n",
      "\n",
      "\n",
      "EPISODE: 160\n",
      "EPISODE DONE IN 0.5539150238037109 SECONDS, reward -7.20137357711792, action 0\n",
      "EPISODE DONE IN 1.0436961650848389 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.5270676612854004 SECONDS, reward -1.3503496646881104, action 4\n",
      "EPISODE DONE IN 2.0261478424072266 SECONDS, reward -1.3503496646881104, action 0\n",
      "\n",
      "\n",
      "EPISODE: 161\n",
      "EPISODE DONE IN 0.519439697265625 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.0385284423828125 SECONDS, reward -0.38944563269615173, action 6\n",
      "EPISODE DONE IN 1.5530340671539307 SECONDS, reward -0.18226875364780426, action 5\n",
      "EPISODE DONE IN 2.0934371948242188 SECONDS, reward -0.18226875364780426, action 0\n",
      "\n",
      "\n",
      "EPISODE: 162\n",
      "EPISODE DONE IN 0.5600142478942871 SECONDS, reward -0.18226875364780426, action 0\n",
      "EPISODE DONE IN 1.0837628841400146 SECONDS, reward -0.35611671209335327, action 5\n",
      "EPISODE DONE IN 1.5999460220336914 SECONDS, reward -0.04206221178174019, action 7\n",
      "EPISODE DONE IN 2.148305892944336 SECONDS, reward -0.04206221178174019, action 0\n",
      "\n",
      "\n",
      "EPISODE: 163\n",
      "EPISODE DONE IN 0.5486314296722412 SECONDS, reward -0.04206221178174019, action 0\n",
      "EPISODE DONE IN 1.105787754058838 SECONDS, reward -1.1282588243484497, action 2\n",
      "EPISODE DONE IN 1.6386706829071045 SECONDS, reward -2.3011415004730225, action 3\n",
      "EPISODE DONE IN 2.2134978771209717 SECONDS, reward -0.27274632453918457, action 4\n",
      "\n",
      "\n",
      "EPISODE: 164\n",
      "EPISODE DONE IN 0.5570640563964844 SECONDS, reward -0.27274632453918457, action 0\n",
      "EPISODE DONE IN 1.084273338317871 SECONDS, reward 0.6113405227661133, action 0\n",
      "EPISODE DONE IN 1.5961062908172607 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 2.045029640197754 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 165\n",
      "EPISODE DONE IN 0.5037136077880859 SECONDS, reward 2.6174464225769043, action 8\n",
      "EPISODE DONE IN 1.0182163715362549 SECONDS, reward 1.7100001573562622, action 2\n",
      "EPISODE DONE IN 1.5508880615234375 SECONDS, reward 1.7100001573562622, action 0\n",
      "EPISODE DONE IN 2.095121383666992 SECONDS, reward 1.7100001573562622, action 0\n",
      "\n",
      "\n",
      "EPISODE: 166\n",
      "EPISODE DONE IN 0.499129056930542 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 0.9229423999786377 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.4333157539367676 SECONDS, reward -3.4047372341156006, action 3\n",
      "EPISODE DONE IN 1.9497814178466797 SECONDS, reward -3.4047372341156006, action 0\n",
      "\n",
      "\n",
      "EPISODE: 167\n",
      "EPISODE DONE IN 0.4978322982788086 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 0.9873766899108887 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 1.4337246417999268 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.946624994277954 SECONDS, reward -17.0012149810791, action 1\n",
      "\n",
      "\n",
      "EPISODE: 168\n",
      "EPISODE DONE IN 0.5093691349029541 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 0.9903693199157715 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.526585578918457 SECONDS, reward 0.38370925188064575, action 4\n",
      "EPISODE DONE IN 2.069488763809204 SECONDS, reward 1.2911248207092285, action 6\n",
      "\n",
      "\n",
      "EPISODE: 169\n",
      "EPISODE DONE IN 0.5187509059906006 SECONDS, reward 1.2911248207092285, action 0\n",
      "EPISODE DONE IN 1.0408668518066406 SECONDS, reward 1.2911248207092285, action 0\n",
      "EPISODE DONE IN 1.579148769378662 SECONDS, reward 1.6599963903427124, action 8\n",
      "EPISODE DONE IN 2.094958782196045 SECONDS, reward 2.6046342849731445, action 0\n",
      "\n",
      "\n",
      "EPISODE: 170\n",
      "EPISODE DONE IN 0.4891014099121094 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 0.9675130844116211 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.4214749336242676 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.8741743564605713 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 171\n",
      "EPISODE DONE IN 0.45462584495544434 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.9360253810882568 SECONDS, reward -1.3662217855453491, action 4\n",
      "EPISODE DONE IN 1.4750385284423828 SECONDS, reward -1.1906092166900635, action 3\n",
      "EPISODE DONE IN 2.027165174484253 SECONDS, reward -1.1906092166900635, action 0\n",
      "\n",
      "\n",
      "EPISODE: 172\n",
      "EPISODE DONE IN 0.5283017158508301 SECONDS, reward -1.1906092166900635, action 0\n",
      "EPISODE DONE IN 1.0091326236724854 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 1.4523797035217285 SECONDS, reward -10000000.0, action 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 1.9020564556121826 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 173\n",
      "EPISODE DONE IN 0.5118062496185303 SECONDS, reward -1.3344745635986328, action 4\n",
      "EPISODE DONE IN 0.9915730953216553 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 1.478712797164917 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 1.9564809799194336 SECONDS, reward -1.3344745635986328, action 0\n",
      "\n",
      "\n",
      "EPISODE: 174\n",
      "EPISODE DONE IN 0.45529842376708984 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.973543643951416 SECONDS, reward 1.8937617540359497, action 7\n",
      "EPISODE DONE IN 1.5344161987304688 SECONDS, reward -1.1390821933746338, action 2\n",
      "EPISODE DONE IN 2.1920855045318604 SECONDS, reward -1.143761396408081, action 4\n",
      "\n",
      "\n",
      "EPISODE: 175\n",
      "EPISODE DONE IN 0.576850175857544 SECONDS, reward -0.1002868041396141, action 8\n",
      "EPISODE DONE IN 1.1196293830871582 SECONDS, reward -1.3601070642471313, action 0\n",
      "EPISODE DONE IN 1.7125554084777832 SECONDS, reward -0.22436289489269257, action 8\n",
      "EPISODE DONE IN 2.2637975215911865 SECONDS, reward 0.3979148268699646, action 0\n",
      "\n",
      "\n",
      "EPISODE: 176\n",
      "EPISODE DONE IN 0.5309829711914062 SECONDS, reward -0.44119709730148315, action 2\n",
      "EPISODE DONE IN 1.0766620635986328 SECONDS, reward -0.44119709730148315, action 0\n",
      "EPISODE DONE IN 1.643465280532837 SECONDS, reward -1.4837092161178589, action 6\n",
      "EPISODE DONE IN 2.2055141925811768 SECONDS, reward 0.16497348248958588, action 7\n",
      "\n",
      "\n",
      "EPISODE: 177\n",
      "EPISODE DONE IN 0.5555739402770996 SECONDS, reward 0.08909468352794647, action 3\n",
      "EPISODE DONE IN 1.1652801036834717 SECONDS, reward -0.9171556234359741, action 7\n",
      "EPISODE DONE IN 1.7517931461334229 SECONDS, reward -0.3198217451572418, action 0\n",
      "EPISODE DONE IN 2.3025882244110107 SECONDS, reward -0.26861801743507385, action 6\n",
      "\n",
      "\n",
      "EPISODE: 178\n",
      "EPISODE DONE IN 0.5718529224395752 SECONDS, reward -0.08748513460159302, action 6\n",
      "EPISODE DONE IN 1.1344399452209473 SECONDS, reward -0.019977696239948273, action 0\n",
      "EPISODE DONE IN 1.7262451648712158 SECONDS, reward -0.14167310297489166, action 6\n",
      "EPISODE DONE IN 2.2932662963867188 SECONDS, reward -0.08508513122797012, action 7\n",
      "\n",
      "\n",
      "EPISODE: 179\n",
      "EPISODE DONE IN 0.5646727085113525 SECONDS, reward 1.514892816543579, action 0\n",
      "EPISODE DONE IN 1.152106761932373 SECONDS, reward -0.04079242795705795, action 5\n",
      "EPISODE DONE IN 1.7127087116241455 SECONDS, reward -0.15156108140945435, action 4\n",
      "EPISODE DONE IN 2.2942135334014893 SECONDS, reward -0.21190756559371948, action 6\n",
      "\n",
      "\n",
      "EPISODE: 180\n",
      "EPISODE DONE IN 0.627434253692627 SECONDS, reward -1.3653583526611328, action 4\n",
      "EPISODE DONE IN 1.2265679836273193 SECONDS, reward -1.688422441482544, action 6\n",
      "EPISODE DONE IN 1.8124771118164062 SECONDS, reward -1.688422441482544, action 4\n",
      "EPISODE DONE IN 2.4078030586242676 SECONDS, reward -0.14986303448677063, action 0\n",
      "\n",
      "\n",
      "EPISODE: 181\n",
      "EPISODE DONE IN 0.5579099655151367 SECONDS, reward -0.38944563269615173, action 0\n",
      "EPISODE DONE IN 1.0595827102661133 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.5265638828277588 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.0616259574890137 SECONDS, reward 1.90692138671875, action 7\n",
      "\n",
      "\n",
      "EPISODE: 182\n",
      "EPISODE DONE IN 0.5632286071777344 SECONDS, reward 0.10833720862865448, action 6\n",
      "EPISODE DONE IN 1.1022403240203857 SECONDS, reward 0.10833720862865448, action 0\n",
      "EPISODE DONE IN 1.6606462001800537 SECONDS, reward 0.10833720862865448, action 0\n",
      "EPISODE DONE IN 2.1937241554260254 SECONDS, reward 1.0881047248840332, action 0\n",
      "\n",
      "\n",
      "EPISODE: 183\n",
      "EPISODE DONE IN 0.5253763198852539 SECONDS, reward -1.3344745635986328, action 4\n",
      "EPISODE DONE IN 1.0374116897583008 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 1.5763804912567139 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 2.137904405593872 SECONDS, reward -1.1766407489776611, action 3\n",
      "\n",
      "\n",
      "EPISODE: 184\n",
      "EPISODE DONE IN 0.5326061248779297 SECONDS, reward -4.660695552825928, action 1\n",
      "EPISODE DONE IN 1.0689008235931396 SECONDS, reward -4.660695552825928, action 0\n",
      "EPISODE DONE IN 1.6673812866210938 SECONDS, reward -4.660695552825928, action 0\n",
      "EPISODE DONE IN 2.1838855743408203 SECONDS, reward -16.960826873779297, action 0\n",
      "\n",
      "\n",
      "EPISODE: 185\n",
      "EPISODE DONE IN 0.4558749198913574 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.9055140018463135 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.4451682567596436 SECONDS, reward 0.03539615496993065, action 5\n",
      "EPISODE DONE IN 1.9812026023864746 SECONDS, reward 0.03539615496993065, action 0\n",
      "\n",
      "\n",
      "EPISODE: 186\n",
      "EPISODE DONE IN 0.5738801956176758 SECONDS, reward -0.17037196457386017, action 6\n",
      "EPISODE DONE IN 1.1411526203155518 SECONDS, reward -0.2936738431453705, action 3\n",
      "EPISODE DONE IN 1.679969072341919 SECONDS, reward 1.2700488567352295, action 0\n",
      "EPISODE DONE IN 2.2660276889801025 SECONDS, reward -0.07823359966278076, action 2\n",
      "\n",
      "\n",
      "EPISODE: 187\n",
      "EPISODE DONE IN 0.5788147449493408 SECONDS, reward -2.3019416332244873, action 0\n",
      "EPISODE DONE IN 1.1154561042785645 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 1.6846771240234375 SECONDS, reward -1.246695637702942, action 2\n",
      "EPISODE DONE IN 2.2515130043029785 SECONDS, reward 1.7036830186843872, action 8\n",
      "\n",
      "\n",
      "EPISODE: 188\n",
      "EPISODE DONE IN 0.5434508323669434 SECONDS, reward 1.7036830186843872, action 0\n",
      "EPISODE DONE IN 1.0678975582122803 SECONDS, reward 1.7036830186843872, action 0\n",
      "EPISODE DONE IN 1.6190409660339355 SECONDS, reward 1.6535899639129639, action 5\n",
      "EPISODE DONE IN 2.197267532348633 SECONDS, reward 1.6535899639129639, action 8\n",
      "\n",
      "\n",
      "EPISODE: 189\n",
      "EPISODE DONE IN 0.5536849498748779 SECONDS, reward 1.6535899639129639, action 0\n",
      "EPISODE DONE IN 1.0801382064819336 SECONDS, reward 1.6535899639129639, action 0\n",
      "EPISODE DONE IN 1.6496400833129883 SECONDS, reward 1.6646051406860352, action 3\n",
      "EPISODE DONE IN 2.200131893157959 SECONDS, reward 1.485865592956543, action 7\n",
      "\n",
      "\n",
      "EPISODE: 190\n",
      "EPISODE DONE IN 0.5284359455108643 SECONDS, reward 1.485865592956543, action 0\n",
      "EPISODE DONE IN 1.062741994857788 SECONDS, reward 1.485865592956543, action 0\n",
      "EPISODE DONE IN 1.5994348526000977 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 2.1119561195373535 SECONDS, reward 2.6046342849731445, action 8\n",
      "\n",
      "\n",
      "EPISODE: 191\n",
      "EPISODE DONE IN 0.576366662979126 SECONDS, reward 0.09173913300037384, action 5\n",
      "EPISODE DONE IN 1.1406943798065186 SECONDS, reward 0.09173913300037384, action 0\n",
      "EPISODE DONE IN 1.720853567123413 SECONDS, reward 0.09173913300037384, action 0\n",
      "EPISODE DONE IN 2.257124423980713 SECONDS, reward -1.6203471422195435, action 2\n",
      "\n",
      "\n",
      "EPISODE: 192\n",
      "EPISODE DONE IN 0.5551600456237793 SECONDS, reward -5.133672714233398, action 1\n",
      "EPISODE DONE IN 1.1188511848449707 SECONDS, reward -5.133672714233398, action 0\n",
      "EPISODE DONE IN 1.713198184967041 SECONDS, reward 0.01561641599982977, action 7\n",
      "EPISODE DONE IN 2.272986888885498 SECONDS, reward 0.2894028425216675, action 7\n",
      "\n",
      "\n",
      "EPISODE: 193\n",
      "EPISODE DONE IN 0.5833001136779785 SECONDS, reward 0.06767349690198898, action 4\n",
      "EPISODE DONE IN 1.1851511001586914 SECONDS, reward 0.06767349690198898, action 0\n",
      "EPISODE DONE IN 1.723252534866333 SECONDS, reward -0.22013716399669647, action 0\n",
      "EPISODE DONE IN 2.220019817352295 SECONDS, reward -1.3344745635986328, action 0\n",
      "\n",
      "\n",
      "EPISODE: 194\n",
      "EPISODE DONE IN 0.5295751094818115 SECONDS, reward -7.049383640289307, action 2\n",
      "EPISODE DONE IN 1.0596511363983154 SECONDS, reward -7.049383640289307, action 0\n",
      "EPISODE DONE IN 1.5577328205108643 SECONDS, reward -7.049383640289307, action 0\n",
      "EPISODE DONE IN 2.055101156234741 SECONDS, reward -7.049383640289307, action 0\n",
      "\n",
      "\n",
      "EPISODE: 195\n",
      "EPISODE DONE IN 0.4683797359466553 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.9339809417724609 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.4393410682678223 SECONDS, reward 2.6046342849731445, action 8\n",
      "EPISODE DONE IN 1.9595494270324707 SECONDS, reward 1.669970154762268, action 7\n",
      "\n",
      "\n",
      "EPISODE: 196\n",
      "EPISODE DONE IN 0.5906221866607666 SECONDS, reward 0.33719220757484436, action 1\n",
      "EPISODE DONE IN 1.171170711517334 SECONDS, reward 0.33719220757484436, action 0\n",
      "EPISODE DONE IN 1.7268259525299072 SECONDS, reward 0.21403712034225464, action 5\n",
      "EPISODE DONE IN 2.2979490756988525 SECONDS, reward 0.005768147762864828, action 4\n",
      "\n",
      "\n",
      "EPISODE: 197\n",
      "EPISODE DONE IN 0.5940043926239014 SECONDS, reward 0.005768147762864828, action 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 1.17685866355896 SECONDS, reward -1.0650131702423096, action 6\n",
      "EPISODE DONE IN 1.7549738883972168 SECONDS, reward -1.0650131702423096, action 5\n",
      "EPISODE DONE IN 2.34547758102417 SECONDS, reward -0.6068392992019653, action 0\n",
      "\n",
      "\n",
      "EPISODE: 198\n",
      "EPISODE DONE IN 0.5793719291687012 SECONDS, reward -0.14849433302879333, action 5\n",
      "EPISODE DONE IN 1.1045212745666504 SECONDS, reward -0.34341686964035034, action 0\n",
      "EPISODE DONE IN 1.6603574752807617 SECONDS, reward 0.04991356283426285, action 0\n",
      "EPISODE DONE IN 2.226088047027588 SECONDS, reward -1.6203471422195435, action 2\n",
      "\n",
      "\n",
      "EPISODE: 199\n",
      "EPISODE DONE IN 0.5630619525909424 SECONDS, reward -2.2807090282440186, action 3\n",
      "EPISODE DONE IN 1.1146337985992432 SECONDS, reward -0.8736726641654968, action 1\n",
      "EPISODE DONE IN 1.7053802013397217 SECONDS, reward -0.8736726641654968, action 0\n",
      "EPISODE DONE IN 2.2968764305114746 SECONDS, reward -1.3457903861999512, action 8\n",
      "\n",
      "\n",
      "EPISODE: 200\n",
      "EPISODE DONE IN 0.5584537982940674 SECONDS, reward -0.5880107283592224, action 8\n",
      "EPISODE DONE IN 1.104842185974121 SECONDS, reward 0.3979148268699646, action 0\n",
      "EPISODE DONE IN 1.6714227199554443 SECONDS, reward 0.3979148268699646, action 0\n",
      "EPISODE DONE IN 2.180446147918701 SECONDS, reward 2.6174464225769043, action 0\n",
      "\n",
      "\n",
      "EPISODE: 201\n",
      "EPISODE DONE IN 0.5005908012390137 SECONDS, reward -1.3344745635986328, action 4\n",
      "EPISODE DONE IN 1.0169308185577393 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 1.5831153392791748 SECONDS, reward -0.8151940703392029, action 4\n",
      "EPISODE DONE IN 2.172600746154785 SECONDS, reward -0.31544968485832214, action 4\n",
      "\n",
      "\n",
      "EPISODE: 202\n",
      "EPISODE DONE IN 0.5609683990478516 SECONDS, reward 0.6520048379898071, action 0\n",
      "EPISODE DONE IN 1.103973150253296 SECONDS, reward 0.6520048379898071, action 0\n",
      "EPISODE DONE IN 1.637108564376831 SECONDS, reward 0.38370925188064575, action 1\n",
      "EPISODE DONE IN 2.1722195148468018 SECONDS, reward -17.0012149810791, action 0\n",
      "\n",
      "\n",
      "EPISODE: 203\n",
      "EPISODE DONE IN 0.5456476211547852 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.076477289199829 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.5379128456115723 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.0550074577331543 SECONDS, reward -17.0012149810791, action 1\n",
      "\n",
      "\n",
      "EPISODE: 204\n",
      "EPISODE DONE IN 0.5233316421508789 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.0554897785186768 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.5905861854553223 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 2.0574042797088623 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 205\n",
      "EPISODE DONE IN 0.5143694877624512 SECONDS, reward 2.6174464225769043, action 8\n",
      "EPISODE DONE IN 1.0134406089782715 SECONDS, reward 2.6174464225769043, action 0\n",
      "EPISODE DONE IN 1.5914959907531738 SECONDS, reward 0.5061954259872437, action 1\n",
      "EPISODE DONE IN 2.1581132411956787 SECONDS, reward 0.5061954259872437, action 0\n",
      "\n",
      "\n",
      "EPISODE: 206\n",
      "EPISODE DONE IN 0.5178654193878174 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.0617263317108154 SECONDS, reward -7.20137357711792, action 1\n",
      "EPISODE DONE IN 1.627777099609375 SECONDS, reward -17.041624069213867, action 0\n",
      "EPISODE DONE IN 2.212409496307373 SECONDS, reward -1.7963917255401611, action 5\n",
      "\n",
      "\n",
      "EPISODE: 207\n",
      "EPISODE DONE IN 0.6160228252410889 SECONDS, reward -0.7405684590339661, action 5\n",
      "EPISODE DONE IN 1.1658642292022705 SECONDS, reward -0.34341686964035034, action 0\n",
      "EPISODE DONE IN 1.7023134231567383 SECONDS, reward -0.34341686964035034, action 0\n",
      "EPISODE DONE IN 2.259331226348877 SECONDS, reward -0.34341686964035034, action 5\n",
      "\n",
      "\n",
      "EPISODE: 208\n",
      "EPISODE DONE IN 0.5337622165679932 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 1.0449552536010742 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 1.550262689590454 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 2.0125784873962402 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 209\n",
      "EPISODE DONE IN 0.47747278213500977 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.0140416622161865 SECONDS, reward -17.041624069213867, action 1\n",
      "EPISODE DONE IN 1.5843665599822998 SECONDS, reward -0.010994260199368, action 7\n",
      "EPISODE DONE IN 2.118830680847168 SECONDS, reward -0.010994260199368, action 0\n",
      "\n",
      "\n",
      "EPISODE: 210\n",
      "EPISODE DONE IN 0.5398774147033691 SECONDS, reward -0.010994260199368, action 0\n",
      "EPISODE DONE IN 1.0700345039367676 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 1.6038470268249512 SECONDS, reward 0.03539615496993065, action 5\n",
      "EPISODE DONE IN 2.13580060005188 SECONDS, reward 0.03539615496993065, action 0\n",
      "\n",
      "\n",
      "EPISODE: 211\n",
      "EPISODE DONE IN 0.5563473701477051 SECONDS, reward 0.09309913218021393, action 8\n",
      "EPISODE DONE IN 1.151212215423584 SECONDS, reward 0.052535392343997955, action 7\n",
      "EPISODE DONE IN 1.7022361755371094 SECONDS, reward 1.6725386381149292, action 0\n",
      "EPISODE DONE IN 2.291527032852173 SECONDS, reward 0.25052252411842346, action 2\n",
      "\n",
      "\n",
      "EPISODE: 212\n",
      "EPISODE DONE IN 0.5371899604797363 SECONDS, reward -1.1390821933746338, action 0\n",
      "EPISODE DONE IN 1.0323412418365479 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 1.56337308883667 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 2.095454692840576 SECONDS, reward 1.90692138671875, action 7\n",
      "\n",
      "\n",
      "EPISODE: 213\n",
      "EPISODE DONE IN 0.5005388259887695 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 0.9970448017120361 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 1.5583946704864502 SECONDS, reward 1.49413001537323, action 2\n",
      "EPISODE DONE IN 2.088552236557007 SECONDS, reward -7.0729827880859375, action 0\n",
      "\n",
      "\n",
      "EPISODE: 214\n",
      "EPISODE DONE IN 0.533336877822876 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 1.0530898571014404 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 1.4931111335754395 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.995330810546875 SECONDS, reward -1.3503496646881104, action 4\n",
      "\n",
      "\n",
      "EPISODE: 215\n",
      "EPISODE DONE IN 0.5351893901824951 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.066037654876709 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.6264779567718506 SECONDS, reward 1.0306061506271362, action 5\n",
      "EPISODE DONE IN 2.138542890548706 SECONDS, reward 0.03539615496993065, action 0\n",
      "\n",
      "\n",
      "EPISODE: 216\n",
      "EPISODE DONE IN 0.529700517654419 SECONDS, reward -0.9063183069229126, action 3\n",
      "EPISODE DONE IN 1.104034662246704 SECONDS, reward -1.1284679174423218, action 3\n",
      "EPISODE DONE IN 1.6960630416870117 SECONDS, reward -1.361255407333374, action 6\n",
      "EPISODE DONE IN 2.275573253631592 SECONDS, reward -0.5533491373062134, action 4\n",
      "\n",
      "\n",
      "EPISODE: 217\n",
      "EPISODE DONE IN 0.5823361873626709 SECONDS, reward -1.3650095462799072, action 6\n",
      "EPISODE DONE IN 1.1709692478179932 SECONDS, reward -0.25700655579566956, action 0\n",
      "EPISODE DONE IN 1.7640385627746582 SECONDS, reward -0.3240374028682709, action 5\n",
      "EPISODE DONE IN 2.3331587314605713 SECONDS, reward -0.17037196457386017, action 0\n",
      "\n",
      "\n",
      "EPISODE: 218\n",
      "EPISODE DONE IN 0.5437202453613281 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 1.0741181373596191 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 1.5806667804718018 SECONDS, reward -7.0729827880859375, action 2\n",
      "EPISODE DONE IN 2.076462984085083 SECONDS, reward -7.0729827880859375, action 0\n",
      "\n",
      "\n",
      "EPISODE: 219\n",
      "EPISODE DONE IN 0.5401694774627686 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 1.1039319038391113 SECONDS, reward -1.6391124725341797, action 5\n",
      "EPISODE DONE IN 1.6047332286834717 SECONDS, reward 0.020883090794086456, action 0\n",
      "EPISODE DONE IN 2.10190486907959 SECONDS, reward 0.020883090794086456, action 0\n",
      "\n",
      "\n",
      "EPISODE: 220\n",
      "EPISODE DONE IN 0.5341546535491943 SECONDS, reward 0.020883090794086456, action 0\n",
      "EPISODE DONE IN 1.0609309673309326 SECONDS, reward 2.5918285846710205, action 8\n",
      "EPISODE DONE IN 1.5920894145965576 SECONDS, reward -0.43465009331703186, action 3\n",
      "EPISODE DONE IN 2.1283326148986816 SECONDS, reward -0.43465009331703186, action 0\n",
      "\n",
      "\n",
      "EPISODE: 221\n",
      "EPISODE DONE IN 0.5685229301452637 SECONDS, reward -0.43465009331703186, action 0\n",
      "EPISODE DONE IN 1.1120994091033936 SECONDS, reward -2.3011415004730225, action 2\n",
      "EPISODE DONE IN 1.6458854675292969 SECONDS, reward -1.4999343156814575, action 6\n",
      "EPISODE DONE IN 2.2246549129486084 SECONDS, reward 0.20946721732616425, action 8\n",
      "\n",
      "\n",
      "EPISODE: 222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 0.5963196754455566 SECONDS, reward 0.20946721732616425, action 0\n",
      "EPISODE DONE IN 1.119208574295044 SECONDS, reward 1.6599963903427124, action 0\n",
      "EPISODE DONE IN 1.6198222637176514 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 2.145961284637451 SECONDS, reward -1.3503496646881104, action 4\n",
      "\n",
      "\n",
      "EPISODE: 223\n",
      "EPISODE DONE IN 0.5315957069396973 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.026911735534668 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.5532655715942383 SECONDS, reward 0.6113405227661133, action 3\n",
      "EPISODE DONE IN 2.110891103744507 SECONDS, reward 1.485865592956543, action 7\n",
      "\n",
      "\n",
      "EPISODE: 224\n",
      "EPISODE DONE IN 0.5837290287017822 SECONDS, reward 0.0042036124505102634, action 2\n",
      "EPISODE DONE IN 1.155113935470581 SECONDS, reward -0.46047618985176086, action 3\n",
      "EPISODE DONE IN 1.7341053485870361 SECONDS, reward -1.888031244277954, action 0\n",
      "EPISODE DONE IN 2.289818048477173 SECONDS, reward -0.07651835680007935, action 0\n",
      "\n",
      "\n",
      "EPISODE: 225\n",
      "EPISODE DONE IN 0.5315923690795898 SECONDS, reward 1.2700488567352295, action 6\n",
      "EPISODE DONE IN 1.0510776042938232 SECONDS, reward 1.253429651260376, action 2\n",
      "EPISODE DONE IN 1.6071956157684326 SECONDS, reward 1.253429651260376, action 0\n",
      "EPISODE DONE IN 2.1608221530914307 SECONDS, reward 1.253429651260376, action 0\n",
      "\n",
      "\n",
      "EPISODE: 226\n",
      "EPISODE DONE IN 0.5598578453063965 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 1.074453592300415 SECONDS, reward -7.09658670425415, action 2\n",
      "EPISODE DONE IN 1.6107985973358154 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 2.16731595993042 SECONDS, reward -7.09658670425415, action 0\n",
      "\n",
      "\n",
      "EPISODE: 227\n",
      "EPISODE DONE IN 0.5480124950408936 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 1.03316330909729 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.5687541961669922 SECONDS, reward -3.4047372341156006, action 3\n",
      "EPISODE DONE IN 2.1192526817321777 SECONDS, reward -0.29685959219932556, action 1\n",
      "\n",
      "\n",
      "EPISODE: 228\n",
      "EPISODE DONE IN 0.5886132717132568 SECONDS, reward -0.29685959219932556, action 0\n",
      "EPISODE DONE IN 1.164491891860962 SECONDS, reward -0.29685959219932556, action 0\n",
      "EPISODE DONE IN 1.7233610153198242 SECONDS, reward 1.5414730310440063, action 7\n",
      "EPISODE DONE IN 2.221367120742798 SECONDS, reward 1.90692138671875, action 0\n",
      "\n",
      "\n",
      "EPISODE: 229\n",
      "EPISODE DONE IN 0.5357894897460938 SECONDS, reward -0.22013716399669647, action 4\n",
      "EPISODE DONE IN 1.0978140830993652 SECONDS, reward -0.22013716399669647, action 0\n",
      "EPISODE DONE IN 1.6642241477966309 SECONDS, reward -0.08761638402938843, action 8\n",
      "EPISODE DONE IN 2.2679941654205322 SECONDS, reward -0.08807411789894104, action 3\n",
      "\n",
      "\n",
      "EPISODE: 230\n",
      "EPISODE DONE IN 0.568218469619751 SECONDS, reward -0.29054832458496094, action 8\n",
      "EPISODE DONE IN 1.1196918487548828 SECONDS, reward -0.29054832458496094, action 0\n",
      "EPISODE DONE IN 1.6832787990570068 SECONDS, reward -0.41797882318496704, action 0\n",
      "EPISODE DONE IN 2.242358922958374 SECONDS, reward 0.3979148268699646, action 8\n",
      "\n",
      "\n",
      "EPISODE: 231\n",
      "EPISODE DONE IN 0.5305435657501221 SECONDS, reward -0.42051881551742554, action 3\n",
      "EPISODE DONE IN 1.0641651153564453 SECONDS, reward -0.42051881551742554, action 0\n",
      "EPISODE DONE IN 1.658503532409668 SECONDS, reward -0.23342488706111908, action 1\n",
      "EPISODE DONE IN 2.200981378555298 SECONDS, reward -4.6587958335876465, action 0\n",
      "\n",
      "\n",
      "EPISODE: 232\n",
      "EPISODE DONE IN 0.5006265640258789 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.0151875019073486 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.4822807312011719 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.9940330982208252 SECONDS, reward 2.6046342849731445, action 8\n",
      "\n",
      "\n",
      "EPISODE: 233\n",
      "EPISODE DONE IN 0.4993629455566406 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.0491998195648193 SECONDS, reward 0.20716089010238647, action 6\n",
      "EPISODE DONE IN 1.6179890632629395 SECONDS, reward 0.20716089010238647, action 0\n",
      "EPISODE DONE IN 2.1534409523010254 SECONDS, reward -0.793126106262207, action 1\n",
      "\n",
      "\n",
      "EPISODE: 234\n",
      "EPISODE DONE IN 0.5326390266418457 SECONDS, reward -0.793126106262207, action 0\n",
      "EPISODE DONE IN 1.0601418018341064 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.5926318168640137 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 2.0355727672576904 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 235\n",
      "EPISODE DONE IN 0.44036436080932617 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.8984198570251465 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.3680365085601807 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.8361830711364746 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 236\n",
      "EPISODE DONE IN 0.532874584197998 SECONDS, reward 2.6174464225769043, action 8\n",
      "EPISODE DONE IN 1.0427324771881104 SECONDS, reward 2.6174464225769043, action 0\n",
      "EPISODE DONE IN 1.599527359008789 SECONDS, reward 0.09309913218021393, action 5\n",
      "EPISODE DONE IN 2.1830344200134277 SECONDS, reward 0.09309913218021393, action 0\n",
      "\n",
      "\n",
      "EPISODE: 237\n",
      "EPISODE DONE IN 0.5727887153625488 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 1.1104989051818848 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 1.6056346893310547 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.0753767490386963 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 238\n",
      "EPISODE DONE IN 0.5315849781036377 SECONDS, reward 2.6174464225769043, action 8\n",
      "EPISODE DONE IN 1.0569190979003906 SECONDS, reward 2.6174464225769043, action 0\n",
      "EPISODE DONE IN 1.5670490264892578 SECONDS, reward 2.6174464225769043, action 0\n",
      "EPISODE DONE IN 2.1033382415771484 SECONDS, reward 2.6174464225769043, action 0\n",
      "\n",
      "\n",
      "EPISODE: 239\n",
      "EPISODE DONE IN 0.5597274303436279 SECONDS, reward 0.04991356283426285, action 5\n",
      "EPISODE DONE IN 1.1302049160003662 SECONDS, reward 0.04991356283426285, action 0\n",
      "EPISODE DONE IN 1.7030038833618164 SECONDS, reward -1.7839012145996094, action 1\n",
      "EPISODE DONE IN 2.2659754753112793 SECONDS, reward -1.7839012145996094, action 0\n",
      "\n",
      "\n",
      "EPISODE: 240\n",
      "EPISODE DONE IN 0.6687443256378174 SECONDS, reward -0.7702431082725525, action 6\n",
      "EPISODE DONE IN 1.3048040866851807 SECONDS, reward -0.7702431082725525, action 0\n",
      "EPISODE DONE IN 1.8055179119110107 SECONDS, reward 1.0881047248840332, action 0\n",
      "EPISODE DONE IN 2.302412509918213 SECONDS, reward 1.0881047248840332, action 0\n",
      "\n",
      "\n",
      "EPISODE: 241\n",
      "EPISODE DONE IN 0.4411792755126953 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.8812093734741211 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.3217573165893555 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.762221097946167 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 242\n",
      "EPISODE DONE IN 0.4423205852508545 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 0.8810503482818604 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.391568899154663 SECONDS, reward -17.0012149810791, action 1\n",
      "EPISODE DONE IN 1.9129385948181152 SECONDS, reward -17.0012149810791, action 0\n",
      "\n",
      "\n",
      "EPISODE: 243\n",
      "EPISODE DONE IN 0.5992028713226318 SECONDS, reward 0.5061954259872437, action 8\n",
      "EPISODE DONE IN 1.1994857788085938 SECONDS, reward 0.2177886664867401, action 4\n",
      "EPISODE DONE IN 1.7544655799865723 SECONDS, reward -0.03173702582716942, action 4\n",
      "EPISODE DONE IN 2.3575751781463623 SECONDS, reward -0.03173702582716942, action 0\n",
      "\n",
      "\n",
      "EPISODE: 244\n",
      "EPISODE DONE IN 0.6017906665802002 SECONDS, reward -0.8290855884552002, action 0\n",
      "EPISODE DONE IN 1.1919281482696533 SECONDS, reward -1.8887147903442383, action 2\n",
      "EPISODE DONE IN 1.7431631088256836 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 2.305483818054199 SECONDS, reward -7.09658670425415, action 0\n",
      "\n",
      "\n",
      "EPISODE: 245\n",
      "EPISODE DONE IN 0.5740671157836914 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 1.144606351852417 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.800058126449585 SECONDS, reward -7.0729827880859375, action 2\n",
      "EPISODE DONE IN 2.4463279247283936 SECONDS, reward -1.4551177024841309, action 1\n",
      "\n",
      "\n",
      "EPISODE: 246\n",
      "EPISODE DONE IN 0.6499600410461426 SECONDS, reward -1.4551177024841309, action 0\n",
      "EPISODE DONE IN 1.2744200229644775 SECONDS, reward -1.4551177024841309, action 0\n",
      "EPISODE DONE IN 1.8513023853302002 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 2.3539552688598633 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 0.5797836780548096 SECONDS, reward 1.0881047248840332, action 6\n",
      "EPISODE DONE IN 1.1528840065002441 SECONDS, reward 1.0881047248840332, action 0\n",
      "EPISODE DONE IN 1.72847318649292 SECONDS, reward 1.0881047248840332, action 0\n",
      "EPISODE DONE IN 2.336498737335205 SECONDS, reward -1.4812191724777222, action 2\n",
      "\n",
      "\n",
      "EPISODE: 248\n",
      "EPISODE DONE IN 0.6114358901977539 SECONDS, reward -0.43096116185188293, action 8\n",
      "EPISODE DONE IN 1.2482919692993164 SECONDS, reward -0.009260287508368492, action 2\n",
      "EPISODE DONE IN 1.8889176845550537 SECONDS, reward -0.009260287508368492, action 0\n",
      "EPISODE DONE IN 2.48856258392334 SECONDS, reward 1.7100001573562622, action 0\n",
      "\n",
      "\n",
      "EPISODE: 249\n",
      "EPISODE DONE IN 0.5764429569244385 SECONDS, reward -7.09658670425415, action 0\n",
      "EPISODE DONE IN 1.1509864330291748 SECONDS, reward 1.0607329607009888, action 6\n",
      "EPISODE DONE IN 1.7268338203430176 SECONDS, reward 1.0607329607009888, action 0\n",
      "EPISODE DONE IN 2.3265621662139893 SECONDS, reward 1.0607329607009888, action 0\n",
      "\n",
      "\n",
      "EPISODE: 250\n",
      "EPISODE DONE IN 0.5807633399963379 SECONDS, reward 1.0607329607009888, action 0\n",
      "EPISODE DONE IN 1.0818283557891846 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.5847437381744385 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.085141658782959 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 251\n",
      "EPISODE DONE IN 0.5036265850067139 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.0045654773712158 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.5078909397125244 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.0092217922210693 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 252\n",
      "EPISODE DONE IN 0.5045442581176758 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.076645851135254 SECONDS, reward 1.0607329607009888, action 6\n",
      "EPISODE DONE IN 1.6824898719787598 SECONDS, reward 1.0607329607009888, action 0\n",
      "EPISODE DONE IN 2.2973313331604004 SECONDS, reward 1.0607329607009888, action 0\n",
      "\n",
      "\n",
      "EPISODE: 253\n",
      "EPISODE DONE IN 0.5986132621765137 SECONDS, reward 1.0607329607009888, action 0\n",
      "EPISODE DONE IN 1.1329128742218018 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.6851041316986084 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.2251739501953125 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 254\n",
      "EPISODE DONE IN 0.541863203048706 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.0511066913604736 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.6545825004577637 SECONDS, reward -1.3503496646881104, action 4\n",
      "EPISODE DONE IN 2.272881269454956 SECONDS, reward -1.3503496646881104, action 0\n",
      "\n",
      "\n",
      "EPISODE: 255\n",
      "EPISODE DONE IN 0.5915517807006836 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.163759469985962 SECONDS, reward -1.3503496646881104, action 0\n",
      "EPISODE DONE IN 1.6683790683746338 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.1691062450408936 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 256\n",
      "EPISODE DONE IN 0.5050919055938721 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.1176342964172363 SECONDS, reward 0.020883090794086456, action 5\n",
      "EPISODE DONE IN 1.7518894672393799 SECONDS, reward -0.9218730926513672, action 3\n",
      "EPISODE DONE IN 2.3941688537597656 SECONDS, reward -1.1396158933639526, action 3\n",
      "\n",
      "\n",
      "EPISODE: 257\n",
      "EPISODE DONE IN 0.7130343914031982 SECONDS, reward -1.4929773807525635, action 2\n",
      "EPISODE DONE IN 1.3879992961883545 SECONDS, reward -0.40052756667137146, action 0\n",
      "EPISODE DONE IN 2.0371100902557373 SECONDS, reward -0.2683163285255432, action 4\n",
      "EPISODE DONE IN 2.647115468978882 SECONDS, reward -1.8735854625701904, action 0\n",
      "\n",
      "\n",
      "EPISODE: 258\n",
      "EPISODE DONE IN 0.6131813526153564 SECONDS, reward -0.5635496973991394, action 5\n",
      "EPISODE DONE IN 1.2271311283111572 SECONDS, reward -0.5635496973991394, action 0\n",
      "EPISODE DONE IN 1.875288963317871 SECONDS, reward -1.7839012145996094, action 1\n",
      "EPISODE DONE IN 2.5104217529296875 SECONDS, reward -1.7839012145996094, action 0\n",
      "\n",
      "\n",
      "EPISODE: 259\n",
      "EPISODE DONE IN 0.6536133289337158 SECONDS, reward -5.132562637329102, action 2\n",
      "EPISODE DONE IN 1.306492567062378 SECONDS, reward -5.132562637329102, action 0\n",
      "EPISODE DONE IN 1.8901033401489258 SECONDS, reward -7.049383640289307, action 0\n",
      "EPISODE DONE IN 2.5017621517181396 SECONDS, reward -2.2815091609954834, action 3\n",
      "\n",
      "\n",
      "EPISODE: 260\n",
      "EPISODE DONE IN 0.5788609981536865 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 1.1648311614990234 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 1.7591214179992676 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 2.2775120735168457 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 261\n",
      "EPISODE DONE IN 0.5396802425384521 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.0441372394561768 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.5865845680236816 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.0952765941619873 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 262\n",
      "EPISODE DONE IN 0.5374276638031006 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.0388636589050293 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.5730390548706055 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.0988378524780273 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 263\n",
      "EPISODE DONE IN 0.5304980278015137 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.108565330505371 SECONDS, reward -1.3662217855453491, action 4\n",
      "EPISODE DONE IN 1.6954982280731201 SECONDS, reward -1.3662217855453491, action 0\n",
      "EPISODE DONE IN 2.284273862838745 SECONDS, reward -1.3662217855453491, action 0\n",
      "\n",
      "\n",
      "EPISODE: 264\n",
      "EPISODE DONE IN 0.6044132709503174 SECONDS, reward 0.3814592659473419, action 1\n",
      "EPISODE DONE IN 1.187164306640625 SECONDS, reward -16.960826873779297, action 0\n",
      "EPISODE DONE IN 1.769562005996704 SECONDS, reward -16.960826873779297, action 0\n",
      "EPISODE DONE IN 2.378157138824463 SECONDS, reward -3.2325775623321533, action 4\n",
      "\n",
      "\n",
      "EPISODE: 265\n",
      "EPISODE DONE IN 0.6128692626953125 SECONDS, reward -1.1773006916046143, action 3\n",
      "EPISODE DONE IN 1.3041694164276123 SECONDS, reward -0.23864814639091492, action 6\n",
      "EPISODE DONE IN 2.01310658454895 SECONDS, reward -0.961571216583252, action 8\n",
      "EPISODE DONE IN 2.6620893478393555 SECONDS, reward -0.4331502616405487, action 0\n",
      "\n",
      "\n",
      "EPISODE: 266\n",
      "EPISODE DONE IN 0.6468091011047363 SECONDS, reward 0.20716089010238647, action 0\n",
      "EPISODE DONE IN 1.2275099754333496 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.7330796718597412 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.358142375946045 SECONDS, reward 1.074416160583496, action 6\n",
      "\n",
      "\n",
      "EPISODE: 267\n",
      "EPISODE DONE IN 0.6485605239868164 SECONDS, reward -0.37451088428497314, action 4\n",
      "EPISODE DONE IN 1.2609672546386719 SECONDS, reward -0.37451088428497314, action 0\n",
      "EPISODE DONE IN 1.8919036388397217 SECONDS, reward -0.37451088428497314, action 0\n",
      "EPISODE DONE IN 2.4710299968719482 SECONDS, reward -1.3344745635986328, action 0\n",
      "\n",
      "\n",
      "EPISODE: 268\n",
      "EPISODE DONE IN 0.5903799533843994 SECONDS, reward -7.049383640289307, action 2\n",
      "EPISODE DONE IN 1.1935338973999023 SECONDS, reward -7.049383640289307, action 0\n",
      "EPISODE DONE IN 1.7931571006774902 SECONDS, reward -7.049383640289307, action 0\n",
      "EPISODE DONE IN 2.4103124141693115 SECONDS, reward -0.44119709730148315, action 8\n",
      "\n",
      "\n",
      "EPISODE: 269\n",
      "EPISODE DONE IN 0.6313419342041016 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.235884189605713 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.864119529724121 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 2.3871066570281982 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 270\n",
      "EPISODE DONE IN 0.5427801609039307 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.092921495437622 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.6332824230194092 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.224789619445801 SECONDS, reward 2.6046342849731445, action 8\n",
      "\n",
      "\n",
      "EPISODE: 271\n",
      "EPISODE DONE IN 0.6253604888916016 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.2525484561920166 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.8800837993621826 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 2.4195189476013184 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 272\n",
      "EPISODE DONE IN 0.5539515018463135 SECONDS, reward -10000000.0, action 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 1.0839309692382812 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.671722650527954 SECONDS, reward 1.90692138671875, action 7\n",
      "EPISODE DONE IN 2.273486852645874 SECONDS, reward 1.90692138671875, action 0\n",
      "\n",
      "\n",
      "EPISODE: 273\n",
      "EPISODE DONE IN 0.5874209403991699 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 1.1871321201324463 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 1.732513666152954 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.245807647705078 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 274\n",
      "EPISODE DONE IN 0.5081555843353271 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.095921277999878 SECONDS, reward -1.3662217855453491, action 4\n",
      "EPISODE DONE IN 1.673543930053711 SECONDS, reward -1.3662217855453491, action 0\n",
      "EPISODE DONE IN 2.2828822135925293 SECONDS, reward -1.3662217855453491, action 0\n",
      "\n",
      "\n",
      "EPISODE: 275\n",
      "EPISODE DONE IN 0.6240212917327881 SECONDS, reward -1.3662217855453491, action 0\n",
      "EPISODE DONE IN 1.1354148387908936 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.7281219959259033 SECONDS, reward 2.6046342849731445, action 8\n",
      "EPISODE DONE IN 2.3306379318237305 SECONDS, reward 2.6046342849731445, action 0\n",
      "\n",
      "\n",
      "EPISODE: 276\n",
      "EPISODE DONE IN 0.6164805889129639 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.2125437259674072 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.7304980754852295 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.2434089183807373 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 277\n",
      "EPISODE DONE IN 0.5091135501861572 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.01155686378479 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.5520517826080322 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.0733563899993896 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 278\n",
      "EPISODE DONE IN 0.5896542072296143 SECONDS, reward -16.960826873779297, action 1\n",
      "EPISODE DONE IN 1.2046091556549072 SECONDS, reward -16.960826873779297, action 0\n",
      "EPISODE DONE IN 1.833522081375122 SECONDS, reward -16.960826873779297, action 0\n",
      "EPISODE DONE IN 2.4372386932373047 SECONDS, reward -16.960826873779297, action 0\n",
      "\n",
      "\n",
      "EPISODE: 279\n",
      "EPISODE DONE IN 0.5108487606048584 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.1022422313690186 SECONDS, reward -17.041624069213867, action 1\n",
      "EPISODE DONE IN 1.6936335563659668 SECONDS, reward -17.041624069213867, action 0\n",
      "EPISODE DONE IN 2.273066520690918 SECONDS, reward -17.041624069213867, action 0\n",
      "\n",
      "\n",
      "EPISODE: 280\n",
      "EPISODE DONE IN 0.618633508682251 SECONDS, reward -17.041624069213867, action 0\n",
      "EPISODE DONE IN 1.19712233543396 SECONDS, reward 2.5918285846710205, action 8\n",
      "EPISODE DONE IN 1.7737846374511719 SECONDS, reward 2.5918285846710205, action 0\n",
      "EPISODE DONE IN 2.3622071743011475 SECONDS, reward 2.5918285846710205, action 0\n",
      "\n",
      "\n",
      "EPISODE: 281\n",
      "EPISODE DONE IN 0.6238255500793457 SECONDS, reward 2.5918285846710205, action 0\n",
      "EPISODE DONE IN 1.1589524745941162 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.6641731262207031 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.269714593887329 SECONDS, reward 1.074416160583496, action 6\n",
      "\n",
      "\n",
      "EPISODE: 282\n",
      "EPISODE DONE IN 0.6254520416259766 SECONDS, reward 1.074416160583496, action 0\n",
      "EPISODE DONE IN 1.2113463878631592 SECONDS, reward 1.074416160583496, action 0\n",
      "EPISODE DONE IN 1.7880902290344238 SECONDS, reward 1.074416160583496, action 0\n",
      "EPISODE DONE IN 2.2900948524475098 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 283\n",
      "EPISODE DONE IN 0.5772550106048584 SECONDS, reward -3.3864054679870605, action 3\n",
      "EPISODE DONE IN 1.150073766708374 SECONDS, reward -3.3864054679870605, action 0\n",
      "EPISODE DONE IN 1.7657513618469238 SECONDS, reward -0.9063183069229126, action 5\n",
      "EPISODE DONE IN 2.4041829109191895 SECONDS, reward 0.042074400931596756, action 7\n",
      "\n",
      "\n",
      "EPISODE: 284\n",
      "EPISODE DONE IN 0.6421537399291992 SECONDS, reward -0.1780347228050232, action 7\n",
      "EPISODE DONE IN 1.3088359832763672 SECONDS, reward -0.9419445395469666, action 8\n",
      "EPISODE DONE IN 1.9977669715881348 SECONDS, reward -0.043189942836761475, action 0\n",
      "EPISODE DONE IN 2.622950315475464 SECONDS, reward 1.669680118560791, action 0\n",
      "\n",
      "\n",
      "EPISODE: 285\n",
      "EPISODE DONE IN 0.5784516334533691 SECONDS, reward 2.5918285846710205, action 0\n",
      "EPISODE DONE IN 1.088855504989624 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.62644362449646 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.160496473312378 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 286\n",
      "EPISODE DONE IN 0.5842738151550293 SECONDS, reward 0.04991356283426285, action 5\n",
      "EPISODE DONE IN 1.1604461669921875 SECONDS, reward 0.04991356283426285, action 0\n",
      "EPISODE DONE IN 1.7383747100830078 SECONDS, reward 0.04991356283426285, action 0\n",
      "EPISODE DONE IN 2.3545188903808594 SECONDS, reward -1.7839012145996094, action 1\n",
      "\n",
      "\n",
      "EPISODE: 287\n",
      "EPISODE DONE IN 0.6517679691314697 SECONDS, reward -7.169710159301758, action 1\n",
      "EPISODE DONE IN 1.3455250263214111 SECONDS, reward -1.5789419412612915, action 2\n",
      "EPISODE DONE IN 2.0091054439544678 SECONDS, reward -1.5789419412612915, action 0\n",
      "EPISODE DONE IN 2.6191446781158447 SECONDS, reward -1.4561176300048828, action 0\n",
      "\n",
      "\n",
      "EPISODE: 288\n",
      "EPISODE DONE IN 0.6252105236053467 SECONDS, reward 1.499874234199524, action 7\n",
      "EPISODE DONE IN 1.203582525253296 SECONDS, reward 1.9200869798660278, action 0\n",
      "EPISODE DONE IN 1.8291347026824951 SECONDS, reward 0.00024068844504654408, action 1\n",
      "EPISODE DONE IN 2.4488563537597656 SECONDS, reward 0.00024068844504654408, action 0\n",
      "\n",
      "\n",
      "EPISODE: 289\n",
      "EPISODE DONE IN 0.5796842575073242 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.2209453582763672 SECONDS, reward -0.793126106262207, action 6\n",
      "EPISODE DONE IN 1.8171343803405762 SECONDS, reward 1.0607329607009888, action 0\n",
      "EPISODE DONE IN 2.444894552230835 SECONDS, reward -0.7186035513877869, action 3\n",
      "\n",
      "\n",
      "EPISODE: 290\n",
      "EPISODE DONE IN 0.6168217658996582 SECONDS, reward -0.7186035513877869, action 0\n",
      "EPISODE DONE IN 1.2123808860778809 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 1.8200733661651611 SECONDS, reward 1.2686488628387451, action 6\n",
      "EPISODE DONE IN 2.399752616882324 SECONDS, reward 1.074416160583496, action 0\n",
      "\n",
      "\n",
      "EPISODE: 291\n",
      "EPISODE DONE IN 0.6019916534423828 SECONDS, reward 1.074416160583496, action 0\n",
      "EPISODE DONE IN 1.1974906921386719 SECONDS, reward 1.074416160583496, action 0\n",
      "EPISODE DONE IN 1.7807469367980957 SECONDS, reward 1.074416160583496, action 6\n",
      "EPISODE DONE IN 2.3547444343566895 SECONDS, reward 1.074416160583496, action 0\n",
      "\n",
      "\n",
      "EPISODE: 292\n",
      "EPISODE DONE IN 0.6444761753082275 SECONDS, reward -0.7813690304756165, action 1\n",
      "EPISODE DONE IN 1.3025267124176025 SECONDS, reward -0.9839819073677063, action 5\n",
      "EPISODE DONE IN 1.950751781463623 SECONDS, reward -0.5918623208999634, action 3\n",
      "EPISODE DONE IN 2.627068519592285 SECONDS, reward -1.0809439420700073, action 5\n",
      "\n",
      "\n",
      "EPISODE: 293\n",
      "EPISODE DONE IN 0.664198637008667 SECONDS, reward -0.3740987777709961, action 0\n",
      "EPISODE DONE IN 1.291867733001709 SECONDS, reward 0.9911530613899231, action 0\n",
      "EPISODE DONE IN 1.8917248249053955 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 2.4872171878814697 SECONDS, reward 0.03539615496993065, action 5\n",
      "\n",
      "\n",
      "EPISODE: 294\n",
      "EPISODE DONE IN 0.6167111396789551 SECONDS, reward -1.6223472356796265, action 2\n",
      "EPISODE DONE IN 1.226123332977295 SECONDS, reward -1.6223472356796265, action 0\n",
      "EPISODE DONE IN 1.9018821716308594 SECONDS, reward -0.16317559778690338, action 3\n",
      "EPISODE DONE IN 2.557607650756836 SECONDS, reward -2.2815091609954834, action 0\n",
      "\n",
      "\n",
      "EPISODE: 295\n",
      "EPISODE DONE IN 0.5971341133117676 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 1.2271857261657715 SECONDS, reward -1.591291904449463, action 3\n",
      "EPISODE DONE IN 1.814016342163086 SECONDS, reward -3.4230685234069824, action 0\n",
      "EPISODE DONE IN 2.3952395915985107 SECONDS, reward -3.4230685234069824, action 0\n",
      "\n",
      "\n",
      "EPISODE: 296\n",
      "EPISODE DONE IN 0.5835614204406738 SECONDS, reward -3.4230685234069824, action 0\n",
      "EPISODE DONE IN 1.0904638767242432 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.6687204837799072 SECONDS, reward -3.4047372341156006, action 3\n",
      "EPISODE DONE IN 2.2828221321105957 SECONDS, reward -3.4047372341156006, action 0\n",
      "\n",
      "\n",
      "EPISODE: 297\n",
      "EPISODE DONE IN 0.6088757514953613 SECONDS, reward -3.4047372341156006, action 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE DONE IN 1.233407974243164 SECONDS, reward -3.4047372341156006, action 0\n",
      "EPISODE DONE IN 1.7914283275604248 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.405158519744873 SECONDS, reward -7.0729827880859375, action 2\n",
      "\n",
      "\n",
      "EPISODE: 298\n",
      "EPISODE DONE IN 0.5934069156646729 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 1.1691758632659912 SECONDS, reward -7.0729827880859375, action 0\n",
      "EPISODE DONE IN 1.7935888767242432 SECONDS, reward 1.49413001537323, action 7\n",
      "EPISODE DONE IN 2.3929927349090576 SECONDS, reward 1.90692138671875, action 0\n",
      "\n",
      "\n",
      "EPISODE: 299\n",
      "EPISODE DONE IN 0.6016314029693604 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 1.2174761295318604 SECONDS, reward 1.90692138671875, action 0\n",
      "EPISODE DONE IN 1.7430806159973145 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.3288495540618896 SECONDS, reward 2.6046342849731445, action 8\n",
      "\n",
      "\n",
      "EPISODE: 300\n",
      "EPISODE DONE IN 0.619732141494751 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.2242896556854248 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 1.8508930206298828 SECONDS, reward 1.7963957786560059, action 1\n",
      "EPISODE DONE IN 2.4585745334625244 SECONDS, reward 1.7963957786560059, action 8\n",
      "\n",
      "\n",
      "EPISODE: 301\n",
      "EPISODE DONE IN 0.6309487819671631 SECONDS, reward 1.7963957786560059, action 0\n",
      "EPISODE DONE IN 1.2415473461151123 SECONDS, reward 1.7963957786560059, action 0\n",
      "EPISODE DONE IN 1.8223233222961426 SECONDS, reward 2.6046342849731445, action 0\n",
      "EPISODE DONE IN 2.3376665115356445 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 302\n",
      "EPISODE DONE IN 0.5235328674316406 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.0282630920410156 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.6342687606811523 SECONDS, reward 1.074416160583496, action 6\n",
      "EPISODE DONE IN 2.2466630935668945 SECONDS, reward 1.3113434314727783, action 5\n",
      "\n",
      "\n",
      "EPISODE: 303\n",
      "EPISODE DONE IN 0.6091628074645996 SECONDS, reward 1.3113434314727783, action 0\n",
      "EPISODE DONE IN 1.2517094612121582 SECONDS, reward -0.15058432519435883, action 5\n",
      "EPISODE DONE IN 1.8968946933746338 SECONDS, reward -0.7447284460067749, action 1\n",
      "EPISODE DONE IN 2.507375478744507 SECONDS, reward -1.8085614442825317, action 0\n",
      "\n",
      "\n",
      "EPISODE: 304\n",
      "EPISODE DONE IN 0.6152632236480713 SECONDS, reward -1.8085614442825317, action 0\n",
      "EPISODE DONE IN 1.1879544258117676 SECONDS, reward -17.0012149810791, action 0\n",
      "EPISODE DONE IN 1.7662243843078613 SECONDS, reward -17.0012149810791, action 1\n",
      "EPISODE DONE IN 2.3389699459075928 SECONDS, reward -17.0012149810791, action 0\n",
      "\n",
      "\n",
      "EPISODE: 305\n",
      "EPISODE DONE IN 0.6131772994995117 SECONDS, reward -7.169710159301758, action 1\n",
      "EPISODE DONE IN 1.222597599029541 SECONDS, reward -7.169710159301758, action 0\n",
      "EPISODE DONE IN 1.8359026908874512 SECONDS, reward -5.133672714233398, action 2\n",
      "EPISODE DONE IN 2.4738612174987793 SECONDS, reward 0.10366038233041763, action 8\n",
      "\n",
      "\n",
      "EPISODE: 306\n",
      "EPISODE DONE IN 0.6468503475189209 SECONDS, reward 0.10366038233041763, action 1\n",
      "EPISODE DONE IN 1.3359391689300537 SECONDS, reward 0.10366038233041763, action 0\n",
      "EPISODE DONE IN 1.9734158515930176 SECONDS, reward 0.4956190586090088, action 0\n",
      "EPISODE DONE IN 2.558208703994751 SECONDS, reward -16.960826873779297, action 0\n",
      "\n",
      "\n",
      "EPISODE: 307\n",
      "EPISODE DONE IN 0.5416362285614014 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.0850951671600342 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.713510513305664 SECONDS, reward 0.03539615496993065, action 5\n",
      "EPISODE DONE IN 2.336017370223999 SECONDS, reward 0.03539615496993065, action 0\n",
      "\n",
      "\n",
      "EPISODE: 308\n",
      "EPISODE DONE IN 0.628382682800293 SECONDS, reward 0.03539615496993065, action 0\n",
      "EPISODE DONE IN 1.2929596900939941 SECONDS, reward -1.7963917255401611, action 1\n",
      "EPISODE DONE IN 1.926421880722046 SECONDS, reward -17.041624069213867, action 0\n",
      "EPISODE DONE IN 2.554441213607788 SECONDS, reward -17.041624069213867, action 0\n",
      "\n",
      "\n",
      "EPISODE: 309\n",
      "EPISODE DONE IN 0.6244375705718994 SECONDS, reward -17.041624069213867, action 0\n",
      "EPISODE DONE IN 1.131338119506836 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 1.6398701667785645 SECONDS, reward -10000000.0, action 0\n",
      "EPISODE DONE IN 2.144484043121338 SECONDS, reward -10000000.0, action 0\n",
      "\n",
      "\n",
      "EPISODE: 310\n",
      "EPISODE DONE IN 0.5762310028076172 SECONDS, reward -1.3344745635986328, action 4\n",
      "EPISODE DONE IN 1.1502792835235596 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 1.734060287475586 SECONDS, reward -1.3344745635986328, action 0\n",
      "EPISODE DONE IN 2.3242459297180176 SECONDS, reward -1.3344745635986328, action 0\n",
      "\n",
      "\n",
      "EPISODE: 311\n",
      "EPISODE DONE IN 0.5127766132354736 SECONDS, reward -10000000.0, action 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-937811616fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrewards_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-6b7e6bff6962>\u001b[0m in \u001b[0;36mmake_action\u001b[0;34m(self, bs_index, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_power\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbs_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpowers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_rate_from_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-6b7e6bff6962>\u001b[0m in \u001b[0;36mpower_grid\u001b[0;34m(self, bs_loc, bs_power, points)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mpowers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mpowers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpower\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpowers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2559\u001b[0m             \u001b[0;31m# special case for speedup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2562\u001b[0m         \u001b[0;31m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# are valid for vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards_history = []\n",
    "for episode in range(num_episodes):\n",
    "    timer = time.time()\n",
    "    print(\"\\n\\nEPISODE:\", episode+1)\n",
    "    \n",
    "    for i in range(env.n_bs):\n",
    "        state = torch.tensor(env.get_state(i), dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        _, action, next_state, reward = env.make_action(i, action)\n",
    "        \n",
    "        rewards_history.append(reward)\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            exps = memory.sample(batch_size)\n",
    "            states, actions, next_states, rewards = extract_tensors(exps)\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.smooth_l1_loss(\n",
    "                current_q_values, target_q_values.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        print(\"EPISODE DONE IN {} SECONDS, reward {}, action {}\".format(time.time() - timer, reward[0], action[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28aee0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20000 \n",
      " 10 episode moving avg: -inf\n"
     ]
    }
   ],
   "source": [
    "plot(rewards_history, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c6aca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-239b063bd2c2>:74: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 10*under_1mb/total_points + 1/min_speed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09ceeef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.contour.QuadContourSet at 0x7f6d3dd38700>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.contourf(env.points[:,:,0], env.points[:,:,1], env.bit_rate, 100, cmap = plt.cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "623f5b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf -inf -inf -inf]\n"
     ]
    }
   ],
   "source": [
    "print(env.bs_power)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
